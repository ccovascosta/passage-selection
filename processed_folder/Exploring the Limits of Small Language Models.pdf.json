{
    "filename": "Exploring the Limits of Small Language Models.pdf",
    "type": "pdf",
    "title": null,
    "authors": null,
    "lastmodifiedtime": "D:20230512120704-07'00'",
    "text": "Exploring the Limits of Small Language Models\nNicholas Lee\nKurt Keutzer\nGopala Krishna Anumanchipalli\nElectrical Engineering and Computer Sciences\nUniversity of California, Berkeley\nTechnical Report No. UCB/EECS-2023-141\nhttp://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-141.html\nMay 12, 2023Copyright \u00a9 2023, by the author(s). \nAll rights reserved. \n \nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission.\n \nAcknowledgement \n \nI would like to thank Professor Kurt Keutzer and Dr. Amir Gholami for their\ncontinuous\nsupport and advice on this project and my research career. They have\nconsistently been\nthere to guide me in my research career thus far. I would also like to thank\nJa (Thanakul)\nWattanawong for working together with me and assisting me with\ncompleting this project.\nI also have to thank Karttikeya Mangalam, Sehoon Kim, Sheng Shen,\nColeman Hooper, and\nXiuyu Li for their continued advice on navigating and exploring different\nresearch directions\nduring this project. Thanks to Professor Kurt Keutzer and Professor Gopala\nAnumanchipallifor their feedback on this thesis. I am grateful to have worked with\neveryone on this project. \n \n \nExploring the Limits of Small Language Models \n \nby Nicholas Z Lee \n \n \n \n \nResearch Project \n \nSubmitted to the Department of Electrical Engineering and Computer Sciences, \nUniversity of California at Berkeley, in partial satisfaction of the requirements for the \ndegree of Master of Science, Plan II. \n \n \nApproval for the Report and Comprehensive Examination: \n \n \n \nCommittee: \n \n \n \nProfessor Kurt Keutzer \nResearch Advisor \n \n \n(Date) \n \n \n \n* * * * * * * \n \n \n \nProfessor Gopala Krishna Anumanchipalli \nSecond Reader \n \n \n05 / 11 / 2023\n05 / 09 / 2023\nDoc ID: a81e6d94b043f77b9818c7f361b10ec66b5503fc1\nAbstract\nExploring the Limits of Small Language Models\nby\nNicholas Z Lee\nMaster of Science in Electrical Engineering and Computer Science\nUniversity of California, Berkeley\nProfessor Kurt Keutzer, Chair\nWith the emergence of a plethora of Large Language Models (LLMs) to date, the future\nof having LLMs run locally at the edge has come closer and closer with every passing day.\nHowever, there has not been as much work on smaller language models that can potentially\nsolve tasks where it would be inefficient to run a full LLM at scale. In this paper, we ex-\nplore Small Language Models (SLMs) and how we can make them more efficient at the edge\nwithout sacrificing performance. Pruning or simplifying SLMs can cause a significant degra-\ndation of downstream performance. To this end, we investigate weight reparameterization\nand knowledge distillation as two avenues for these small language models to mitigate these\npitfalls. This study investigates the structure of the FFN module in the transformer archi-\ntecture in order to improve the inference speed of these language models for short sequence\nlength tasks. We also investigate whether we can distill from these LLMs into significantly\nsmaller SLMs in order to take advantage of the plethora of pretrained models available to\nthe public. We find that when simplifying the FFN module, one can use weight reparameter-\nization at training time to help the model converge and improve downstream accuracy. We\nalso find that knowledge distillation may not be a surefire way to improve the downstream\nmodel performance as the difference between the model capacities of these LLMs and small\nlanguage models may be difficult to overcome.i\nContents\nContents i\nList of Figures ii\nList of Tables iii\n1 Introduction 1\n2 Background and Prior Work 3\n2.1 T5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.2 Efficient Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2.3 Reparameterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.4 Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3 Methodology 6\n3.1 FFN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n3.2 Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n4 Results and Analysis 9\n4.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n4.2 Architecture Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n4.3 Knowledge Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n5 Conclusion 13\nBibliography 14ii\nList of Figures\n3.1 Diagram of proposed weight reparameterization methodology. On the very left\nis the standard FFN module in a transformer (with ReLU as a stand in for\nthe activation function). The second and third and proposed architectures that\nreduce the FLOPs requirement of the model significantly. The fourth and fifth\nat inference time will have the same number of parameters and FLOPs as the\nsecond, but the weight reparameterization can help stabilize training. . . . . . . 7\n4.1 On the left is a pretraining loss curve for T5-Mini on the exact same settings but\n5 different random seeds. Early on, one of the runs fails to converge and this\nleads to degraded performance downstream. We mitigate this by treating that\nrun as an outlier and reporting only the maximum value across all 5 runs. . . . 10iii\nList of Tables\n3.1 T5 Model Sizes from [32]. In this case, NLrepresents the number of layers in the\nencoder and decoder parts of the model. For example, 4 /4 means that there are\n4 encoder and 4 decoder blocks. dffis the dimension of the feed forward module,\ndmodel is the hidden dimension of the model, dkvis the dimension of the attention\nheads and NHis the number of attention heads. . . . . . . . . . . . . . . . . . . 6\n4.1 Performance of T5-Mini after removing the FFN from the whole model and from\njust the encoder. Surprisingly, we can remove the FFN completely with only a\n2.7 drop in performance. This establishes a lower bound for our FFN experiments\nand guides our plan to recover this accuracy. . . . . . . . . . . . . . . . . . . . . 11\n4.2 Training T5-Mini for more and more steps. Similarly to [32] and [26], we find\nthat the baseline model is undertrained. . . . . . . . . . . . . . . . . . . . . . . 11\n4.3 Training T5-Mini with different head dimensions. Increasing the head size or head\ndimension improves performance significantly, but increases the cost of running\nthe model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n4.4 Training T5-Mini with different weight reparameterization techniques. The ex-\nperiments with 2 and 3 weight matrices show that weight reparameterization\nindeed improves the model\u2019s downstream performance. . . . . . . . . . . . . . . 12\n4.5 Training T5-Mini three weight matrices and different expansion rates. FFN ex-\npansion is important during training, but can be collapsed during deployment for\nefficient inference. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n4.6 Distillation results for T5-Mini and T5-Small. As you can see, distilling from a\nsmaller model reduced the variance of the downstream performance of the model\nwhile maintaining or improving on the performance compared to distilling from\nthe T5-Base model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12iv\nAcknowledgments\nI would like to thank Professor Kurt Keutzer and Dr. Amir Gholami for their continuous\nsupport and advice on this project and my research career. They have consistently been\nthere to guide me in my research career thus far. I would also like to thank Ja (Thanakul)\nWattanawong for working together with me and assisting me with completing this project.\nI also have to thank Karttikeya Mangalam, Sehoon Kim, Sheng Shen, Coleman Hooper, and\nXiuyu Li for their continued advice on navigating and exploring different research directions\nduring this project. Thanks to Professor Kurt Keutzer and Professor Gopala Anumanchipalli\nfor their feedback on this thesis. I am grateful to have worked with everyone on this project.1\nChapter 1\nIntroduction\nIn the last couple of months, we have seen a variety of research into Large Language Models\ncome onto the scene pushing the frontier of what kinds of large language models can be run\nreliably and efficiently. What began with the release of LLaMa [34] has expanded to include\nthe release of many variants trained on top of LLaMa such as Alpaca [31], Koala [13] and\nVicuna [3] as well as other publicly released models such as Pythia [1] and Stability LM [29].\nThis Cambrian Explosion of different large language models has led to an increased interest\nin the idea of running LLMs locally, instead of going to the cloud. This could allow for many\ndifferent applications where data cannot leave the device such as in legal or medical settings\nor in resource constrained applications such as on edge devices or personal computers.\nIn this work, we investigate small language models ( <1B parameters) and in particular,\ninvestigate how we can make these models run more efficiently without sacrificing perfor-\nmance. To this end, we investigate two main avenues of interest:\n\u2022Weight Reparameterization ([11],[12],[10]) is a technique where the neural network has\na complex structure during pretraining that can be collapsed down to a more efficient\nstructure at inference time. While we pay an increased cost at training time, being able\nto reduce the footprint of these models at inference time will lead to reduced latency\nand power costs during the lifetime of the model.\n\u2022Knowledge Distillation [16] is a technique where the information from a teacher model\nis distilled into a smaller model by providing pseudo-labels for the inputs. With the\nadvent of so many new LLMs coming into the fray, being able to take advantage of\nthese models in order to supercharge smaller models at the edge would prove to be\nuseful at allowing SLMs to handle tasks in a cost or resource constrained environment.\nWe make the following contributions:\n\u2022We investigate the extent to which these small language models have been trained and\ninvestigate the FFN module, as it takes up most of the latency for short sequence\nlength applications [19]. We find that one can remove the FFN module and cut theCHAPTER 1. INTRODUCTION 2\nmodel size and FLOPs by two-thirds, but we also see a significant drop in test time\nperformance.\n\u2022We investigate ways to use weight reparameterization to improve the performance of\nthese small architectures without reducing model performance or accuracy at test time.\nWe find that weight reparameterization can mitigate these drawbacks of pruning the\nFFN module.\n\u2022We use KD to test the efficacy of distilling from these large language models. We find\nthat while KD can improve the performance of these models, it may not be straightfor-\nward to distill from much larger language models as the model capacity of the student\nmodels may not be enough to emulate the teacher.3\nChapter 2\nBackground and Prior Work\n2.1 T5\nT5 [26] is a Transformer LLM released by Google. Unlike current GPT-style LLMs [2] which\nare decode-only or BERT-like [9] encoder-only architectures, T5 uses an encoder-decoder\nstyle architecture similar to that of the original Transformer [35] architecture. Since the\ninitial release, several variants of T5 have been built on-top of the original architecture. mT5\n[41] is a multilingual variant of T5 pretrained on data covering over 100 languages. ByT5\n[40] is a token-free model that uses the byte representation of text as the vocabulary. Flan-\nT5 [6] was one of the first instruction-finetuned LLMs built on-top of T5 that dramatically\nimproved its performance on a variety of different NLP benchmarks.\nA similar publication in the direction of this work is [32], which analyzed the effect of\nboth model size and model shape of T5 on both the upstream pretraining loss and the\ndownstream finetuning performance. The upstream task was i.i.d. denoising on the C4\ndataset [26] and the downstream tasks were performance on a multitude of different NLP\ntasks such as GLUE [36] and SQuAD [27]. This paper found that upstream pretraining loss\ndid not directly correlate with downstream performance. It instead found that at different\nmodel sizes, the shape of the model played a significant role in the final accuracy of the\nmodel after finetuning. While this work was primarily focused on scaling towards larger and\nlarger variants of T5, in this project, we will instead go the opposite direction, and see what\ncan be gained at the smaller end of the spectrum.\n2.2 Efficient Transformers\nThere have been many papers over the years exploring the realm of efficient transformers\nsuch as [30, 18, 22, 7, 9]. Many follow the vein of finding more efficient attention mechanisms:\n[21, 37, 42, 33, 5, 39, 15, 8, 24, 25, 28, 17, 23]. These works find more efficient approximations\nor implementations of attention so that for long form content, the quadratic complexity ofCHAPTER 2. BACKGROUND AND PRIOR WORK 4\nthe attention mechanism can be reduced or linearized to become more efficient. Some like\n[23] even replace the attention mechanism all together.\nWhile these works mainly target the attention mechanism, the attention mechanism does\nnot dominate the runtime of these large language models at every scale. A recent survey\npaper [19] found that the latency of these large language models is dominated by the large\nmatrix multiplications of the FFN modules of the transformer architecture for sequence\nlengths less than 1024. In fact, for sequences of length 128, the FFN module dominates over\nhalf of the latency of BERT-Base. For this reason, we investigate the structure of the FFN\nmodule in these language models to see if we can improve their performance for these short\nsequence length scenarios.\n2.3 Reparameterization\nReparameterization is a general technique when at training time, the neural network has a\nmore complex architecture that can be collapsed into a simpler and more efficient mechanism\nat inference time with no loss to performance. The standard example is the scale and bias\nparameters of a normalization layer, which is commonly absorbed into the subsequent FFN\nlayer. Reparameterization is commonly used in computer vision, e.g. [11], [12], and [10].\nWhile this structure can be reduced with no loss to accuracy, it is still important for this\nkind of structure to exist during pretraining as it helps the model converge. For example, in\n[20], the authors found that removing an extra normalization term in between blocks resulted\nin worse performance overall. The key was that while the back to back normalization had\nno effect, the difference between the scale and bias terms of the activations that go into the\nskip connection and the activations that go into the attention module significantly affected\ndownstream performance.\n2.4 Distillation\nKnowledge Distillation [16] is a method to transfer information and knowledge from a pre-\ntrained teacher model to a smaller student model. The standard distillation methodology is\nto run the input through both the pretrained teacher model as well as the student model.\nThen the logits of the output of the teacher model are used as pseudo-labels for the input and\ncan be used in conjunction with the ground truth from the dataset. Knowledge Distillation\nhave been used to augment the training language models in the past, such as in [30] and\n[38]. For a more comprehensive survey of knowledge distillation, see [14].\nWith the advent of so many LLMs being released to the public, it would be good to\nstudy whether we can distill the performance from these very large models down to smaller\nlanguage models in order to get an easy boost to performance. It should be noted that KD\nis not always beneficial to the training of neural networks. In [4], the authors launched a\nstudy of KD in vision with an emphasis on analyzing the relationship between teacher modelCHAPTER 2. BACKGROUND AND PRIOR WORK 5\nperformance and size with student model downstream performance. The authors found that\nKD can hinder the downstream performance of the student model. In addition, they found\nthat increasing the size and improving the performance of the teacher model did not correlate\nto improvement of the student model. In essence, they hypothesize that there is a disconnect\nbetween the training objective of learning from the teacher and the downstream task itself.\nFurthermore, it is conjectured that the smaller student models may not have the capacity to\nemulate and learn from the teacher model. The authors find ways of circumventing this by\nusing smaller teacher models, stopping distillation early, and using early stopped teachers.6\nChapter 3\nMethodology\nFollowing [32], we utilize the smaller T5 variants in order to run our ablations. The ar-\nchitecture parameters are shown in Table 3.1. We are interested in how we can make the\nextremely small T5-Tiny and T5-Small models perform better. In particular, we would like\nto see what the upper limit in performance of these models is, modulo training resources\nand time.\n3.1 FFN\nThe FFN module in the transformer architecture typically consists of an expansion layer\nfrom dmodel todff, an activation function, and then a projection layer from dfftodmodel.\nTaking into account the insights from [19], we focus on ablating the FFN module of T5 to\nsee how we can improve the efficiency of the model. To this end, we see how the model\nperforms under the following adjustments:\n1. Skipping the FFN altogether: Since the model is so small, the W0component of the\nModel NL dff dmodel dkvNH#Params\nTiny 4/4 1024 256 32 4 16M\nMini 4/4 1536 384 32 8 31M\nSmall 6/6 2048 512 32 8 60M\nBase 12/12 3072 768 64 12 220M\nTable 3.1: T5 Model Sizes from [32]. In this case, NLrepresents the number of layers in the\nencoder and decoder parts of the model. For example, 4 /4 means that there are 4 encoder\nand 4 decoder blocks. dffis the dimension of the feed forward module, dmodel is the hidden\ndimension of the model, dkvis the dimension of the attention heads and NHis the number\nof attention heads.CHAPTER 3. METHODOLOGY 7\nFigure 3.1: Diagram of proposed weight reparameterization methodology. On the very left\nis the standard FFN module in a transformer (with ReLU as a stand in for the activation\nfunction). The second and third and proposed architectures that reduce the FLOPs require-\nment of the model significantly. The fourth and fifth at inference time will have the same\nnumber of parameters and FLOPs as the second, but the weight reparameterization can help\nstabilize training.\nattention mechanism acts like a pseudo-FFN already, just without the skip connection.\n2. Adjusting Head Dimensions\n3. Weight Reparameterization\nWith respect to item 3, we would like to see if we can simplify the FFN module for tiny\nmodels by removing the expansion and contraction component of the module. However,\nsince this may have adverse effect on the performance of the model, we use weight reparam-\neterization in order to stabilize the training of the model. Figure 3.1 shows the proposed\nablations for the FFN module. Each of the 4 on the right would reduce the number of\nparameters and FLOPs of the FFN layer significantly.\n3.2 Knowledge Distillation\nWe apply knowledge distillation to T5-mini and T5-tiny from T5-Small and T5-Base. Given\na temperature Tand logits l(x), we can define the smoothed probability distribution\npT\nj(x) =exp(lj(x)/T)\nP\nkexp(lk(x)/T)(3.1)CHAPTER 3. METHODOLOGY 8\nwhere pT\nj(x) is the probability of token jgiven input x. Given the smoothed teacher distri-\nbution pt\nkand the smoothed student distribution ps\nk, the total loss would be\nL=\u03b1Lcls+ (1\u2212\u03b1)LKD (3.2)\nwhere\nLKD=\u2212T2X\nkpt\nk(x) logps\nk(x) (3.3)\nis the KD loss and Lclsis the standard cross entropy loss.9\nChapter 4\nResults and Analysis\n4.1 Experimental Setup\nIn the following experiments, we follow the baseline methodology for pretraining and fine-\ntuning the models as in [32] and [26]. We train with a batch size of 128 and pretrained\nfor 524,288 Steps with an inverse square root learning rate scheduler on the i.i.d. denoising\ntask on the C4 dataset. During finetuning, we finetune on GLUE for 262,144 steps with a\nconstant 10\u22123learning rate.\nWhile pretraining and finetuning these models, we found that these smaller models suffer\nfrom instability. As seen in Figure 4.1, some of the runs fail to converge during pretraining\nand subsequently have poor performance during finetuning. We hypothesize that this is due\nto poor initialization, as restarting training from an intermediate checkpoint did not yield any\nof this high variance behavior. Interestingly, we did not see this behavior when pretraining\nT5-Base or T5-Small which suggests that this is an issue that affect only models at this\nscale. Across many runs, the behavior is that the outlier always performs poorly compared\nto the base model. In order to mitigate this issue, we did extensive hyperparameter tuning\nand found that for smaller models, using a linear warmup of 20,000 steps (compared to a\nconstant warmup of 10,000 steps from [26]) manages to reduce the variance across the board.\nIn the following results, we always report the maximum result across 5 runs in order\nto eliminate the effect that this outlier has on our results. This comes from the fact that\nwe never found an outlier that performed better than the other runs, only worse. We report\nour results using MNLI-mm (mismatched) from the GLUE test suite.\n4.2 Architecture Ablations\nIn order to establish a baseline on what kind of improvements we can gain, we first experiment\nwith removing the FFN module in Table 4.1. Based on this, we can see that removing the\nFFN module completely from the model only results in a 2.7 drop in performance, in exchange\nfor a hefty drop in parameter count and FLOPs.CHAPTER 4. RESULTS AND ANALYSIS 10\nFigure 4.1: On the left is a pretraining loss curve for T5-Mini on the exact same settings\nbut 5 different random seeds. Early on, one of the runs fails to converge and this leads to\ndegraded performance downstream. We mitigate this by treating that run as an outlier and\nreporting only the maximum value across all 5 runs.\nTaking a step further, we also experiment with training for more steps. The results in\nTable 4.2 show there are diminishing returns to training longer on the model. These results\nalign themselves well with [26] in that it shows that the baseline model is pretrained sub-\noptimally. We also explored adjusting the head size and head dimensions of the attention\nmodule in Table 4.3. While increasing the number of heads and the size of the heads improves\nperformance overall, it also comes with a non-trivial increase in parameter count and FLOPs.\nTable 4.4 shows the results of the weight reparameterization experiments on T5-mini. As\nwe can see, replacing the FFN module with just one weight matrix leads to a 33% drop in\nFLOPs and parameters, but leads to significant degradation in performance. However, by us-\ning weight reparameterization with two or three weight matrices that can be re-parameterized\nat inference time, we can recover some of the performance back. Overall, this shows that\nwe can cut the parameters and FLOPs of this model in exchange for only a 1 point drop in\nMNLI-mm.\nGoing further, we experimented on changing the FFN expansion rate of the FFN module\nfor the 3 weight reparameterization scheme. The idea here is that the second weight matrix\nof size dff\u00d7dffcan have a significant impact on pretraining, so reducing the expansion rate\nfrom 4 xcan help make this method more lightweight for pretraining. As seen in Table 4.5,\nthe FFN expansion can have some effect on the final finetuning performance. Reducing the\nFFN expansion too much led to degraded performance as after reducing beyond 1 .5x\u22121.2x\nexpansion, the model now performs worse than just having two weight matrices. This shows\nthat having a large expansion factor is still important for pretraining even if the weights will\nbe reparameterized at runtime.CHAPTER 4. RESULTS AND ANALYSIS 11\nMNLI-mm Improvement (+) FLOPs(M) Parameters (M)\nBaseline 78.2 0.0 1.909 15.351\nSkip FFN 75.5 -2.7 0.682 5.907\nSkip Encoder FFN 76.1 -2.1 1.296 10.629\nTable 4.1: Performance of T5-Mini after removing the FFN from the whole model and from\njust the encoder. Surprisingly, we can remove the FFN completely with only a 2.7 drop in\nperformance. This establishes a lower bound for our FFN experiments and guides our plan\nto recover this accuracy.\nSteps MNLI-mm Improvement (+) Training Tokens\nBaseline 500k 78.2 0.0 34B\n1M 79.0 +0.8 68B\n2M 79.4 +1.2 136B\n5M 80.1 +1.9 340B\nTable 4.2: Training T5-Mini for more and more steps. Similarly to [32] and [26], we find\nthat the baseline model is undertrained.\nMNLI-mm Improvement (+) FLOPS (M) Parameters (M)\nBaseline 78.2 0.0 1.909 15.351\n1.5x Heads ( NH= 12) 79.2 1.0 2.246 16.530\n2x Head Size ( dkv= 64) 79.2 1.0 2.245 16.530\n1.5x Heads, 2x Head Size 80.0 1.8 2.741 18.273\nTable 4.3: Training T5-Mini with different head dimensions. Increasing the head size or head\ndimension improves performance significantly, but increases the cost of running the model.\n4.3 Knowledge Distillation\nTable 4.6 shows the results from the knowledge distillation experiments. As we can see,\ndistillation manages to improve the downstream performance of the T5-Mini and T5-Tiny\nmodels. However, we see that distilling from T5-Small rather than T5-Base manages to\nsignificantly reduce the variance of the result while also maintaining or even improving on the\ndownstream performance of the model. This aligns well with the results from [4], and shows\nevidence that distilling from a larger language model can have limited results if the smaller\nlanguage model does not have the capacity to replicate the larger model\u2019s representations.\nGoing forward, it seems like it is important to consider the relative size differences between\nthe teacher and student models when using knowledge distillation.CHAPTER 4. RESULTS AND ANALYSIS 12\nWeights MNLI-mm Improvement (+) FLOPS (M) Parameters (M)\nBaseline 78.2 0.0 1.909 15.351\n1W 76.5 -1.7 1.291 10.626\n1W + ReLU 76.1 -2.1 1.299 10.626\n2W 76.7 -1.5 1.291 10.626\n3W 77.2 -1.0 1.291 10.626\nTable 4.4: Training T5-Mini with different weight reparameterization techniques. The exper-\niments with 2 and 3 weight matrices show that weight reparameterization indeed improves\nthe model\u2019s downstream performance.\nMNLI-mm Improvement (+) FLOPS (M) Parameters (M)\nBaseline 78.2 0.0 1.909 15.351\n4x Expansion, 3W 77.2 -1.0 1.291 10.626\n2x Expansion, 3W 77.3 -0.9 1.291 10.626\n1.5x Expansion, 3W 76.7 -1.5 1.291 10.626\n1.2x Expansion, 3W 76.6 -1.6 1.291 10.626\nTable 4.5: Training T5-Mini three weight matrices and different expansion rates. FFN\nexpansion is important during training, but can be collapsed during deployment for efficient\ninference.\nSetup MNLI-mm Stdev Improvement (+) \u03b1 T\nMini (Baseline) 78.2 0.32\nSmall\u2192Mini 79.5 0.56 +1.3 0.1 3\nBase\u2192Mini 79.4 2.18 +1.2 0.9 4\nTiny (Baseline) 76.3 0.39\nSmall\u2192Tiny 77.7 0.99 +1.4 0.1 2\nBase\u2192Tiny 77.7 3.35 +1.4 0.9 4\nTable 4.6: Distillation results for T5-Mini and T5-Small. As you can see, distilling from\na smaller model reduced the variance of the downstream performance of the model while\nmaintaining or improving on the performance compared to distilling from the T5-Base model.13\nChapter 5\nConclusion\nIn conclusion, we have investigated very small language models and found some interesting\nresults. The FFN can be removed from the model completely in order to achieve a two-thirds\ndrop in parameter count and FLOPs, while only giving up a 2.7 drop in finetuning accuracy.\nReducing the FFN layer to a simple projection layer can reduce the parameter count by one-\nthird and the use of weight reparameterization can reduce the performance derogation from\n1.7 to 1.0 points on MNLI-mm. We have also investigated the use of distillation in language\nmodels. While KD improves the performance of the model, it is also important to take into\naccount the relative difference between the capacities and models sizes of the teacher and\nstudents models in order to achieve good performance. In particular, we found that while\ndistilling from T5-Base model improved performance on T5-Mini and T5-Tiny by 1.2 and\n1.4 points respectively, distilling from T5-Small resulted in improved performance by 1.3\nand 1.4 points respectively while also significantly reducing the variance of the distillation\nexperiments. These results can help us develop newer and more efficient architectures that\ncan run more efficiently at inference time.\nIn the future, further work can be done to see what kind of changes can be done in the\nsmall model regime. One idea would be to somehow merge the MHA and FFN modules into\none. The output layer of the MHA layer is a projection layer that looks very similar to the\n1 weight matrix FFN variant, except that that variant includes a skip connection from the\ninput to the MHA layer. It would be interesting to see if we can develop a model without\nthis skip connection, which would improve the memory and computation costs of the model\nat runtime.14\nBibliography\n[1] Stella Biderman et al. Pythia: A Suite for Analyzing Large Language Models Across\nTraining and Scaling . 2023. arXiv: 2304.01373 [cs.CL] .\n[2] Tom B. Brown et al. Language Models are Few-Shot Learners . 2020. arXiv: 2005.14165\n[cs.CL] .\n[3] Wei-Lin Chiang et al. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%*\nChatGPT Quality . Mar. 2023. url:https://lmsys.org/blog/2023-03-30-vicuna/ .\n[4] Jang Hyun Cho and Bharath Hariharan. On the Efficacy of Knowledge Distillation .\n2019. arXiv: 1910.01348 [cs.LG] .\n[5] Krzysztof Choromanski et al. Rethinking Attention with Performers . 2022. arXiv:\n2009.14794 [cs.LG] .\n[6] Hyung Won Chung et al. Scaling Instruction-Finetuned Language Models . 2022. arXiv:\n2210.11416 [cs.LG] .\n[7] Kevin Clark et al. ELECTRA: Pre-training Text Encoders as Discriminators Rather\nThan Generators . 2020. arXiv: 2003.10555 [cs.CL] .\n[8] Tri Dao et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-\nAwareness . 2022. arXiv: 2205.14135 [cs.LG] .\n[9] Jacob Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Lan-\nguage Understanding . 2019. arXiv: 1810.04805 [cs.CL] .\n[10] Xiaohan Ding et al. ACNet: Strengthening the Kernel Skeletons for Powerful CNN via\nAsymmetric Convolution Blocks . 2019. arXiv: 1908.03930 [cs.CV] .\n[11] Xiaohan Ding et al. Diverse Branch Block: Building a Convolution as an Inception-like\nUnit. 2021. arXiv: 2103.13425 [cs.CV] .\n[12] Xiaohan Ding et al. RepVGG: Making VGG-style ConvNets Great Again . 2021. arXiv:\n2101.03697 [cs.CV] .\n[13] Xinyang Geng et al. Koala: A Dialogue Model for Academic Research . Blog post. Apr.\n2023. url:https://bair.berkeley.edu/blog/2023/04/03/koala/ (visited on\n04/03/2023).BIBLIOGRAPHY 15\n[14] Jianping Gou et al. \u201cKnowledge Distillation: A Survey\u201d. In: International Journal of\nComputer Vision 129.6 (Mar. 2021), pp. 1789\u20131819. doi:10 . 1007 / s11263 - 021 -\n01453-z .url:https://doi.org/10.1007%2Fs11263-021-01453-z .\n[15] Albert Gu, Karan Goel, and Christopher R\u00b4 e. Efficiently Modeling Long Sequences with\nStructured State Spaces . 2022. arXiv: 2111.00396 [cs.LG] .\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural\nNetwork . 2015. arXiv: 1503.02531 [stat.ML] .\n[17] Weizhe Hua et al. Transformer Quality in Linear Time . 2022. arXiv: 2202 . 10447\n[cs.LG] .\n[18] Forrest N. Iandola et al. SqueezeBERT: What can computer vision teach NLP about\nefficient neural networks? 2020. arXiv: 2006.11316 [cs.CL] .\n[19] Sehoon Kim et al. Full Stack Optimization of Transformer Inference: a Survey . 2023.\narXiv: 2302.14017 [cs.CL] .\n[20] Sehoon Kim et al. Squeezeformer: An Efficient Transformer for Automatic Speech\nRecognition . 2022. arXiv: 2206.00888 [eess.AS] .\n[21] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. \u201cReformer: The Efficient Trans-\nformer\u201d. In: CoRR abs/2001.04451 (2020). arXiv: 2001.04451 .url:https://arxiv.\norg/abs/2001.04451 .\n[22] Zhenzhong Lan et al. ALBERT: A Lite BERT for Self-supervised Learning of Language\nRepresentations . 2020. arXiv: 1909.11942 [cs.CL] .\n[23] James Lee-Thorp et al. FNet: Mixing Tokens with Fourier Transforms . 2022. arXiv:\n2105.03824 [cs.CL] .\n[24] Xuezhe Ma et al. Mega: Moving Average Equipped Gated Attention . 2023. arXiv: 2209.\n10655 [cs.LG] .\n[25] Jason Phang, Yao Zhao, and Peter J. Liu. Investigating Efficiently Extending Trans-\nformers for Long Input Summarization . 2022. arXiv: 2208.04347 [cs.CL] .\n[26] Colin Raffel et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text\nTransformer . 2020. arXiv: 1910.10683 [cs.LG] .\n[27] Pranav Rajpurkar et al. SQuAD: 100,000+ Questions for Machine Comprehension of\nText. 2016. arXiv: 1606.05250 [cs.CL] .\n[28] Aurko Roy et al. Efficient Content-Based Sparse Attention with Routing Transformers .\n2020. arXiv: 2003.05997 [cs.LG] .\n[29] Stability-AI. Stability-ai/stablelm: Stablelm: Stability ai language models .url:https:\n//github.com/Stability-AI/StableLM .\n[30] Zhiqing Sun et al. MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited\nDevices . 2020. arXiv: 2004.02984 [cs.CL] .BIBLIOGRAPHY 16\n[31] Rohan Taori et al. Stanford Alpaca: An Instruction-following LLaMA model .https:\n//github.com/tatsu-lab/stanford_alpaca . 2023.\n[32] Yi Tay et al. \u201cScale Efficiently: Insights from Pre-training and Fine-tuning Transform-\ners\u201d. In: CoRR abs/2109.10686 (2021). arXiv: 2109.10686 .url:https://arxiv.\norg/abs/2109.10686 .\n[33] Yi Tay et al. Sparse Sinkhorn Attention . 2020. arXiv: 2002.11296 [cs.LG] .\n[34] Hugo Touvron et al. LLaMA: Open and Efficient Foundation Language Models . 2023.\narXiv: 2302.13971 [cs.CL] .\n[35] Ashish Vaswani et al. \u201cAttention Is All You Need\u201d. In: CoRR abs/1706.03762 (2017).\narXiv: 1706.03762 .url:http://arxiv.org/abs/1706.03762 .\n[36] Alex Wang et al. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural\nLanguage Understanding . 2019. arXiv: 1804.07461 [cs.CL] .\n[37] Sinong Wang et al. \u201cLinformer: Self-Attention with Linear Complexity\u201d. In: CoRR\nabs/2006.04768 (2020). arXiv: 2006.04768 .url:https://arxiv.org/abs/2006.\n04768 .\n[38] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured Pruning Learns Compact\nand Accurate Models . 2022. arXiv: 2204.00408 [cs.CL] .\n[39] Yunyang Xiong et al. Nystr\u00a8 omformer: A Nystr\u00a8 om-Based Algorithm for Approximating\nSelf-Attention . 2021. arXiv: 2102.03902 [cs.CL] .\n[40] Linting Xue et al. ByT5: Towards a token-free future with pre-trained byte-to-byte\nmodels . 2022. arXiv: 2105.13626 [cs.CL] .\n[41] Linting Xue et al. mT5: A massively multilingual pre-trained text-to-text transformer .\n2021. arXiv: 2010.11934 [cs.CL] .\n[42] Manzil Zaheer et al. \u201cBig Bird: Transformers for Longer Sequences\u201d. In: CoRR abs/2007.14062\n(2020). arXiv: 2007.14062 .url:https://arxiv.org/abs/2007.14062 ."
}