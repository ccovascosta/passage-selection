{
    "filename": "Large language models SIMPLIFIED.pptx",
    "type": "pptx",
    "title": "Large language models",
    "authors": "Andrew Banks",
    "lastmodifiedtime": "2024-06-28T18:25:02",
    "text": "Large language models\u000b\n\n\nAndrew Banks\nLight overview\nDefinitions\nTemperature\nHow much the model can \u201challucinate\u201d. You want a higher temperature for more creative tasks. \nTransformer\nWhat LLMs are built from. The architecture comes from Googles \u201cAttention is all you need\u201d paper. They work by calculating \u201cattention\u201d between tokens\nInference time\nThe time it takes for the model to predict the next token. \nAutoregressive\nThe output of a model is fed into itself repeatedly.\nParameter/weights\nA variable that adjust the output of a neuron in a neural net. These variables are trained with the training set .\n\n\n\n\n\nDefinitions\nLLM\nLarge language model\nContext\nThe tokens that a LLM is currently predicting on\nToken\nRepresents a single instance of data our model \nToken-limit / context-limit / context-size\nThe maximum number of tokens a LLM can predict on \nEmbedding\nVector representation of our data/token. \nEmbedding matrix/vocabulary\nRepresents all possible vocabulary/tokens that a LLM can output\nHallucination\nLLM\u2019s will occasionally focus more on sounding grammatically correct rather than being factually correct\n\n\nWhat is a large language model?\nA machine learning model trained on a massive amount of text, that can be used to generate text\nThey generate text word by word, until an \u201cend token\u201d is generated.\u00a0\nThe model is rerun for every token\nAlso known as foundational models\nUse cases\nSituation that requires a large amount of conditionals\nQA chatbot on local data\nStory/Dialogue generation\nProgramming help\nText summarization\n\nPotential issues\nHallucinations\nMath\nExpensive\nUnderstanding vs mimicking..?\n\n\nTokens\nThe unit of data LLMs operate on. It is a piece of a much larger text\nThe process of splitting input into a list of tokens is called \u201ctokenization\u201d\nA simple tokenization process would be to make every word in a sentence a token\nTokenizer example\nhttps://platform.openai.com/tokenizer\n\nEmbedding\nAn embedding vector is a numerical representation of a token\nThe embedding is encoded with information such as contextual information, meaning, etc.\nTokens that occur in similar contexts will usually have similar embedding representations.\nA token is converted into an embedding through a separate embedding model.\n\nEmbedding\nEach dimension/cell represents a property of the token.\n\n0.125\n0\n0\n0.25\n0.6\n0.025\n0.1\n0.1\n0.1\n0\n0.5\n0.2\n\u2026\n\u2026\n\u2026\n\u2026\nTemperature\nTemperature could be thought of a creativity lever.\nHigher temperature means the model will have more potential next tokens. \nLower temperature means the model will be more deterministic.\nI got sick drinking very sour ____\nTemperature of 1 \nTemperature of 0.01 \nHigh level overview\nThe LLM takes in a list of tokens\nEvery input token is converted into an embedding\nThe LLM works with the embeddings to produce a final distribution over the embedding matrix / output vocabulary. \nThe next token produced by the LLM is chosen from the probability distribution over the vocabulary.\nRepeat until end token is predicted/generated by the model\nContext size\nDue to how language models work, they are only able to process up to a fixed number of tokens at a time.\nContext size seems to range from 2400 tokens to around 80,000 tokens, depending on model architecture.\nSome prominent large language models\nGPT\nCLAUDE\nLLaMA\nPalm-2\nGPT\n\u201cGenerative Pretrained Transformer\u201d\nThe model that powers Chat-GPT\nGPT4\nContext size: 32,768 tokens\nEnglish words: ~24000\nMultiModal(internal only)!\nGPT3.5-turbo\nContext size: 4096 tokens\nEnglish words: ~3000\t\n\n\n\nLLaMA\nLlama is an opensource foundational model from Meta.\nMeta also provided variable parameter versions of LLaMa, enabling the ability to run them on low VRAM hardware\nComes with varying number of weights/parameters: 7B, 13B, 33B, 65B\nB here is billion parameters/weights\nAlmost all current opensource models are modifications of Llama, as it was the first opensource model that worked good enough\u2122\n\n\nLLaMA optimizations\nLora(Low Rank Adaptation) is a way to \u201cfine-tune\u201d LLM\u2019s that doesn\u2019t require a large amount of GPU\u2019s, and can be done with cheap hardware\nQuantization is a technique that reduces the precision of floating point numbers. This can reduce the model size\n\n\nQuick note on LoRa\u2019s and finetuning\nLoRa\u2019s are usually trained to focus on a specific style of text\nThis does not take up context space\nYou can greatly modify the output of your LLM with prompting\nThis takes up context space\nFinetuning a foundational model is incredibly expensive\nLlama model examples\nLLaMa 7B\nVRAM required: 9.2GB\nLLaMA 7B quantized to 4bit with AutoGPTQ\nVram required: 6GB\nDeveloping with LLMS\nVector databases\nSemantic kernels\nVector databases\nVector databases can store, index, and search across massive datasets of unstructured vector data (embeddings)\nVector search is usually done using ANN\nSimilarity could be calculated with cosine similarity or L2 distance\nIf there aren\u2019t many vectors, a na\u00efve O(n) search could be faster in a relational DB\nVector databases\nMilvus\nOpen source, FAST\nWeaviate\nOpen source, go based, semantic search, multiple API support\nRest, GraphQL\nPinecone\nCommercial\nOperates on GCP or AWS\nFAISS (Facebook AI Similarity Search)\nNot really a database, but provides similarity search for vectors\nLangchain\nLangchain can be used to interact with LLM in a programmatic way and \u201cchain\u201d LLM\u2019s together. Currently only supports python\u2026.\nThe goals of langchain are\n1. \u201cBe data-aware: connect a language model to other sources of data\u201d\n2. \u201cBe agentic: Allow a language model to interact with its environment\u201d\n\nMicrosoft has semantic kernel which works very similarly\nLangchain: Chains\nChains can be used to accomplish a common use case. \nThe flow of logic is predetermined.\n\n\nLangchain: Agent\nAgents are useful when the code path may not be predetermined.\nThis could occur if we need to use different chains depending on user input.\nThese type of \u201cchains\u201d have an \u201cagent\u201d that can be used solve the problem\nActions an agent do can either be using a tool and observing its output, or returning a result to the user\n\nLangchain: Tools\nYou can create your own custom tools, or use tools provided by the opensource community\nTools are akin to plugins in Chatgpt\n\nhttps://python.langchain.com/en/latest/modules/agents/tools/getting_started.html\n\n\nCurrent models in the space. \nCloud based/proprietary:\nGPT4\nGPT3 / 3.5\nBard / Palm2\nLocally runnable: \nFacebook Llama\nAlpaca\ngpt4Xalpaca\nsupercot\nVicuna\nWizard\nStability ML\n\nInteresting developments with LLM\u2019s\nAuto-GPT\nAn agent that can recursively call agents to complete tasks.\nCharacterai/tavernai\nA platform that allows users to set the behavior, setting, and other personality features of a chat bot. Useful for role playing and character simulation\nChatgpt\nA wrapper over GPT3.5 and GPT4 that is quite capable as a chat bot\nGPTQ\nQuantization of models parameters, enabling the model to run models on cheap hardware such as raspberry pi\nVicuna/Cerebras/alpaca\nPopular open source models that are quite capable (around GPT3.5 level), and locally runnable. Built off of facebook llama\nLoRA\nReduces the memory required to run \u201cfinetuned\u201d models. Also trains much faster. \nIt may be possible to merge these with quantized models! Currently active research\nClaude \nModel with 100k token context limit.\nOwned by anthropic. \nFor comparison, Gpt3.5 has a 4096 token limit and GPT4 has 32768 token limit\n\n\n\n"
}