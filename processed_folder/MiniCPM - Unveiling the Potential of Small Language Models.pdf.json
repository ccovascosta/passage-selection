{
    "filename": "MiniCPM - Unveiling the Potential of Small Language Models.pdf",
    "type": "pdf",
    "title": "",
    "authors": "",
    "lastmodifiedtime": "D:20240604013058Z",
    "text": "MiniCPM: Unveiling the Potential of Small Language Models\nwith Scalable Training Strategies\nShengding Hu1, Yuge T u2, Xu Han1\u2217, Chaoqun He1, Ganqu Cui1, Xiang Long2,\nZhi Zheng2, Yewei Fang2, Yuxiang Huang1, Weilin Zhao1, Xinrong Zhang1,\nZheng Leng Thai1,Kaihuo Zhang2, Chongyi Wang2, Yuan Yao1,\nChenyang Zhao1, Jie Zhou2, Jie Cai2, , Zhongwu Zhai2, Ning Ding1,\nChao Jia2, Guoyang Zeng2, Dahai Li2, Zhiyuan Liu1*, Maosong Sun1*\n1Department of Computer Science and Technology, Tsinghua University.\n2Modelbest Inc.\nshengdinghu@gmail.com\nAbstract\nThe burgeoning interest in developing Large Language Models (LLMs) with\nup to trillion parameters has been met with concerns regarding resource\nefficiency and practical expense, particularly given the immense cost of\nexperimentation. This scenario underscores the importance of exploring\nthe potential of Small Language Models (SLMs) as a resource-efficient\nalternative. In this context, we introduce MiniCPM, specifically the 1.2B and\n2.4B non-embedding parameter variants, not only excel in their respective\ncategories but also demonstrate capabilities on par with 7B-13B LLMs.\nWhile focusing on SLMs, our approach exhibits scalability in both model\nand data dimensions for future LLM research. Regarding model scaling, we\nemploy extensive model wind tunnel experiments for stable and optimal\nscaling. For data scaling, we introduce a Warmup-Stable-Decay (WSD)\nlearning rate scheduler (LRS), conducive to continuous training and domain\nadaptation. We present an in-depth analysis of the intriguing training\ndynamics that occurred in the WSD LRS. With WSD LRS, we are now able\nto efficiently study data-model scaling law without extensive retraining\nexperiments on both axes of model and data, from which we derive the\nmuch higher compute optimal data-model ratio than Chinchilla Optimal.\nAdditionally, we introduce MiniCPM family, including MiniCPM-DPO,\nMiniCPM-MoE and MiniCPM-128K, whose excellent performance further\ncementing MiniCPM\u2019s foundation in diverse SLM applications. MiniCPM\nmodels are available publicly1.\n1 Introduction\nFollowing the revelation of the scaling law (Kaplan et al., 2020), there has been a vigorous\npursuit in the field of Large Language Models (LLMs) (Hoffmann et al., 2022; Bai et al., 2023;\nGemini et al., 2023; Chowdhery et al., 2023; Achiam et al., 2023), encompassing models with\nup to an astonishing number of parameters in the trillions (Fedus et al., 2022). These models\nhave emerged as a pivotal driving force in the evolution of artificial intelligence.\nNonetheless, the training of such large-scale models is both financially burdensome and\noperationally inefficient. On one hand, the empirical understanding of the mechanisms\nunderpinning the training of LLMs remains elusive. Given the significant economic and\nenvironmental costs, experiments on LLMs are prohibitively expensive for most researchers\nand corporations. On the other hand, the deployment of these colossal models in everyday\nscenarios, such as on personal computers or smartphones, is either inefficient or unfeasible.\n\u2217Corresponding Authors.\n1https://github.com/OpenBMB/MiniCPM\n1arXiv:2404.06395v3  [cs.CL]  3 Jun 2024Both aspects underscore the imperative to refocus efforts on comprehensively exploring\nsmaller, yet potent, language models (SLMs). These models on the one hand provide efficient\nsolutions to practical deployment, on the other hand, if trained with scalable strategies, they\ncan potentially guide the development of future larger models.\nRecently, a resurgence of interest has been observed in the domain of SLMs, evidenced by\nthe advent of a series of innovative models such as the Phi series (Gunasekar et al., 2023; Li\net al., 2023b; Javaheripi & Bubeck, 2023), TinyLlama (Zhang et al., 2024a), MobileLLM (Liu\net al., 2024), and Gemma (Banks & Warkentin, 2024), among others. While these models have\nsignificantly contributed to the expansion and diversification of the SLM landscape, there\nremain two pivotal areas where these models have yet to fully satisfy prevailing interests:\n(1) the development of comprehensive abilities akin to those exhibited by LLMs; and (2) the\nformulation of transparent and scalable training methodologies that could further propel\nthe evolution of both SLMs and LLMs.\nIn this paper, we introduce MiniCPM, a series of SLMs, which primarily builds on two\nmodels, endowed with 2.4B and 1.2B non-embedding parameters respectively, and they\nrank preeminently in their respective 2B and 1B scale categories. MiniCPM also exhibits\ncomparable capabilities to those of 7B \u223c13B language models, such as Llama2-7B (Touvron\net al., 2023), Mistral-7B (Jiang et al., 2023), Gemma-7B (Banks & Warkentin, 2024), and\nLlama-13B (Touvron et al., 2023), etc. Notwithstanding their small model sizes, our training\nmethodology is meticulously designed to facilitate seamless scaling of both model scale\nand data horizons. This is exemplified through our model wind tunnel experiments that\nencompass comprehensive hyper-parameter optimization (Section 3), and the deployment\nof a WSD (Warmup-Stable-Decay) learning rate scheduler (Section 4). The latter is tailored\nfor continuous training with an un-predefined pre-training token number and makes the\nreusing of model intermediate checkpoints highly feasible. A detailed analysis of the training\ndynamics of MiniCPM is presented, suggesting that the WSD scheduler demonstrates the\nintriguing loss landscape of model pre-training. With the WSD scheduler, we are now also\ncapable of studying the data-model scaling law with linear effort on the model axis and a\nnegligible effort on the data axis, while the traditional ones need quadratic effort considering\nthe scaling along both model and data axes. The result of the scaling law indicates a much\nhigher data size/model size ratio compared with Chinchilla Optimal (Hoffmann et al.,\n2022).\nMoreover, we introduce the MiniCPM family, including MiniCPM-DPO, MiniCPM-128K,\nand MiniCPM-MoE. We conduct evaluations of the MiniCPM family against established\nbenchmarks and illuminate their impressive capabilities as SLMs: (1) The foundation models\nsurpass Mistral-7B, and LLama-13B. (2) The DPO model surpasses zephyr-7B (Tunstall et al.,\n2023) on MTBench (Zheng et al., 2024) (3) The 2.4B MiniCPM-128K model demonstrates\nperformance either surpassing or matching that of models like Yarn-Mistral-7B-128K (Peng\net al., 2023) and ChatGLM3-6B-128K (Du et al., 2021). (4) The MiniCPM-MoE, with 4B\nactivated parameters, is on par with Llama2-34B (Touvron et al., 2023).\nIn summary, MiniCPM propounds a new stage in the development of small language\nmodels, exemplifying the latent potential within SLMs and advocating for a more scientific\nand sustainable approach toward scaling up LLMs.\n2 Related Work\nSmall Language Models. \u201cSmall Language Models\u201d (SLMs) is an evolving concept that has\nundergone significant transformations over time. Presently, SLMs are generally construed as\nmodels that are smaller in scale compared to the well-known LLMs, typically not exceeding\n7 billion parameters. These models are distinguished by their capacity for deployment\non end-user devices, such as personal computers and smartphones, even in the absence\nof a GPU. Notable examples within the current landscape of SLMs include the Phi series\n(Gunasekar et al., 2023; Li et al., 2023b; Javaheripi & Bubeck, 2023), TinyLlama (Zhang\net al., 2024a), MobileLLM (Liu et al., 2024), and Gemma (Banks & Warkentin, 2024), etc.\nA variety of methodologies have been explored to augment the efficacy of SLMs. These\ninclude the incorporation of high-quality data (Gunasekar et al., 2023; Li et al., 2023b;\nJavaheripi & Bubeck, 2023), the application of structure pruning techniques (Xia et al., 2023),\nand the reconfiguration of model architectures (Liu et al., 2024), among others. MiniCPM\n2enhances the capabilities of SLMs through a meticulous amalgamation of hyper-parameter\noptimization, strategic training methodologies, architectural design, and high-quality data.\nScalable Pre-training Strategies. Since the discovery of scaling law (Kaplan et al., 2020; Rae\net al., 2021; Aghajanyan et al., 2023), scientifically and predictably (Achiam et al., 2023; Hu\net al., 2023; Du et al., 2024) scaling up the LLMs has been pursued from diverse perspectives,\nespecially for the pre-training stage. In terms of training stability, the Tensor Program\nseries (Yang et al., 2022; 2023) is introduced to ensure optimal hyper-parameter consistency\nacross varying model scales, a technique employed in training CerebrasGPT (Dey et al.,\n2023). Furthermore, Wortsman et al. (2023) suggest leveraging smaller models to anticipate\nand mitigate instabilities in larger model training. From the training data standpoint,\nvarious data-centric strategies have been advocated (Xie et al., 2024; Shi et al., 2023; Ye\net al., 2024). In the realm of training methodologies, prior research has delved into diverse\nlearning rate schedulers (LRS) (Howard & Ruder, 2018; Raffel et al., 2020; Hundt et al., 2019),\nwith the Cosine LRS (Loshchilov & Hutter, 2016) emerging as the predominant choice in\nLLMs. Kaplan et al. (2020) and Hoffmann et al. (2022) have meticulously examined the\nhyper-parameters of Cosine LRS, thereby laying a foundational groundwork for subsequent\npre-training works. Of these, DeepSeek (Bi et al., 2024) bears the closest resemblance to our\nproposed WSD LRS. Concerning batch size scheduling, Smith et al. (2017) advocates for\nincrementing batch size as an alternative to diminishing learning rate, a strategy recently\nadopted by Yi-9B (Young et al., 2024).\n3 Model Wind T unnel Experiments\nAlthough we target at training SLMs that can be quickly deployed onto end devices, we\nenvision that many aspects of model training are universal across scales. Extensive experi-\nments should be conducted through an SLM to explore the limit of SLM before transferring\nthe experience into LLMs. These experiments take the spirit of wind tunnel testing in\ndeveloping an aircraft, thus we name it Model Wind Tunnel Experiments (MWTE). In this\npaper, the MWTE contains three parts: (1) Hyper-parameters; (2) Optimal Batch-size Scaling;\nand (3) Optimal Learning Rate Stability.\n3.1 Scaling Hyper-parameters Invariant LM\nHyper-parameters have a significant impact on the performance of a model. However,\nadjusting hyper-parameters for each model in traditional training is not feasible for LLMs.\nEven for SLM like MiniCPM, extensive experiments on hyper-parameters search take a lot\nof resources. Tensor Program (Yang et al., 2022; 2023) proposes a framework to stabilize the\nhyper-parameters for models with different scales. The main part of the Tensor Program\nis the width scaling (Yang et al., 2022) and the depth scaling (Yang et al., 2023). The\nformer technique supports CerebrasGPT (Dey et al., 2023) to predict the loss of LLMs more\naccurately. In MiniCPM, we use both two scaling techniques. The specific scaling operations\nare listed in Table 7. We do not apply the attention softmax scaling techniques (Yang\net al., 2022). Despite Yang et al. (2023) observing that depth scaling for a network with\nblock depth larger than two is not satisfying, we find the resulting optimal learning rate is\nstable empirically. Details of the hyper-parameters and Tensor Program Operations are in\nAppendix A.1.\n3.2 Optimal Batch Size\nBatch size determines the balance between the convergence speed of the model and the\nconsumption of computational resources. If the batch size is too large, it will result in a\nsignificant amount of data and computation costs. On the other hand, if the batch size\nis too small, it will require a large number of training steps and may result in a limited\ndecrease in the loss function. We follow Kaplan et al. (2020) to determine the batchsize from\nexpected loss, with a slight modification from their setting (see Appendix A.2). We conduct\nexperiments on 0.009B, 0.03B, and 0.17B models, respectively, toward this goal. Each model\nsize is trained on 6 batch sizes with a global learning rate of 0.01 and cosine learning rate\n3scheduler. We observe the trend of the optimal batch size with loss on the C4 (Raffel et al.,\n2019) dataset (red line in the Figure 1).\nFigure 1: We demonstrate the loss curve of three size models\ntrained using different batch sizes. Each vertical line formed\nby points with a gradient color represents a training curve.\nLighter colors denote higher loss.\nFigure 2: The connected op-\ntimal batch sizes.\nAs shown in Figure 1, we plot the batch size in the x-axis, and token consumption in the\ny-axis, the color of the points represents a loss. Thus a horizontal line formed by the color\npoints denotes a training curve. we use parabolas to fit the equal-loss points and connect\nthe minima of the parabolas with red lines. The lines demonstrate the optimal batch size\nshifts large as the loss decreases. We then connect the three lines (see Figure 2) and find that\nthe lines connect each other well into a linear relationship in the log space, from which we\nobtain the following relationship between batch size bsand C4 Loss L:bs=1.21\u00d7109\nL6.24 . We\nnote that it might seem strange that the batch size should be estimated from a rough loss\nprediction that we can only have after training. We provide our comment in Appendix A.2.\n3.3 Optimal Learning Rate\nDue to our use of Tensor Program (Yang et al., 2022; 2023), we anticipate that the learning\nrate, will not undergo significant changes during model scaling. To verify this, we conduct\nsix sets of learning rate experiments at 0.04B, 0.1B, 0.3B, and 0.5B. In Figure 3, we find that\nalthough the model size has increased by ten times, the optimal base learning rate2does not\nshow a noticeable shift and remains around 0.01. We further conduct a simple validation on\na scale of 2.1B and confirm that a learning rate of 0.01 indeed achieves the lowest loss.\nFigure 3: Loss vs Learning Rate. After ap-\nplying for the Tensor Program, the learn-\ning rate shift becomes minimal.\n20N 40N 60N 80N 100N 120N 140N 160N\nTokens (B)3.63.84.04.24.4Loss on C4Cosine(20N)\nCosine(40N)\nCosine(60N)\nCosine(80N)\nCosineLoop(20N)Figure 4: Cosine Learning Rate Scheduler\nwith different periods. The Y-axis is the\nloss on the C4 corpus.\n4 WSD Learning Rate Scheduler\n4.1 Analysing Cosine LRS\nThe learning rate scheduler (LRS), which adjusts the learning rate used in different stages\nof training, is crucial for model performance. The current commonly used learning rate\n2The actual learning rate of 2-D tensors will be scaled according to Tensor Program.\n4strategy is the Cosine LRS (Kaplan et al., 2020; Hoffmann et al., 2022; Rae et al., 2021; Touvron\net al., 2023; Bai et al., 2023; Almazrouei et al., 2023), which gradually decreases the learning\nrate following a cosine curve after it reaches its maximum after the warmup stage.\nA key hyper-parameter in the Cosine LRS is the step Tat which Cosine decreases to the\nminimum for the first time. Typically, Tis set to the total training step Sfor training with a\npredefined training step. Generally, it is believed that the learning rate should be high to\nenable sufficient exploration. For example, Kaplan et al. (2020) demonstrate that the loss\ndecreases when the summed learning rate over the entire training increases (see Figure 22\nin their paper). This indicates setting T<Sis not optimal. On the other hand, Hoffmann\net al. (2022) make a key observation that setting T>Sresults in dropped performance\nwhile setting S=Tresults in improved training efficiency, confirming that the learning\nrate shouldn\u2019t be kept high throughout the training. To reproduce these observations,\nwe conduct experiments on the 0.036B model. We try Cosine (T)and CosineLoop (T)LRS,\nfollowing the formula shown in Appendix B.1. The result can be seen in Figure 4. We can see\nthat when the training step is S=20N, 40N, 60N, 80N, the lowest loss is always achieved\nby the Cosine (T)where T=S. Both T<Sand T>Sare not optimal.\nWe hypothesize that the Cosine LR performs exceptionally well when T=Sbecause of the\nfollowing two reasons: (1) Cosine LRS with T=Shas a longer duration of high learning\nratetraining compared to T<Sand other LRS such as Linear LRS. This high learning rate\nmight help the model find a better global optimum. (2) Cosine LRS with T=Shas a more\nthorough learning rate decay phase compared to Cosine LRS with T>Sand Constant LRS.\nThis learning rate decay may involve unique training dynamics that enable the model to\nfind a better local optimum.\n4.2 WSD LRS\nIn light of the above perspective, we propose to explicitly divide the training stage into the\nhigh learning rate stage and learning rate decay stage. We name it as the Warmup-Stable-\nDecay (WSD) LRS. Especially, the WSD LRS contains three stages: the warmup stage (whose\nend step is denoted by W), the stable training stage (whose end step is denoted by T), and\nthe remaining decay stage. The function form of WSD is:\nWSD (T;s) =\uf8f1\n\uf8f2\n\uf8f3s\nW\u03b7,s<W\n\u03b7,W<s<T\nf(s\u2212T)\u03b7,T<s<S(1)\nwhere 0 <f(s\u2212T)\u22641 is a decreasing function about s,\u03b7is the maximum learning rate.\nTypically, as long as the warmup stage is enough, it affects little performance, therefore, we\nomit Win the subsequent discussion. With an abuse of notation, we will denote WSD with\na clear stop point\n20N 40N 60N 80N 100N 120N 140N\nTokens (B)3.63.84.04.24.4Loss on C4WSD(40N,2N)\nWSD(60N,2N)\nWSD(80N,2N)\nWSD(40N,4N)\nWSD(60N,6N)\nWSD(80N,8N)\nCosine(80N)\nFigure 5: Model training loss has a sudden\ndecrease in the decay stage of WSD LRS.\n0.0 0.5 1.0 1.5 2.0 2.5\nCompute\u00d710193.43.63.84.04.24.4Loss on C4\n3.32WSD(20N,2N)\nWSD(40N,4N)\nWSD(60N,6N)\nWSD(80N,8N)\nWSD(160N,16N)\nWSD(320N,32N)\n0.17B, WSD(40N,4N)\nOptimal Loss EnvelopeFigure 6: Continous training a 0.036B model\ncan match the performance of 0.17B model\nwith an acceptable increase in training com-\npute.\n54.3 Experiments\nNext, we present several experimental findings of WSD LRS.\nLoss Decreases Dramatically in Decay Stage. We try WSD LRS on 0.036B models. As\nshown in Figure 5, in the decay stage, as the learning rate begins to decrease, the loss\nexperiences a significant rapid decline and quickly decreases to be equal to or lower than\nthe Cosine LRS at step T=S. At the same time, we can reuse the model before decay and\ncontinue training with the previous high learning rate. After more steps of training S\u2032, we\ncan also perform annealing to achieve the same loss as the Cosine LRS at Cosine (S\u2032). This\nverifies our assumption that the training stage can be explicitly split into the stable training\nand decay stages.\n10% Steps are Enough. From the two-stage training perspective, shortening the decay stage\nwill greatly benefit the fast test of different model checkpoints of stable training. Therefore,\nwe conduct experiments that start from the same stable training checkpoints and have\ndifferent decay steps. Also shown in Figure 5, among all three stable training checkpoints in\n40N, 60N, and 80N training data, having a decay of 10% of the total tokens is sufficient to\nachieve the best results, while a decay of 2.5% of total tokens falls short. Therefore, in the\nsubsequent training experiments, we use a decay of about 10% to ensure full convergence.\nEffective Data Scaling with WSD LRS. With WSD LRS, we can continuously train the LM\nto extreme convergence. To further demonstrate the potential of training a fixed-sized model\nto convergence, we compare continuously training a 0.036B LM with a 0.17B model with\n40N data. In Figure 6, the green lines represent 0.036B models trained with different stable\ntraining tokens. Despite the last point of the 0.036B series being trained with many more\ntokens than Chinchilla Optimal (Hoffmann et al., 2022), it still has space for performance\nimprovement.\nTo find the limit of continuously training this fixed-sized LM, we estimate how the model\u2019s\noptimal performance changes with its computation during continuous training. By optimal\nperformance, we mean the loss of training token Dis achieved by WSD (D, 0.1D). With a\nseries of D, the losses will form the optimal loss envelope. Due to uncertainty about the\nfunction form of the loss envelope, we try two fitting formulas: (1) exponential: L(C) =\n\u03b1e\u2212\u03b2C+L0and (2) power-law: L(C) =\u03b2C\u2212\u03b1+L0. The fitting results for both functions\nare in Appendix B.2. We find that the power-law form fits better (similar to the Cosine\nLRS (Kaplan et al., 2020)). In Figure 6, the fitted curve is shown in green dotted lines. To\nintuitively estimate and comprehend the effect of continuous training such a fixed-sized\nmodel, we also trained a 0.17B model with WSD (40N, 4N), which is shown in pink in\nFigure 6. We can see that a 0.036B model can match the performance of a 0.17B model\nwith an acceptable increase ( \u223c4 times) in training compute while saving a lot of inference\ncomputation (Sardana & Frankle, 2023) (saving \u223c5 times per inference call), indicating a\nbetter inference-compute-optimal setting (Sardana & Frankle, 2023).\n4.4 Analysis of the Decay Stage\nIn this section, we provide a brief analysis of the loss drop in the decay stage, examining\nit through the prisms of checkpoint updates and gradient information. We calculate the\nmaximum weight element update max ij(W(t+1)\nij\u2212W(t)\nij)across all weight matrices in the\nMiniCPM-2.4B (Introduced in Section 6). As depicted in Figure 7, the updates exhibit a\nrobust correlation with the learning rate\u2019s magnitude. Notwithstanding the illustration of\nthe two submodules (gate proj and q proj module of the 25th layer), this pattern is prevalent\nacross every layer and submodule within the network. This observation may not be trivial:\nthe model checkpoints experience significant updates preceding the learning rate\u2019s decay,\nyet the loss exhibits minimal reduction. Conversely, during the decay stage, despite less\npronounced weight alterations, there is an accelerated decrease in loss.\nFurther examination of the gradient data is undertaken by training a 0.2B model, meticu-\nlously recording every step gradient information, and evaluating the differences between\nconsecutive steps, thereby providing an approximation of second-order gradient infor-\n60 100 200 300 400 500 600\nCheckpoint Index0.000.020.040.060.080.100.120.14Max Element-wise Difference of Weights between Checkpoints\ngate_proj of the 25th Layer in MiniCPM-2.4B\n0 100 200 300 400 500 600\nCheckpoint Index0.000.020.040.060.080.100.120.14Max Element-wise Difference of Weights between Checkpoints\nq_proj of the 25th Layer in MiniCPM-2.4BFigure 7: Max Difference of Checkpoints.\nmation. We treat the gradient at step tas a flattened vector g(t), and the parameter (also\nflattened as a vector x(t)) update between step tand t+1 is v(t)=x(t+1)\u2212x(t). The\ngradient norm take the L2 norm of the gradient \u2225g(t)\u2225, gradient inner product is g(t+1)\u00b7g(t),\nthe cosine of the gradient\u2019s angle is given byg(t+1)\u00b7g(t)\n\u2225g(t+1)\u2225\u2225g(t)\u2225. Imaging the optimization process\nas a trajectory over a high-dimension manifold, first order directional derivative along the\ntrajectory is computed as D1=g(t+1)\u00b7v(t)\n\u2225v(t)\u2225, and the second order directional derivative is\nD2=(g(t+1)\u2212g(t))\u00b7v(t)\n\u2225v(t)\u22252.D1,D2enables an approximate estimation of the loss curvature on the\ntrajectory, K=|D2|\n(1+D2\n1)3\n2. The results of these statistics over time are shown in Figure 8. We\ncan see that the gradient norm diminishes during the decay phase, and upon commence-\nment of this stage, the cosine between gradients predominantly assumes positive values,\nsuggesting that in the decay phase, model parameters undergo consistent changes across\nsteps. Concerning directional derivatives, it is remarkable that the first-order directional\nderivative diminishes exponentially with each step, aligning closely with the learning rate,\nwhile the second-order directional derivative exhibits a slight increase in magnitude. The\ncurvature of the loss function also increases by a magnitude, indicating the proximity to a\nlocal optimum. These findings potentially offer a deeper insight into the shape optimization\nspace, a subject reserved for future exploration.\n0 2000 4000 6000 8000 10000\nIteration0.00.10.20.30.40.5maximum gradient\n0 2000 4000 6000 8000 10000\nIteration0.00.20.40.60.81.0gradient norm\n0 2000 4000 6000 8000 10000\nIteration\u22121.0\u22120.50.00.51.0cosine value of gradient\u2019s angle\n0 2000 4000 6000 8000 10000\nIteration\u22120.010\u22120.0050.0000.0050.0102nd order directional derivative\n0 2000 4000 6000 8000 10000\nIteration10\u2212310\u2212210\u22121100negative 1st order directional derivative (log scale)\n0 2000 4000 6000 8000 10000\nIteration10\u2212410\u2212210\u22121loss curvature on parameter trajectory (log scale)\nFigure 8: Gradient statistics over the training of a 0.2B model using WSD LRS. The exponen-\ntial decay stage begins at 8000 steps.\n4.5 Measuring the Scaling Law with WSD LRS\nScaling laws serve as a fundamental guiding principle in the development of LLMs. Al-\nthough these scaling laws exhibit variability in specific coefficients due to diverse configura-\ntions across model series, the compute optimal data-to-model ratio remains a meaningful\n7metric across different scaling law functions, which \u201cmarginalizes\u201c out the specific value\nof loss. Regarding this ratio, Kaplan et al. (2020) posit that a tenfold increase in model\nscale should equate to a singlefold increase in data scale. Conversely, Hoffmann et al.\n(2022) argue for the same scaling rate between model size and data size. What\u2019s more,\ncurrent models such as LLama 2 (Touvron et al., 2023), train much more data than what\nHoffmann et al. (2022) claims, still yielding considerable performance gain. Indicating a\nhigher data-to-model ratio.\nThis unaddressed uncertainty stems from the challenges inherent in training multiple\nmodels of varying sizes and data sizes in traditional scaling experiments. Previously, if the\naverage cost of training one model size on one data size is C, then conducting the scaling\nexperiments with mmodel sizes and mdata sizes takes approximately O(m2)C.\nIn this section, we introduce the utilization of the WSD scheduler as an effective approach\nto explore the scaling law with linear cost ( O(mC)). Since the WSD scheduler has the\nadvantage of arriving at the optimal loss of Cosine LRS after decaying from stable stage\ncheckpoints of any step, we are now able to precisely measure the optimal scaling properties\nwithout re-training the models from scratch to different amounts of tokens, thus making the\nscaling law measurement much more efficient along the data axis.\nWe measure the scaling law along the data and model axes by training SLMs of 6 sizes\nranging from 0.04B to 2B, each with 6 decayed models starting from the checkpoint of\n10Nto 60 Ndata during the stable training stage. The final loss is evaluated on five held-\nout evaluation datasets. To potentially compare the loss when the model uses different\ntokenizers, we take the average of loss by a number of bytes instead of a number of tokens,\nfollowing Achiam et al. (2023). The final loss of each pair of data size and model size is\nshown in the blue lines in Figure 17.\nThen we fit the losses with model size Nand data size Dfollowing Hoffmann et al. (2022)\nusing scipy curvefit function:\nL(N,D) =CNN\u2212\u03b1+CDD\u2212\u03b2+L0 (2)\nThe fitted curve along the data axis for each dataset and each checkpoint are shown in\norange lines in Figure 17. Then we have the optimal model size Nopt, dataset size Dopt,\ngiven a fixed amount of compute C=6ND (Rae et al., 2021) as:\nNopt\nDopt=K2\u0012C\n6\u0013\u03b7\n, (3)\nwhere K= (\u03b1CN\n\u03b2CD)1\n\u03b1+\u03b2, and \u03b7=\u03b2\u2212\u03b1\n\u03b1+\u03b2. The derivation of Noptclosely follows Hoffmann et al.\n(2022) by substituting DwithC\n6Nin Equation 2, and minimize L(N)given C. A similar way\nis adopted for Dopt. From Equation 3, when \u03b1=\u03b2,Nopt/Doptis a constant, supporting\nHoffmann et al. (2022)\u2019s claim, and when \u03b1<\u03b2, we should emphasize more on parameter\nscaling (Kaplan et al., 2020), and vise versa.\nIn our experiments, the fitted relationship between loss and N,Dis shown in the contour\nplot of equal loss in Figure 10. The equation of fitted scaling law is shown in the first text box\nin each subplot. We can see that in all evaluation corpora, we have \u03b2<\u03b1. More specifically,\non average, we have \u03b1=0.29, \u03b2=0.23, K2=0.01, \u03b7=\u22120.10. Since \u03b1is slightly larger than\n\u03b2, this result shows that as the computation scale, we should slightly emphasize more on\ndata scaling than model scaling, which aligns with Hoffmann et al. (2022).\nAs for the concrete data-to-model ratioDopt\nNopt, we notice that there is a huge gap in compute\noptimal regime between ours and Hoffmann et al. (2022) despite that the trend ofDopt\nNopt\u2019 with\ncompute Cis aligned between ours and theirs. Specifically, the data size should be 192 times\nlarger than the model size on average, as opposed to 20 times in Hoffmann et al. (2022). We\nnote that this aligns with the observation in Section 4.3 and Figure 6.\n8With respect to the large deviation from Chinchilla OptimalNopt\nDopt, we notice that their scaling\nexperiment was conducted in a not very recent configuration. To compare with more recent\nconfiguration such as Llama2 (Touvron et al., 2023), we extract the training loss data from\nLlama2 paper (left part) in Appendix Figure 18 and estimate the compute optimalDopt\nNoptin\ntheir paper using the right part of Figure 18. Since they use Cosine LRS, the loss is not\noptimal in the middle of the training, depicted by the concave curve during training in\nthe right figure of Figure 18. We fill the concave part with a straight line to estimate the\noptimal loss envelope if they had used the WSD LRS. After that, the compute model size\nshould roughly be the regime in which a model\u2019s loss curve is about to intersect with a\nlarger model\u2019s loss curve. With this intuition, the 13B model is about to intersect with the\n34B model at 105EFlops (1018 Flops), and the 34B model is about to intersect with the 70B\nmodel at 5 \u00d7105EFlops. Therefore, we estimate theDopt\nNoptto be roughly5\u00d7105\n6\u00d7342\u223c105\n6\u00d7132, which\nis 70\u223c100. Therefore, under this approximate comparison, their data-model ratio is closer\nto ours. And our configuration can absorb more data into a smaller model compared to\nprevious ones. However, we note that the above estimates are only a rough one.\nA larger data-to-model ratio means that we can absorb more data into a smaller model than\nwe previously thought, which is more efficient for inference and deployment. We hope WSD\nLRS will help more researchers explore L(N,D)with less effort and make the relationship\nclearer in LLMs.\n5 Two Stage Pre-training Strategy\nTypically, the training of instruction following LLMs contains the pre-training stage and\nthe supervised fine-tuning (SFT) stage (Zhang et al., 2023; Wei et al., 2021). In the pre-\ntraining stage, the data is composed of large-scale unlabeled data, while in the SFT stage,\nhigh-quality labeled data becomes the optimization target. In light of the pronounced loss\ndecrease observed during the decay stage of the WSD LRS, we postulate that the integration\nof high-quality labeled data in this phase presents dual advantages:\n\u2022Introducing this data during the annealing phase, in addition to the SFT stage,\nfosters a more comprehensive model learning. Specifically, it facilitates a more\npronounced loss reduction in relation to the SFT data distribution, rather than the\npre-training data distribution. This approach is more congruent with actual user\nscenarios.\n\u2022In contrast to a uniform distribution of high-quality data throughout the entire\npre-training process, this method enhances training by concentrating on data and\nsustaining continuous pre-training. If we do not predetermine a training step, we\nwill repeat a small dataset throughout an ongoing pre-training process, which could\nlead to negative effects.\nBased on these two hypotheses, we propose the following training strategy: during the pre-\ntraining phase, only use large-scale coarse-quality pre-training data, which is abundant and\ncan support continuous training when provided with more computational resources. During\nthe annealing phase, we use diverse and high-quality knowledge and ability-oriented SFT\ndata, mixed into the pre-training data.\nTo validate the advantages of our training strategy, we conduct comparison experiments\nusing (A) MiniCPM-2.4B\u2019s intermediate checkpoint in the stable stage; and (B) MiniCPM-\n1.2B\u2019s last checkpoints in the stable stage. Specifically, we compare the following:\n1. A-1: 2.4B model, decay using only pre-training data, followed by 4B token SFT.\n2.A-2: 2.4B model, decay using the aforementioned high-quality data unlabeled data\nand SFT data mixed into pre-training data, also followed by 4B token SFT.\n3. B-1: 1.2B model, decay using only pre-training data, followed by 6B token SFT.\n4. B-2: 1.2B model, decay using only pre-training data, followed by 12B token SFT.\n910\u221211001011021033\u00d710\u221214\u00d710\u221216\u00d710\u22121Code\n10\u221211001011021035\u00d710\u221216\u00d710\u221217\u00d710\u221218\u00d710\u22121English (Wikihow)\n10\u22121100101102103100\n6\u00d710\u221217\u00d710\u221218\u00d710\u221219\u00d710\u22121Chinese (Wikihow)\n10\u22121100101102103\nCompute4\u00d710\u221216\u00d710\u22121Ultratext\n10\u22121100101102103\nCompute100\n7\u00d710\u221218\u00d710\u221219\u00d710\u22121Chinese (Yayi Corpus)\n10\u22121100101102103\nCompute5\u00d710\u221216\u00d710\u221217\u00d710\u221218\u00d710\u221219\u00d710\u22121Average\n10\u2212210\u22121100101102103104105106100\n2\u00d710\u221213\u00d710\u221214\u00d710\u221216\u00d710\u22121Code\n10\u2212210\u22121100101102103104105106100\n4\u00d710\u221216\u00d710\u22121English (Wikihow)\n10\u2212210\u22121100101102103104105106100\n6\u00d710\u22121Chinese (Wikihow)\n10\u2212210\u22121100101102103104105106\nCompute100\n3\u00d710\u221214\u00d710\u221216\u00d710\u22121Ultratext\n10\u2212210\u22121100101102103104105106\nCompute100\n6\u00d710\u22121Chinese (Yayi Corpus)\n10\u2212210\u22121100101102103104105106\nCompute100\n4\u00d710\u221216\u00d710\u22121Average0.031B\n0.11B\n0.25B\n0.49B\n0.85B\n2.0BReal Loss w.r.t. Compute\nFitted Loss w.r.t. ComputeFigure 9: The result of scaling experiments with WSD Scheduler (above) and the fitted\nscaling curve (below). The x-axis is the computation Flops C=6ND, each color of the\nline represents the same model with different computation Flops. We can see that smaller\nmodels are better than larger models when the Flops are small and worse when the Flops\nare large. Thus models of different sizes will intersect with each other in the plot around the\ncompute optimal regime.\n5.B-3: 1.2B model, annealing using the aforementioned high-quality data + SFT data\nmixed into pre-training data, also followed by 6B token SFT.\nThe results of the experiments are shown in Table 1. We can see that, despite the A-2 and\nA-1 have undergone the same SFT distribution, adding SFT data to the decay stage pushes\nthe boundary . Comparison between B-2 and B-3 demonstrate that the deficiency of only\nSFT is not due to the insufficient training tokens in SFT stage.\nThe results indicate that the benefits of introducing high-quality data at the beginning of the\ndecay stage are much higher than simply adding it during the SFT phase. Therefore, we\nrecommend that specialization and enhancement of model capabilities should start from\nthe decay phase.\n6 Model\nIn this section, we begin to introduce the MiniCPM model that aggregates the aforemen-\ntioned observations and techniques.\n1010\u2212110010110\u22121100101102103104Compute (1018Flops)\n3.32\u00d710\u22122\nN0.37+2.17\u00d710\u22121\nD0.34+ 0.17\nK2= 0.01\u03b7=\u22120.05\nDopt\nNopt/vextendsingle/vextendsingle\nC=1021= 194.93Code\n10\u2212110010110\u22121100101102103104\n8.08\u00d710\u22122\nN0.26+2.97\u00d710\u22121\nD0.18+ 0.27\nK2= 0.02\u03b7=\u22120.18\nDopt\nNopt/vextendsingle/vextendsingle\nC=1021= 166.04English (Wikihow)\n10\u2212110010110\u22121100101102103104\n5.14\u00d710\u22122\nN0.35+3.90\u00d710\u22121\nD0.18+ 0.40\nK2= 0.01\u03b7=\u22120.33\nDopt\nNopt/vextendsingle/vextendsingle\nC=1021= 833.95Chinese (Wikihow)\n10\u22121100101\nNon-embedding Parameters (109)10\u22121100101102103104Compute (1018Flops)\n7.54\u00d710\u22122\nN0.30+2.92\u00d710\u22121\nD0.30+ 0.25\nK2= 0.01\u03b7=\u22120.00\nDopt\nNopt/vextendsingle/vextendsingle\nC=1021= 95.60Ultratext\n10\u22121100101\nNon-embedding Parameters (109)10\u22121100101102103104\n1.53\u00d710\u22121\nN0.19+3.76\u00d710\u22121\nD0.17+ 0.35\nK2= 0.01\u03b7=\u22120.05\nDopt\nNopt/vextendsingle/vextendsingle\nC=1021= 112.79Chinese (Yayi Corpus)\n10\u22121100101\nNon-embedding Parameters (109)10\u22121100101102103104\n7.15\u00d710\u22122\nN0.29+3.00\u00d710\u22121\nD0.23+ 0.31\nK2= 0.01\u03b7=\u22120.10\nDopt\nNopt/vextendsingle/vextendsingle\nC=1021= 191.87Average\n0.220.300.430.600.851.201.682.373.34\n0.420.490.580.690.810.961.141.341.58\n0.550.650.770.901.061.251.471.732.04\n0.340.450.600.801.071.431.902.533.37\n0.600.690.800.921.061.231.411.631.88\n0.430.530.650.800.981.201.481.812.23Figure 10: The fit result of the scaling experiment with WSD Scheduler. The black dots in a\nhorizontal line denote the decayed checkpoints in different compute within the same model\nsize.\nC-Eval CMMLU MMLU GSM8K MATH HumanEval MBPP\nA-1 40.0 41.5 44.6 27.7 5.1 27.7 24.4\nA-2 52.6 51.1 50.9 42.3 5.4 30.4 30.3\nB-1 40.9 41.5 47.9 34.2 7.9 43.9 30.5\nB-2 41.2 42.0 47.9 34.4 7.3 43.9 29.8\nB-3 49.1 46.8 49.6 31.8 10.5 44.5 32.8\nTable 1: The ablation study of different training strategies.\n6.1 Model Details\nVocabulary. We use two tokenizers of 122,753 vocabulary size for MiniCPM-2.4B and\n73,440 vocabulary for MiniCPM-1.2B. A small vocabulary for 1.2B favors efficiency without\nharming much performance. Details of the tokenizers are in Appendix C. Including the\nembedding parameters increases total parameters by 0.3B and 0.2B respectively.\nShared Input-output Layer. For SLM, the embedding takes up a large parameter space. To\nmake the model parameters smaller, we use the Embedding Sharing techniques for both\nMiniCPM-2.4B and MiniCPM-1.2B.\nDeep-and-thin Network. We train MiniCPM-2.4B before training MiniCPM-1.2B. When\ntraining MiniCPM-2.4B, we adopt a deeper and thinner architecture compared to Phi-\nModel N (B) dm df f dhnqnkv L Batch size (M) Tokens (T)\nMiniCPM-1.2B 1,247,442,432 1,536 3,840 64 24 8 52 2M \u21924M 1.1T\nMiniCPM-2.4B 2,442,057,984 2,304 5,760 64 36 36 40 4M 1.1T\nTable 2: Model configurations for MiniCPM. N (B), dm,df f,dh,nq,nkv,L, Batch size (M),\nTokens (T) represents the number of non-embedding parameters of the model, model hidden\ndimension, feedforward layer bottleneck dimension, attention head dimension, number of\nqueries, number key/values, number of layers, training batch size, total training tokens.\n112 (Javaheripi & Bubeck, 2023) (40 layers compared to 32 layers). Recently, Liu et al. (2024)\npropose to train deep and thin networks for SLMs, which aligns with our perspective.\nTherefore, we further make the architecture deeper and thinner for MiniCPM-1.2B.\nGroup Query Attention. We train MiniCPM-2.4B without modification on the attention\nlayer. Whereas we apply Group Query Attention (Ainslie et al., 2023) to MiniCPM-1.2B,\ninspired by Liu et al. (2024), to further reduce the parameters number.\n6.2 Training Stages\nThe overall training of the MiniCPM base model includes three stages: stable training stage,\ndecay stage, SFT stage (Zhang et al., 2023; Wei et al., 2021). Throughout the stages, we use\nAdam Optimizer (Kingma & Ba, 2014).\nStable Training Stage. We utilize around 1T data (see Section 11 for data distribution),\nwith the majority of the data sourced from open datasets. We use the optimal configuration\ndiscovered during the model wind tunnel experiments, WSD LRS, with a batch size of 3.93\nmillion and a max learning rate of 0.01.\nDecay Stage. We use a mixture of the pretraining data and high-quality SFT data. For\nthe specific annealing form of the WSD scheduler, we employ exponential annealing, i.e.\nf(s\u2212T) =0.5(s\u2212S)/T, in which Tis set to be 5000 steps (20B tokens).\nSFT Stage. We find it still necessary to conduct a separate SFT phase. We utilize SFT data\nsimilar to the annealing phase excluding pre-training data and train with approximately 6\nbillion tokens. The learning rate for SFT is aligned with the one at the end of annealing, and\na WSD Scheduler with exponential decay is also employed.\n6.3 Training Data Distribution\nCode Pretrain\n25.0CommonCrawl Chn\n25.0\nDolma24.0\nC415.0Pile8.0peS2o1.0 Arxiv1.0 Open Web Math 1.0Data Mixture of Stable Stage\nCode Pretrain\n19.6Dolma\n15.7\nCommonCrawl Chn\n14.6\nC49.5\nWikipedia6.7\nPile5.1\nSFT mixed4.8\nBaidu Baike4.5\nCode SFT3.8\nBook Chinese2.8\nKnowledge SFT2.5\nUltraChat2.0\nMath Synthetic1.9\nOther1.3\nStack Exchange QA1.1\nMath SFTpeS2oArxivOpen Web MathLaw PretrainShareGPT4Logic SFTSimOrcaOssInstructEvolInstructData Mixture of Decay Stage\nFigure 11: Data mixture of different training stages. The stable stage is shown on the left\nand the decay stage is shown on the right.\nWe introduce our training data distribution in Figure 11. In the figure, CommonCrawl Chn\nin a Chinese Corpus is derived from CommonCrawl raw corpus and goes through thorough\ncleaning. Dolma (Soldaini et al., 2024), C4 (Raffel et al., 2019), and Pile (Gao et al., 2020;\nBiderman et al., 2022) are English corpora. They are deduplicated inner corpus and across\ncorpus using MinHash algorithms (Broder, 1997). The Code Pre-train data contains the\nstack (Kocetkov et al., 2022) and StarCoder Li et al. (2023a), with inner deduplication and\ncross deduplication. In the decay stage, the data mixture contains more diverse data and\nproprietary data, including UltraChat (Ding et al., 2023), SlimOrca (Lian et al., 2023a;b),\nOssInstruct (Wei et al., 2023), EvolInstruct (Xu et al., 2023). The data with the suffix SFT is\nour proprietary data including LeetCode questions, Kindergarten through 12th grade (K12)\ntextbooks and questions, etc.\n126.4 Training Loss\nThe overall training loss on the C4 dataset is shown in Figure 12. We can see that as expected\nin the preliminary experiments, the loss decreases sharply in the decay stage. Since we use\nthe exponential decay, the loss still drops after the learning rate drops below 10% of the max\nlearning rate. However, since we continue to SFT the model after the decay stage, we do\nnot utilize the final checkpoints. The checkpoints we finetune from are shown in the last\ncheckpoint of dark green segment. The first drop in MiniCPM-1.2B is the result of enlarging\nbatch size, which might have a similar effect as decreasing learning rate (Smith et al., 2017).\n0 200 400 600 800 1000 1200\nTokens (B)2.52.62.72.82.93.0Loss on C4\n0 200 400 600 800 1000 1200\nTokens (B)2.42.52.62.72.82.93.0Loss on C4\nFigure 12: Loss curve on C4 dataset for MiniCPM-1.2B (Left) and MiniCPM-2.4B (Right).\nThe orange segment at the tail of the loss curve represents the remaining decay process,\nwhich is not utilized in the released version of MiniCPM.\n6.5 Evaluation\nThe overall evaluation utilizes our open-source tool UltraEval3. UltraEval is an open-source\nframework for assessing the capabilities of foundation models. It provides a lightweight and\nuser-friendly evaluation system, supporting performance assessment for mainstream large\nmodels, and catering to the rapid evaluation needs of model training teams. The underlying\ninference and acceleration use the open-source framework vLLM (Kwon et al., 2023), and\nthe dataset includes commonly used datasets: MMLU (Hendrycks et al., 2020) for English\nknowledge, CMMLU (Li et al., 2024) and C-Eval (Huang et al., 2024) for Chinese knowledge,\nHumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for coding, GSM8K (Cobbe\net al., 2021) and MATH (Hendrycks et al., 2021) for mathematics, and HellaSwag (Zellers\net al., 2019), ARC-e (Clark et al., 2018), ARC-c (Clark et al., 2018) for commonsense reasoning,\nand BBH (Suzgun et al., 2022) for logic reasoning.\nDue to the difficulty of standardizing evaluations for large models and the lack of publicly\navailable prompts and test codes for many models\u2019 evaluations, we try our best to adapt the\nevaluation methods to suit various model types. Specifically, we start from a standardized\ninput prompt during testing and adjust it according to each model\u2019s appropriate input-\noutput template. The evaluation scripts and prompts are also open-source in our repository,\nand we welcome developers to continually improve our evaluation methods.\nWhen testing QA tasks (ARC-e, ARC-c, HellaSwag), two approaches are typically employed.\nThe first involves using Perplexity (PPL): we extend each option as the continuation of\nthe question and use the PPL of the option as the selection criterion. The second is direct\ngeneration, where the model directly outputs answer options. We observe significant\ndifferences in results obtained using these two methods. MiniCPM performs similarly in\ndirect generation and PPL tests, with better performance in direct generation. On the other\nhand, Mistral-7B-v0.1 performs better in PPL tests but exhibits poorer performance in direct\ngeneration. To address this phenomenon, when reporting the scores for each model, we\nadopt the score from the evaluation method that yields the highest score, ensuring fairness\nin comparison.\n3https://ultraeval.openbmb.cn/home\n13Model C-Eval CMMLU MMLU HumanEval MBPP GSM8K MATH\nLlama2-7B 32.42 31.11 44.32 12.20 27.17 13.57 1.80\nQwen-7B 58.96 60.35 57.65 17.07 42.15 41.24 5.34\nDeepseek-7B 42.82 44.45 47.82 20.12 41.45 15.85 1.53\nMistral-7B 46.12 42.96 62.69 27.44 45.20 33.13 5.00\nGemma-7B 42.57 44.20 60.83 38.41 50.12 47.31 6.18\nLlama2-13B 37.32 37.06 54.71 17.07 32.55 21.15 2.25\nMPT-30B 29.34 32.09 46.56 21.95 35.36 10.31 1.56\nFalcon-40B 40.29 41.57 53.53 24.39 36.53 22.44 1.92\nTinyLlama-1.1B 25.02 24.03 24.3 6.71 19.91 2.27 0.74\nQwen-1.8B 49.81 45.32 43.37 7.93 17.8 19.26 2.42\nQwen1.5-1.8B 55.00 50.85 43.81 5.49 24.82 26.16 3.25\nGemini Nano-3B - - - - 27.20 22.80 -\nStableLM-Zephyr-3B 30.34 30.89 45.90 35.37 31.85 52.54 12.12\nPhi-2(2B) 23.37 24.18 52.66 47.56 55.04 57.16 3.50\nGemma-2B 29.26 28.56 38.49 24.39 29.74 16.83 3.34\nMiniCPM-1.2B 49.14 46.81 49.63 44.51 32.75 31.77 10.60\nMiniCPM-2.4B 51.13 51.07 53.46 50.00 47.31 53.83 10.24\nModel BBH ARC-e ARC-c HellaSwag Avg Avg en Avg chn\nLlama2-7B 33.23 75.25\u202042.75 75.62\u202035.40 36.21 31.77\nQwen-7B 37.75 83.42 64.76 75.32\u202049.46 47.19 59.66\nDeepseek-7B 33.38 74.58\u202042.15\u202075.45\u202039.96 39.15 43.64\nMistral-7B 41.06 83.92 70.73 80.43\u202048.97 49.96 44.54\nGemma-7B 39.19 89.35 76.79 79.47 52.22 54.18 43.39\nLlama2-13B 37.92 78.87\u202058.19 79.23\u202041.48 42.44 37.19\nMPT-30B 38.22 78.66\u202046.08\u202079.72\u202038.17 39.82 30.72\nFalcon-40B 36.24 81.94\u202057.68 83.26\u202043.62 44.21 40.93\nTinyLlama-1.1B 28.78 60.77\u202028.15\u202058.33\u202025.36 25.55 24.53\nQwen-1.8B 29.07 63.97\u202043.69 59.28\u202034.72 31.87 47.57\nQwen1.5-1.8B 28.82 64.86 45.56 59.39 37.09 33.57 52.93\nGemini Nano-3B 42.40 - - - - - -\nStableLM-Zephyr-3B 37.68 73.78 55.38 71.87\u202043.46 46.32 30.62\nPhi-2(2B) 43.39 86.11 71.25 73.07\u202048.84 54.42 23.78\nGemma-2B 30.93 74.33 40.70 69.51 35.10 36.47 28.91\nMiniCPM-1.2B 34.70 80.93 66.81 54.72 45.67 45.16 47.98\nMiniCPM-2.4B 36.87 85.44 68.00 68.25 52.33 52.60 51.10\nTable 3: Benchmark Score of MiniCPM-2.4B and MiniCPM-1.2B (both without RLHF). The\ntwo tables are continuous horizontally. Avg is over all dataset in the table, Avg chnis the\naverage of C-Eval and CMMLU while Avg enis the average of remaining datasets. \u2020means\nthe result is tested using PPL metrics. Bold numbers represent the best score among the\nSLMs. Results of Gemini Nano-3B are borrowed from Gemini et al. (2023).\n14The overall evaluation results are in Table 4. Overall, on the mentioned datasets, we have\nseveral observations. (1) On average, MiniCPM-2.4B ranks the highest among all the SLMs.\n(2) MiniCPM-2.4B performs similarly to Mistral-7B-v0.1 in English but significantly out-\nperforms Mistral-7B-v0.1 in Chinese. (3) MiniCPM-2.4B outperforms Llama2-13B except\nin MMLU, BBH, and HellaSwag, while MiniCPM-1.2B outperforms Llama2-7B except\nin HellaSwag. (4)Generally, BBH is harder for SLMs than LLMs compared to another\nknowledge-oriented dataset, demonstrating that reasoning ability might be more depen-\ndent on model size than knowledge. (5) Among SLMs, Phi-2 performance is on par with\nMiniCPM on academic-oriented datasets. This might be because their training data mostly\ninvolves textbook-style data that emphasize educational and academic scenarios. Since our\npre-training data covers more distribution, we think MiniCPM is better at knowledge and\nability coverage, which can be seen in Appendix F.\n7 MiniCPM Family\nIn this section, we introduce the other models that build on MiniCPM base models. Specif-\nically, we trained the aligned model, long-context model, and MoE model for MiniCPM\n2.4B.\n7.1 MiniCPM-DPO\nAfter SFT, we employ DPO (Rafailov et al., 2024) for human preference alignment of the\nmodel. During this stage, UltraFeedback (Cui et al., 2023) is utilized as the primary align-\nment dataset, and a proprietary preference dataset is constructed to enhance the model\u2019s\ncode and mathematical capabilities. We conduct one epoch of DPO training with a learning\nrate of 1 \u00d710\u22125and utilize a Cosine LRS since we have a pre-defined training step.\nAfter applying DPO for preference alignment, the model\u2019s score on MTBench (Zheng et al.,\n2024) increased from 6.89 after SFT to 7.25, surpassing even large models such as Llama2-\n70B-Chat (see Figure 13). However, we also noticed that the performance on benchmarks is\nslightly compromised, which is known as the alignment tax (Askell et al., 2021).\nModel C-Eval CMMLU MMLU HumanEval MBPP GSM8K MATH\nChatGLM2-6B 52.05 49.21 45.77 10.37 9.38 22.74 5.96\nMistral-7B-Instruct-v0.1 38.06 36.96 53.56 29.27 39.34 28.73 3.48\nMistral-7B-Instruct-v0.2 42.55 41.92 60.51 36.59 48.95 40.49 4.95\nQwen-7B-Chat 58.57 57.23 56.03 15.85 40.52 42.23 8.3\nYi-6B-Chat 70.88 71.11 62.95 14.02 28.34 36.54 3.88\nBaichuan2-7B-Chat 53.28 53.50 53.00 21.34 32.32 25.25 6.32\nDeepseek-7B-chat 46.95 49.72 51.67 40.85 48.48 48.52 4.26\nLlama2-7B-Chat 34.54 32.64 47.64 14.02 27.40 21.15 2.08\nMiniCPM-2.4B-DPO 48.64 48.37 53.05 51.22 48.01 53.37 9.86\nModel BBH ARC-e ARC-c HellaSwag Avg Avg en Avg chn\nChatGLM2-6B 32.60 74.45 56.82 58.48\u202037.98 35.17 50.63\nMistral-7B-Instruct-v0.1 39.52 81.61 63.99 73.47\u202044.36 45.89 37.51\nMistral-7B-Instruct-v0.2 39.81 86.28 73.38 84.55\u202050.91 52.83 42.24\nQwen-7B-Chat 37.34 64.44\u202039.25\u202074.52\u202044.93 42.05 57.90\nYi-6B-Chat 37.43 84.89 70.39 74.60\u202050.46 45.89 71.00\nBaichuan2-7B-Chat 37.46 79.63 60.15 69.23\u202044.68 42.74 53.39\nDeepseek-7B-chat 35.70 76.85 63.05 76.68\u202049.34 49.56 48.34\nLlama2-7B-Chat 35.54 74.28 54.78 75.65\u202038.16 39.17 33.59\nMiniCPM-2.4B-DPO 36.22 85.02 68.17 65.67 51.60 52.29 48.51\nTable 4: Benchmark scores for MiniCPM-2.4B-DPO compared with larger chat models.\n15Figure 13: MTBench score of MiniCPM-DPO-2.4B surpasses many models of larger size.\n7.2 MiniCPM-128K\nTasks involving lengthy contexts depend on the implicit information within these contexts,\ncircumventing the need for the extensive knowledge often absent in SLMs. In this section,\nwe expand the context length of MiniCPM-2.4B from 4,096 to 128,000 tokens, illustrating the\ncapability of SLM to effectively process long contexts.\nInitialization. For the initialization, we disable sharing embeddings between input and\noutput, primarily to accommodate vocabulary parallelism essential for training with long\ncontext. The LM head is initialized from the input embedding.\nTraining. Similar to MiniCPM, MiniCPM-2.4B-128K utilizes the WSD as its learning rate\nscheduler and reuses the last checkpoint of the stable training stage of MiniCPM-2.4B.\nConcerning training data, we categorize the dataset distribution detailed in Section 6.3\ninto \u201cshort data\u201d and \u201clong data\u201d. We classify books, wikis, and papers as \u201clong data\u201d,\nand the other as the \u201cshort data\u201d. The training comprises 44% long data and 56% short\ndata for continued training. For the extension of long contexts, we apply Adjusted Base\nFrequency (ABF) (Xiong et al., 2023) in the 4K to 32k range and employ NTK-Aware RoPE\nScaling (bloc97, 2023) and curriculum learning from 32K to 128K. Both two stages involve\nfuture training. Furthermore, as indicated in Yi Tech Report (Young et al., 2024) and\nZebra (Song et al., 2023), we use of synthetic long QA data that significantly enhances model\nperformance in context-aware tasks.\nEvaluation. We evaluate MiniCPM-2.4B-128K in \u221eBench (Zhang et al., 2024b), a pioneering\nbenchmark for long context evaluations. The tasks in \u221eBench (Zhang et al., 2024b) extend\nbeyond typical retrieval tasks and challenge the model with long context reasoning. We can\nsee in Table 5, we achieve comparable results in Mistral-7B-Instruct-v0.2 (ABF1000w) and\noutperform ChatGLM3-6B-128K despite being 2.5 times smaller.\n7.3 MiniCPM-MoE\nWe further extend the ability of MiniCPM using Mixture-of-Expert.\nInitialization. MiniCPM-MoE is initialized utilizing Sparse Upcycling (Komatsuzaki et al.,\n2022). The dense model checkpoint, derived from the stable phase of MiniCPM, undergoes\na transformation wherein each MLP layer is substituted by an MoE layer. These new MoE\nlayers are exact replicas of the original MLP layers from the dense checkpoint. The router\nparameters are randomly initialized following a normal distribution with a mean of 0 and a\nvariance of 0.01.\nRouting Mechanism. The number of total non-embedding parameters of MiniCPM-MoE is\n13.6B. During training and inference, two out of eight experts are activated for each token,\n16Model PasskeyNumber\nStringKV Re-\ntrievalLong\nBook\nChoice\nEngLong\nBook\nQA\nChnLong\nBook\nQA\nEngLong\nBook\nSum\nEng\nLWM-Text-128K 100 97.8 0.6 28.82 15.93 14.31 9.99\nYarn-Mistral-7b-128K 92.71 56.61 0 27.95 15.49 9.55 9.06\nMistral-7B-Instruct-\nv0.2(ABF 1000w)100 78.98 3.6 37.12 11.74 17.37 21.12\nYi-6B-200K 100 94.92 0 36.68 15.07 9.2 0.92\nChatGLM3-6B-128K 89.93 99.66 5.2 46.29 10.7 8.38 25.91\nMiniCPM-2.4B-128K 98.31 99.83 9 29.69 23.06 16.33 15.73\nModelLong Di-\nalogue\nQA EngMath\nCalcMath\nFindCode\nDebugCode\nRunAvgAvg\nw/o\nCode &\nMath\nLWM-Text-128k 1.5 0 3.43 20.05 1 24.45 33.62\nYarn-Mistral-7b-128k 7.5 0 17.14 0.76 1.25 19.84 27.36\nMistral-7B-Instruct-\nv0.2(ABF 1000w)9.5 0 29.43 17.51 0 27.75 36.9\nYi-6B-200K 3.5 0 4.29 0.51 0.75 22.15 32.54\nChatGLM3-6B-128K 6.5 0 8 5.33 1 25.58 36.57\nMiniCPM-2.4B-128K 9.5 0 4.29 22.08 0 27.32 37.68\nTable 5: MiniCPM-2.4B-128K result in \u221eBench (Zhang et al., 2024b)\nresulting in the number of activated parameters being approximately 4B. To prevent training\nfrom collapsing, an additional load balancing loss (Fedus et al., 2022) is applied to the final\ntraining objective. This auxiliary loss is multiplied by 0.01 which is large enough to ensure a\nbalanced distribution of tokens assigned to different experts.\nTraining. Similar to MiniCPM, we employ WSD as our learning rate scheduler. Regarding\nthe training data, we adhere strictly to the distribution specified in Section 6.3. The training\nbatch size is maintained at 4M tokens during the stable training and decay stages and is\nreduced to 2M tokens during the SFT stage. The pre-training phase (including continue pre-\ntrain and decay stage) spans 130K steps, after which we notice diminishing improvement.\nThe benchmark results are detailed in Table 6.\nModel C-Eval CMMLU MMLU HumanEval MBPP GSM8K MATH BBH\nLlama2-34B - - 62.6 22.6 33.0\u202042.2 6.24 44.1\nDeepseek-MoE (16B) 40.6 42.5 45.0 26.8 39.2 18.8 4.3 -\nMistral-7B 46.12 42.96 62.69 27.44 45.20 33.13 5.0 41.06\nGemma-7B 42.57 44.20 60.83 38.41 50.12 47.31 6.18 39.19\nMiniCPM-2.4B 51.13 51.07 53.46 50.00 47.31 53.83 10.24 36.87\nMiniCPM-MoE (13.6B) 58.11 58.80 58.90 56.71 51.05 61.56 10.52 39.22\nTable 6: Benchmark results of MiniCPM-MoE.\u2020means evaluation results on the full set\nof MBPP , instead of the hand-verified set (Austin et al., 2021). The evaluation results of\nLlama2-34B and Qwen1.5-7B are taken from their technical reports.\n8 Conclusion\nThis paper introduces MiniCPM, comprising two SLMs with 2.4 B and 1.2 B non-embedding\nparameters, respectively. These models demonstrate superior performance compared to\ntheir larger counterparts. Our training methodologies are scalable both in terms of model\nand data size, offering potential applicability in the development of LLMs. The introduction\nof our WSD scheduler is notable for promoting continuous training, exhibiting compelling\ntraining dynamics, and enabling efficient study of scaling law. We further introduce the\nMiniCPM family, including DPO, long context, and MoE versions. Future directions include\nin-depth analysis of the loss decrease in the decay stage, and enhancing the capability of\nMiniCPM by scaling in both model size and data size.\n17Author Contributions\nAll authors contribute substantially to the MiniCPM project. Shengding Hu lead and\nparticipated in all aspects of the projects. This included the scaling experiments (conducted\nalongside Yuge Tu), babysitting the training of MiniCPM base models, and contributing\nto various other parts of the research. Shengding Hu wrote the paper. Chaoqun He was\nresponsible for evaluating MiniCPM, while Ganqu Cui handled the RLHF training. Xiang\nLong, Zhi Zheng, Xinrong Zhang and Shengding Hu extended the context window to\n128K. The MoE research was conducted by Yewei Fang and Zhi Zheng. Weilin Zhao and\nKaihuo Zhang contributed to the training and inference infrastructure. The open-sourcing of\nMiniCPM was prepared by Yuxiang Huang and Shengding Hu. Shengding Hu, along with\nChenyang Zhao, also provided analysis on the WSD scheduler\u2019s training dynamics. Zheng\nLeng Thai developed the tokenizer. The development of MiniCPM-V was carried out by\nChongyi Wang and Yuan Yao. The training corpus of MiniCPM was prepared by Jie Zhou, Jie\nCai, Shengding Hu, Zhi Zheng, and Zhongwu Zhai. The paper was proofread by Xingrong\nZhang and Chaoqun He. Insightful instructions on training MiniCPM were provided by\nXu Han, Ning Ding, and Zhiyuan Liu. Finally, Zhiyuan Liu, Maosong Sun, Guoyang Zeng,\nChao Jia, and Dahai Li offered essential resources for the training of MiniCPM.\nLimitations\nAlthough we have proposed a thorough study of the scaling law with SLMs, this paper\ndoes not extend to training an LLM to validate the scaling law. The application of WSD\nLRS on LLMs has not been fully explored to date. However, we remain optimistic about its\npotential advantages.\nAcknowledgement\nMiniCPM was initially published as a technical blog on February 1st, 2024. Since then, we\nhave received numerous insightful feedback from the community, significantly contributing\nto the development of this paper. We extend our gratitude to Chunting Zhou and Armen\nAghajanyan for their valuable discussions. Special thanks go to Peiqin Sun and Yan Wang for\ntheir meticulous feedback on clarifying ambiguities in the blog. Additionally, we appreciate\nthe open-source community\u2019s efforts in integrating MiniCPM into inference frameworks\nlike llama.cpp, etc.\n18References\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report. arXiv preprint arXiv:2303.08774 , 2023.\nArmen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan,\nSusan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling\nlaws for generative mixed-modal language models. In International Conference on Machine\nLearning , pp. 265\u2013279. PMLR, 2023.\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron,\nand Sumit Sanghai. GQA: Training generalized multi-query transformer models from\nmulti-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing , pp. 4895\u20134901,\nSingapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/\n2023.emnlp-main.298. URL https://aclanthology.org/2023.emnlp-main.298 .\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxan-\ndra Cojocaru, M \u00b4erouane Debbah, \u00b4Etienne Goffinet, Daniel Hesslow, Julien Launay,\nQuentin Malartic, et al. The falcon series of open language models. arXiv preprint\narXiv:2311.16867 , 2023.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as\na laboratory for alignment. arXiv preprint arXiv:2112.00861 , 2021.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program\nsynthesis with large language models, 2021.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\nGe, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 ,\n2023.\nJeanine Banks and Tris Warkentin. Gemma: Introducing new state-of-the-art open models.\nhttps://blog.google/technology/developers/gemma-open-models/ , 2024. Accessed:\ndate-of-access.\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui\nDing, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language\nmodels with longtermism. arXiv preprint arXiv:2401.02954 , 2024.\nStella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. arXiv preprint\narXiv:2201.07311 , 2022.\nbloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended\n(8k+) context size without any fine-tuning and minimal perplexity degrada-\ntion. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_\nrope_allows_llama_models_to_have/ , 2023. Accessed: [Insert Date of Access].\nAndrei Z Broder. On the resemblance and containment of documents. In Proceedings.\nCompression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171) , pp. 21\u201329. IEEE,\n1997.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evalu-\nating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\net al. Palm: Scaling language modeling with pathways. Journal of Machine Learning\nResearch , 24(240):1\u2013113, 2023.\n19Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv preprint arXiv:1803.05457 , 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers\nto solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,\nZhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-\nquality feedback, 2023.\nNolan Dey, Gurpreet Gosal, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin\nTom, Joel Hestness, et al. Cerebras-gpt: Open compute-optimal language models trained\non the cerebras wafer-scale cluster. arXiv preprint arXiv:2304.03208 , 2023.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu,\nMaosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality\ninstructional conversations, 2023.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\nGlm: General language model pretraining with autoregressive blank infilling. arXiv\npreprint arXiv:2103.10360 , 2021.\nZhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities\nof language models from the loss perspective. arXiv preprint arXiv:2403.15796 , 2024.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\nparameter models with simple and efficient sparsity. Journal of Machine Learning Research ,\n23(120):1\u201339, 2022.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: accurate post-\ntraining quantization for generative pre-trained transformers. CoRR , abs/2210.17323, 2022.\ndoi: 10.48550/ARXIV .2210.17323. URL https://doi.org/10.48550/arXiv.2210.17323 .\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of\ndiverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.\nTeam Gemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui\nYu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family\nof highly capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C \u00b4esar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi,\net al. Textbooks are all you need. arXiv preprint arXiv:2306.11644 , 2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\narXiv:2009.03300 , 2020.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang,\nDawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the\nmath dataset. arXiv preprint arXiv:2103.03874 , 2021.\nAlex Henry, Prudhvi Raj Dachapally, Shubham Shantaram Pawar, and Yuxuan Chen. Query-\nkey normalization for transformers. In Trevor Cohn, Yulan He, and Yang Liu (eds.),\nFindings of the Association for Computational Linguistics: EMNLP 2020 , pp. 4246\u20134253, Online,\nNovember 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nfindings-emnlp.379. URL https://aclanthology.org/2020.findings-emnlp.379 .\n20Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\net al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 ,\n2022.\nJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text\nclassification. arXiv preprint arXiv:1801.06146 , 2018.\nShengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin,\nNing Ding, Zebin Ou, Guoyang Zeng, et al. Unlock predictable scaling from emergent\nabilities. arXiv preprint arXiv:2310.03262 , 2023.\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng\nLiu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline\nchinese evaluation suite for foundation models. Advances in Neural Information Processing\nSystems , 36, 2024.\nAndrew Hundt, Varun Jain, and Gregory D Hager. sharpdarts: Faster and more accurate\ndifferentiable architecture search. arXiv preprint arXiv:1903.09900 , 2019.\nMojan Javaheripi and S \u00b4ebastien Bubeck. Phi-2: The surprising power of\nsmall language models. https://www.microsoft.com/en-us/research/blog/\nphi-2-the-surprising-power-of-small-language-models/ , 2023. Accessed: date-of-\naccess.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural\nlanguage models. arXiv preprint arXiv:2001.08361 , 2020.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980 , 2014.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu \u02dcnoz Fer-\nrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau,\nLeandro von Werra, and Harm de Vries. The stack: 3 tb of permissively licensed source\ncode. Preprint , 2022.\nAran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa,\nJoshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training\nmixture-of-experts from dense checkpoints. arXiv preprint arXiv:2212.05055 , 2022.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\nJoseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large\nlanguage model serving with pagedattention. In Proceedings of the 29th Symposium on\nOperating Systems Principles , pp. 611\u2013626, 2023.\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and\nTimothy Baldwin. Cmmlu: Measuring massive multitask language understanding in\nchinese, 2024.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-\nhao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the\nsource be with you! arXiv preprint arXiv:2305.06161 , 2023a.\nYuanzhi Li, S \u00b4ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat\nLee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 ,\n2023b.\n21Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong,\nand \u201dTeknium\u201d. Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with\nverification, 2023a. URL https://https://huggingface.co/Open-Orca/SlimOrca .\nWing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong,\n\u201dTeknium\u201d, and Nathan Hoos. Slimorca dedup: A deduplicated subset of slimorca, 2023b.\nURL https://huggingface.co/datasets/Open-Orca/SlimOrca-Dedup/ .\nZechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov,\nYunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mo-\nbilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv\npreprint arXiv:2402.14905 , 2024.\nIlya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts.\narXiv preprint arXiv:1608.03983 , 2016.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 , 2017.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context\nwindow extension of large language models. arXiv preprint arXiv:2309.00071 , 2023.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis\nSong, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling\nlanguage models: Methods, analysis & insights from training gopher. arXiv preprint\narXiv:2112.11446 , 2021.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel. Advances in Neural Information Processing Systems , 36, 2024.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. arXiv e-prints , 2019.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of machine learning research , 21(140):1\u201367, 2020.\nNikhil Sardana and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference\nin language model scaling laws. arXiv preprint arXiv:2401.00448 , 2023.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\nwords with subword units. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp.\n1715\u20131725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi:\n10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162 .\nWeijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A\nSmith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. In-context pretraining: Language\nmodeling beyond document boundaries. arXiv preprint arXiv:2310.10638 , 2023.\nSamuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don\u2019t decay the\nlearning rate, increase the batch size. arXiv preprint arXiv:1711.00489 , 2017.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell\nAuthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann,\nAnanya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson,\nJacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Pe-\nters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant\nSubramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh\nHajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: An Open\nCorpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint ,\n2024. URL https://arxiv.org/abs/2402.00159 .\n22Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, and Dong Yu. Zebra: Ex-\ntending context window with layerwise grouped local-global attention. arXiv preprint\narXiv:2312.08618 , 2023.\nMirac Suzgun, Nathan Scales, Nathanael Sch \u00a8arli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V . Le, Ed H. Chi, Denny Zhou, and Jason Wei.\nChallenging big-bench tasks and whether chain-of-thought can solve them, 2022.\nLLMFarm team. LLMFarm, 2023a. URL https://github.com/guinmoon/LLMFarm .\nMLC team. MLC-LLM, 2023b. URL https://github.com/mlc-ai/mlc-llm .\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth \u00b4ee Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 ,\n2023.\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes\nBelkada, Shengyi Huang, Leandro von Werra, Cl \u00b4ementine Fourrier, Nathan Habib,\nNathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr:\nDirect distillation of lm alignment, 2023.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners.\narXiv preprint arXiv:2109.01652 , 2021.\nYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source\ncode is all you need. arXiv preprint arXiv:2312.02120 , 2023.\nMitchell Wortsman, Peter J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John D\nCo-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et al. Small-scale proxies for\nlarge-scale transformer training instabilities. arXiv preprint arXiv:2309.14322 , 2023.\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating\nlanguage model pre-training via structured pruning. arXiv preprint arXiv:2310.06694 , 2023.\nSang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S\nLiang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures\nspeeds up language model pretraining. Advances in Neural Information Processing Systems ,\n36, 2024.\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis\nMartin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective\nlong-context scaling of foundation models. arXiv preprint arXiv:2309.16039 , 2023.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao,\nand Daxin Jiang. Wizardlm: Empowering large language models to follow complex\ninstructions. arXiv preprint arXiv:2304.12244 , 2023.\nGreg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick\nRyder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large\nneural networks via zero-shot hyperparameter transfer. arXiv preprint arXiv:2203.03466 ,\n2022.\nGreg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning\nin infinite-depth neural networks. arXiv preprint arXiv:2310.02244 , 2023.\nJiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu. Data\nmixing laws: Optimizing data mixtures by predicting language modeling performance.\narXiv preprint arXiv:2403.16952 , 2024.\nAlex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li,\nJiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai.\narXiv preprint arXiv:2403.04652 , 2024.\n23Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source\nsmall language model. arXiv preprint arXiv:2401.02385 , 2024a.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li,\nRunyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A\nsurvey. arXiv preprint arXiv:2308.10792 , 2023.\nXinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao,\nXu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. \u221ebench: Extending long context\nevaluation beyond 100k tokens. arXiv preprint arXiv:2402.13718 , 2024b.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with\nmt-bench and chatbot arena. Advances in Neural Information Processing Systems , 36, 2024.\n24A Additional Results in Model Wind T unnel Experiments\nA.1 \u00b5P hyper-parameter search\nWe conduct an extensive Bayesian search over a set of predefined parametric spaces. For effi-\nciency, we search for the N=0.009 Bmodel. In our pilot experiments, we confirm that when\nhyper-parameter optimization is conducted using datasets scaled at magnitudes of 10N\nand 20N, there is a consistency observed in the efficacy of hyper-parameters. Therefore, we\ntrain the models with |D|=10N=0.09Btokens. Meanwhile, we also try QK-Norm (Henry\net al., 2020) and independent weight decay (Loshchilov & Hutter, 2017) as well to stabilize\nthe learning rate. The overall results are shown in Figure 14. After applying the QK-norm,\nwe observe a significant decrease in the learning rate sensitivity similar to Wortsman et al.\n(2023). However, as the MiniCPM project itself is an SLM, we do not require low learning\nrate sensitivity as long as we find the best learning rate with TensorProgram (Yang et al.,\n2022; 2023). Therefore, we do not introduce QK-norm and independent weight decay in\nlater experiments of MiniCPM. In Figure 14, we identify the best hyper-parameters for\nscale depth =1.4,scale emb=12,initstd=0.1, and lr=0.01.\n/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b\n/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000042/uni00000047/uni00000048/uni00000053/uni00000057/uni0000004b/uni00000018/uni00000011/uni00000013/uni00000018/uni00000011/uni00000018/uni00000019/uni00000011/uni00000013/uni00000019/uni00000011/uni00000018/uni0000001a/uni00000011/uni00000013/uni0000004f/uni00000052/uni00000056/uni00000056/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000054/uni0000004e/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000e/uni00000003/uni00000047/uni00000048/uni00000046/uni00000052/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000047/uni00000010/uni00000047/uni00000048/uni00000046/uni00000044/uni0000005c\n/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000054/uni0000004e/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000e/uni00000003/uni00000047/uni00000048/uni00000046/uni00000052/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000047/uni00000010/uni00000047/uni00000048/uni00000046/uni00000044/uni0000005c\n/uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni0000001a/uni00000011/uni00000018 /uni00000015/uni00000013/uni00000011/uni00000013\n/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000042/uni00000048/uni00000050/uni00000045/uni00000018/uni00000011/uni00000013/uni00000018/uni00000011/uni00000018/uni00000019/uni00000011/uni00000013/uni00000019/uni00000011/uni00000018/uni0000001a/uni00000011/uni00000013/uni0000004f/uni00000052/uni00000056/uni00000056/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000054/uni0000004e/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000e/uni00000003/uni00000047/uni00000048/uni00000046/uni00000052/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000047/uni00000010/uni00000047/uni00000048/uni00000046/uni00000044/uni0000005c\n/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000054/uni0000004e/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000e/uni00000003/uni00000047/uni00000048/uni00000046/uni00000052/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000047/uni00000010/uni00000047/uni00000048/uni00000046/uni00000044/uni0000005c\n/uni00000013/uni00000011/uni00000013/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018/uni00000013 /uni00000013/uni00000011/uni00000013/uni0000001a/uni00000018 /uni00000013/uni00000011/uni00000014/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000014/uni00000015/uni00000018 /uni00000013/uni00000011/uni00000014/uni00000018/uni00000013 /uni00000013/uni00000011/uni00000014/uni0000001a/uni00000018 /uni00000013/uni00000011/uni00000015/uni00000013/uni00000013\n/uni0000004c/uni00000051/uni0000004c/uni00000057/uni00000042/uni00000056/uni00000057/uni00000047/uni00000018/uni00000011/uni00000013/uni00000018/uni00000011/uni00000018/uni00000019/uni00000011/uni00000013/uni00000019/uni00000011/uni00000018/uni0000001a/uni00000011/uni00000013/uni0000004f/uni00000052/uni00000056/uni00000056/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000054/uni0000004e/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000e/uni00000003/uni00000047/uni00000048/uni00000046/uni00000052/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000047/uni00000010/uni00000047/uni00000048/uni00000046/uni00000044/uni0000005c\n/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000054/uni0000004e/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000e/uni00000003/uni00000047/uni00000048/uni00000046/uni00000052/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000047/uni00000010/uni00000047/uni00000048/uni00000046/uni00000044/uni0000005c\n/uni00000013/uni00000011/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000015 /uni00000013/uni00000011/uni00000013/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000019 /uni00000013/uni00000011/uni00000013/uni0000001b /uni00000013/uni00000011/uni00000014/uni00000013\n/uni0000004f/uni00000055/uni00000018/uni00000011/uni00000013/uni00000018/uni00000011/uni00000018/uni00000019/uni00000011/uni00000013/uni00000019/uni00000011/uni00000018/uni0000001a/uni00000011/uni00000013/uni0000004f/uni00000052/uni00000056/uni00000056/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000052/uni00000058/uni00000057/uni00000003/uni00000054/uni0000004e/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000e/uni00000003/uni00000047/uni00000048/uni00000046/uni00000052/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000047/uni00000010/uni00000047/uni00000048/uni00000046/uni00000044/uni0000005c\n/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000054/uni0000004e/uni00000010/uni00000051/uni00000052/uni00000055/uni00000050/uni00000003/uni0000000e/uni00000003/uni00000047/uni00000048/uni00000046/uni00000052/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000047/uni00000010/uni00000047/uni00000048/uni00000046/uni00000044/uni0000005c/uni00000013/uni00000011/uni00000013/uni00000013/uni0000001c/uni00000045/uni00000003/uni0000005a/uni0000004c/uni00000057/uni0000004b/uni00000003/uni00000014/uni00000013/uni00000031/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\nFigure 14: Grid search over the \u00b5P parameterization spaces.\nName Specific Operation\nEmbedding Output Scaling Multiply the output of the embedding by scale emb\nResidual Connection Scaling Scale the output tensor of a block before adding\nto each residual connection in each layer by\nscale depth /p\nnum layers\nInitialization of Tensors Set the initialization standard deviation of each two-\ndimensional tensor parameter to initstd/\u221adm/dbase,\nand set other parameters\u2019 initialization to 0.1\nLearning Rate Scaling of Tensors Adjust the learning rate of each two-dimensional\ntensor parameter to 1 /(dm/dbase)times the learning\nrate of other parts (or the overall learning rate)\nLM Head Scaling Adjust the output logits to 1 /(dm/dbase)times the\noriginal value\nTable 7: List of operations used when applying tensor program techniques.\n25A.2 Comment on Optimal Batchsize\nIn Kaplan et al. (2020), OpenAI studies the relationship between the loss function and\nthe number of tokens. In their experiments, they assume that consuming more steps is\nequivalent to consuming more time. Under this assumption, OpenAI defines a critical\nbatch size that achieves a certain loss without consuming too many steps or tokens. This\nrationale is valid if the experiments are provided with unlimited GPUs (at least within\nthe scope of the experiments). Since GPUs are unlimited, enlarging batch size will not\nincrease the single-step duration but will decrease the total number of steps. However, in\nour experiment, since we have a fixed resource (number of GPUs), we observe that doubling\nthe batch size almost equals doubling the single-step time. Therefore, enlarging batch size\nto decrease total training steps has minimal effect on the total training time. In light of\nthis observation, we drop the goal of \u201cnot consuming too many steps\u201d and turn towards\nminimizing the token quantity to achieve the lowest loss, instead.\nThe observation regarding the estimation of optimal batch size in relation to loss resem-\nbles the \u201dChicken-and-egg\u201d paradox. Practically, there\u2019s often a preliminary estimate of\nthe achievable loss for a given model size, informed by prior knowledge of preliminary\nexperiments. However, there is potential for the development of more refined estimation\nprocedures in the future.\nThe optimal batch size and optimal learning rate are likely to be not independent. To\novercome this correlation, we do a preliminary study on the learning rate first, then choose\nan optimal learning rate to do a batch size experiment, and use batch size scaling to do the\nlearning rate again. This is a bit like the Coordinate Descent optimization method. However,\nmore rigorous methods are welcomed in future work.\nA.3 Model Architecture in Model Wind Tunnel Experiments\nWe list the model configuration used in the model wind tunnel experiments in Table 8.\nThe \u201cshape\u201d of the model, i.e., model width compared to model depth is kept as similar as\npossible to avoid any potential performance variation.\nName N (B) dm df f dhnhL\n9M 0.009 320 800 64 5 8\n30M 0.036 512 1280 64 8 12\n70M 0.066 640 1600 64 10 14\n0.1B 0.109 768 1920 64 12 16\n0.17B 0.166 896 2240 64 14 18\n0.2B 0.241 1024 2560 64 16 20\n0.5B 0.499 1344 3360 64 21 24\nTable 8: Model configurations and training configurations of the models in the scaling curve.\nN(B) represents the number of non-embedding parameters of the models, measured in\nbillions.\nB Additional Illustration on WSD LRS\nB.1 Learning Rate Paradigm for Different LRSs\nIn this paper, we describe three kinds of LRSs, Cosine (T),CosineLoop (T), and WSD (T,D).\nCosine and Cosine Loop take the form of the following:\nAn illustrative learning rate diagram for WSD and Cosine Scheduler is shown in Figure 15.\n26Cosine (T;s) =\n\uf8f1\n\uf8f2\n\uf8f3s\nW\u03b7,s<W\n0.9\u03b7cos(\u03c0s\nT) +0.1\u03b7,W<s<T\n0.1\u03b7,s>TCosineLoop (T;s) =\n\u001as\nW\u03b7,s<W\n0.9\u03b7cos(\u03c0s\nT) +0.1\u03b7,W<s\n0 2000 4000 6000 8000 10000\nIteration0.00000.00250.00500.00750.01000.01250.01500.01750.0200Learning RateCosine(40N)\nWSD(40N,4N)\nWSD(80N,8N)\nFigure 15: Illustrative comparison be-\ntween Cosine LRS and WSD LRS. The\nWSD LRS with different end steps share\nthe same stable training stage.\nFigure 16: We use two different function\nforms to fit the data scaling law achieved\nby WSD LRS and choose power law as\nthe best fit.\nB.2 Fitting the Data Scaling Law\nIn this section, we describe the fitted data scaling law for continue training with WSD LRS.\nEach point in Figure 16 is the end of the decay stage in WSD LRS with a different end step.\nWe try two function forms: exponential and polynomial. The fitted result shows that the\npolynomial scaling law is still the best for continue training.\nB.3 Individual Figure for Model-Data Scaling Law\nFor each task and model, the scaling law L(N,D)\u2019s fitness with real loss values along the\ndata axis is plotted in Figure 17.\nB.4 Analysis of Llama2\u2019s Data-to-Model Ratio\nAs mentioned in Section 4.5, we analyze Llama2\u2019s Data-to-Model Ratio based on their\ntraining loss curve. The extracted loss is plotted on the left of Figure 18. We convert the\nx-axis to computation Flops to compare the computed optimal regime on the right part of\nthe Figure.\nC MiniCPM\u2019s Vocabulary\nDespite being small in parameter size, MiniCPM targets modeling diverse data distribution,\nexcelling in English and Chinese. Therefore, our vocabulary is relatively large. For the 2.4B\nmodel, we use a tokenizer consisting of 122,753 tokens (denoted by MiniCPMTokenizer-\n120K). This vocabulary is constructed from extensive and diverse language data, utilizing\nthe sentencepiece library4for Byte Pair Encoding (BPE) (Sennrich et al., 2016), and includes\nspecial symbols like traditional Chinese characters, rare characters, emojis, and special\nsymbols such as Greek letters, Cyrillic letters, etc.\nFor the SLM, the embedding parameters will take up a lot of parameter space if the vocabu-\nlary is large. Therefore, for our 1.2B model, we use a smaller vocab MiniCPMTokenizer-70K.\n4https://github.com/google/sentencepiece\n270.5 1.0 1.5 2.00.450.500.550.60Code0.031B\n2 4 60.360.380.400.420.440.11B\n5 10 150.320.340.360.25B\n10 20 300.280.300.320.340.49B\n20 40 600.260.270.280.290.300.85B\n50 1000.230.240.250.260.272.0B\n0.5 1.0 1.5 2.00.740.760.780.800.82English (Wikihow)\n2 4 60.620.640.660.680.70\n5 10 150.560.580.600.62\n10 20 300.540.560.58\n20 40 600.500.520.54\n50 1000.460.470.480.490.500.51\n0.5 1.0 1.5 2.00.9250.9500.9751.0001.0251.050Chinese (Wikihow)\n2 4 60.8000.8250.8500.8750.900\n5 10 150.720.740.760.780.80\n10 20 300.680.700.720.740.76\n20 40 600.640.660.680.70\n50 1000.620.640.66\n0.5 1.0 1.5 2.00.700.750.800.85Ultratext\n2 4 60.5500.5750.6000.6250.6500.675\n5 10 150.500.520.540.56\n10 20 300.440.460.480.500.52\n20 40 600.420.440.46\n50 1000.380.400.42\n0.5 1.0 1.5 2.0\nNumber of Tokens (B)0.9751.0001.0251.0501.0751.100Chinese (Yayi Corpus)\n2 4 6\nNumber of Tokens (B)0.860.880.900.920.94\n5 10 15\nNumber of Tokens (B)0.800.820.840.86\n10 20 30\nNumber of Tokens (B)0.740.760.780.80\n20 40 60\nNumber of Tokens (B)0.700.720.740.76\n50 100\nNumber of Tokens (B)0.660.680.700.72\n0.5 1.0 1.5 2.00.7500.7750.8000.8250.8500.875Average\n2 4 60.640.660.680.700.720.74\n5 10 150.580.600.620.64\n10 20 300.540.560.580.60\n20 40 600.500.520.540.56\n50 1000.480.500.52real\n\ufb01ttedFigure 17: The fitted scaling law plotted along the data amount axis for each model and\neach task. The fitted result is satisfying except for the last checkpoints of the 0.11B and 0.25B\nmodel.\n0 250 500 750 1000 1250 1500 1750 2000\nToken (B)1.41.51.61.71.81.92.02.12.2Train Loss of Llama27B\n13B\n34B\n70B\n103104105106\nCompute (1018FLOPs)1.41.51.61.71.81.92.02.12.2Train Loss of Llama27B\n13B\n34B\n70B\nFigure 18: We extract the training loss data from Llama2 paper (left part) and estimate the\ncompute optimalDopt\nNoptin their paper using the right part. The straight lines are plotted to\nestimate the optimal loss envelope assuming using WSD Scheduler.\n28Compared to the MiniCPMTokenizer-120K tokenizer, we have re-trained the tokenization\non the same documents, while setting the max number of vocabs to 64,000. For the special\ncharacters, we only add the traditional Chinese characters, emojis, and special symbols, but\nleave out the rare characters in Chinese.\nWe conduct evaluations on 300,000 documents in Chinese, English, code, and academic\npapers that are not in the training set of the Tokenizer. The MiniCPM-120K tokenizer\nachieves the highest compression ratio (Bytes/Tokens).\nBaichuan2 ChatGLM2 Llama2 MiniCPM-120K MiniCPM-70K\nVocab Size 125,696 64,794 32,000 122,753 73,440\nCompression Rate (Bytes/Tokens)\nChinese 3.64 3.54 1.87 3.73 3.56\nEnglish 4.12 4.02 3.78 4.14 4.02\nCode 2.71 2.71 2.74 2.81 2.76\nPaper 2.74 2.88 2.97 2.93 2.88\nAverage 3.30 3.29 2.84 3.40 3.31\nTable 9: Compression ratio comparison.\nD Quantization\nWe conduct 4-bit quantization on our model. We do not quantize the parameters of em-\nbedding and layer normalization, since the performance of the model is sensitive to these\nparameters. Therefore, we only need to quantify each weight matrix. Denote the weight\nmatrix as W\u2208Rdout\u00d7din. We group every Gconsecutive parameter at the dindimension and\nform din/Ggroup. Then we quantize each group of the parameters separately. For each\ngroup parameter w, we calculate the quantization scale and zero point as follows:\nscale =max(w)\u2212min(w)\n24\u22121, zero =\u2212min(w)\nscale\u221223.\nGroup parameter ware then quantized to\n\u02c6w=quant (w) =round (w\nscale+zero),\nwhere round operation round a floating point to nearest integer. The dequantization opera-\ntion is approximately the reverse of the quantization method, which is\ndequant (\u02c6w) =scale(\u02c6w\u2212zero).\nFinally, matrix W\u2208Rdout\u00d7dinis quantized to int4 \u02c6W\u2208Rdout\u00d7din, float scale\u2208Rdout\u00d7din\nGand\nfloat zero\u2208Rdout\u00d7din\nG.\nTo reduce the quantization loss, we adopt GPTQ (Frantar et al., 2022) to apply weight\ncalibration. We sample calibration data Xfrom a similar distribution of SFT data. The quan-\ntization objective is to minimize the disturbance of the quantization \u2225WX\u2212dequant (\u02c6W)X\u22252\n2.\nWe follow GPTQ to quantize weight iteratively and update the remaining non-quantized\nweight by\n\u03b4F=\u2212wq\u2212dequant (quant (wq))\n[H\u22121\nF]qq\u00b7(H\u22121\nF):,q,\nwhere qis the quantization position in the current iteration while Fdenotes the remaining\nnon-quantized weights. HFis the hessian matrix of the objective function.\n29SmartPhone Operating System ProcessorPhone\nMemory\n(GB)Inference\nThroughput\n(token/s)\nOPPO Find N3 Android 13 snapdragon 8 Gen2 12 6.5\nSamsung S23 Ultra Android 14 snapdragon 8 Gen2 12 6.4\nMeizu M182Q Android 11snapdragon\n888Plus8 3.7\nXiaomi 12 Pro Android 13 snapdragon 8 Gen1 8+3 3.7\nXiaomi Redmi K40 Android 11 snapdragon 870 8 3.5\nOneplus LE 2100 Android 13 snapdragon 870 12 3.5\nOneplus HD1900 Android 11 snapdragon 865 8 3.2\nOneplus HD1900 Android 11 snapdragon 855 8 3.0\nOneplus HD1905 Android 10 snapdragon 855 8 3.0\nOneplus HD1900 Android 11 snapdragon 855 8 3.0\nXiaomi MI 8 Android 9 snapdragon 845 6 2.3\nHuawei Nova 11SE Harmony 4.0.0 snapdragon 778 12 1.9\nXiaomi MIX 2 Android 9 snapdragon 835 6 1.3\niPhone 15 Pro iOS 17.2.1 A17 pro 8 18.0\niPhone 15 iOS 17.2.1 A16 6 15.0\niPhone 12 Pro iOS 16.5.1 A14 6 5.8\niPhone 12 iOS 17.2.1 A14 4 5.8\niPhone 11 iOS 16.6 A13 4 4.6\nTable 10: Speed of MiniCPM-2.4B on different end devices.\nE Edge Device Benchmarking\nAfter Int4 quantization in Appendix D, MiniCPM-2.4B\u2019s footprint is reduced to 2GB, fa-\ncilitating deployment on mobile edge devices. We adapted the model for Android and\nHarmonyOS using MLC-LLM (team, 2023b) and employed LLMFarm (team, 2023a) for\nadaptation to the iPhone system. This adaptation was tested on various edge mobile devices.\nIt is important to emphasize that our efforts did not focus on optimization for mobile de-\nployment, but rather on demonstrating the practicality of MiniCPM\u2019s inference capabilities\non mobile platforms. We encourage further optimization and updates by the developer\ncommunity to enhance the performance of large models like MiniCPM in mobile contexts.\nThe result is shown in Table 10, we can see that on the most advanced smartphone iPhone\n15 Pro, the inference throughput is as high as 18 tokens per second. In other devices, the\ninference throughput is also acceptable.\nF Case Demonstration\nA notable feature distinguishing the MiniCPM series from other robust SLMs like Phi (Li\net al., 2023b) by Microsoft is that we train MiniCPM on general-purpose corpora, which\nguarantees the models\u2019 versatility and generalization across a variety of tasks. In this\nsection, we present several intriguing generation instances, typically achievable only by\nlarger models, to demonstrate the capabilities of MiniCPM. Although we do not conduct\nextensive checks on whether specific cases exist in the training set, due to the diverse ways\nthey can be expressed, we ensure that no data similar to the test cases is intentionally added.\n30Translation\nPrompt: Translate this sentence into French: \u201dI am a freshman on Chinese, do you know how this sentence is\ntranslated: \u5982\u4f55\u7528Python\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u7f51\u9875\u722c\u866b\uff1f\u201d (How to create a simple web crawler in Python?)\nMiniCPM-1.2B:\nJe suis un homme nouveau en chinois, vous \u02c6etes-vous\nau courant de la mani `ere dont cette phrase est traduite :\nComment cr \u00b4eer un petit scraper web simple `a Python ?MiniCPM-2.4B-DPO:\nJe suis un nouveau-n \u00b4e en chinois, sais-tu comment on cr \u00b4ee\nun simple web crawler en Python ? Translation: \u201dJe suis\nun nouveau-n \u00b4e en chinois, sais-tu comment on cr \u00b4ee un\nsimple web crawler en Python ?\u201d\nFigure 19: Examples on translation. We notice that MiniCPM-2.4B-DPO performs better in\ntranslating the technical part, but both models are unnatural in expressing \u201djust starting to\nlearn Chinese.\u201d. However, both models successfully understand the instruction with traps\nand generate French despite we do not include French corpus intentionally.\nMath and Reasoning\nPrompt: \u5982\u679c\u4ee5\u4e0b\u7b97\u5f0f\u4e2d\u51cf\u53f7\u5747\u8868\u793a\u52a0\u53f7\uff0c\u800c\u52a0\u53f7\u5b9e\u9645\u4e0a\u662f\u51cf\u53f7\uff0c\u7ed9\u51fa\u7b97\u5f0f\u7684\u8ba1\u7b97\u7ed3\u679c15-25-12+34=\n(If in the following equation, minus signs represent addition, and plus signs represent subtraction, give the result of\nthe calculation: 15-25-12+34=)\nMiniCPM-1.2B:\n\u5982\u679c\u51cf\u53f7\u8868\u793a\u52a0\u53f7\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u5c06\u7b97\u5f0f\u6539\u5199\u4e3a\uff1a15 + 25\n+ 12 - 34\n\u9996\u5148\u8ba1\u7b97\u524d\u4e24\u4e2a\u6570\u7684\u548c\uff1a15 + 25 = 40\n\u7136\u540e\u8ba1\u7b97\u524d\u4e09\u4e2a\u6570\u7684\u548c\uff1a40 + 12 = 52\n\u6700\u540e\u8ba1\u7b97\u6700\u540e\u4e00\u4e2a\u6570\u51cf\u53bb\u524d\u4e09\u4e2a\u6570\u7684\u548c\uff1a52 - 34 = 18\n\u6240\u4ee5\uff0c\u7b97\u5f0f\u7684\u8ba1\u7b97\u7ed3\u679c\u4e3a18\u3002\n(If the minus sign represents a plus sign, then we can\nrewrite the equation as: 15 + 25 + 12 - 34.\nFirst, calculate the sum of the first two numbers:\n15 + 25 = 40.\nThen, calculate the sum of the first three numbers: 40 + 12\n= 52.\nFinally, calculate the difference between the last number\nand the sum of the first three numbers: 52 - 34 = 18.\nTherefore, the result of the equation is 18.)MiniCPM-2.4B-DPO:\n\u5982\u679c\u51cf\u53f7\u5747\u8868\u793a\u52a0\u53f7\uff0c\u800c\u52a0\u53f7\u5b9e\u9645\u4e0a\u662f\u51cf\u53f7\uff0c\u90a3\u4e48\u7b97\u5f0f\u53ef\n\u4ee5\u91cd\u65b0\u8868\u793a\u4e3a\uff1a\n15+25+12-34\n\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u6309\u7167\u52a0\u51cf\u6cd5\u7684\u987a\u5e8f\u8fdb\u884c\u8ba1\u7b97\uff1a\n(15+25)+12-34=40+12-34=52-34=18\n\u6240\u4ee5\uff0c\u6700\u7ec8\u7684\u8ba1\u7b97\u7ed3\u679c\u662f18\u3002\n(If minus signs all represent addition, and plus signs\nactually represent subtraction, then the equation can be\nre-expressed as:\n15+25+12-34.\nNow we can calculate according to the order of\naddition and subtraction:\n(15+25)+12-34 = 40+12-34 = 52-34 = 18\nSo, the final result of the calculation is 18.)\nFigure 20: Examples on math and reasoning. Both models are successful in symbolic\nsubstitution and math calculation.\nInstruction Following\nPrompt: \u5199\u85cf\u5934\u8bd7\uff0c\u85cf\u5934\u662f\u9f99\u5e74\u5927\u5409\n(Writing an acrostic poem, with the hidden message \u201dGood luck in the Year of the Dragon.\u201d)\nMiniCPM-1.2B:\n\u9f99\u6c60\u6625\u6c34\u6da8\uff0c\n\u51e4\u9619\u65e5\u521d\u957f\u3002\n\u5409\u5e86\u4f55\u65f6\u81f3\uff0c\n\u592a\u5e73\u6709\u8c61\u85cf\u3002\n(The spring water in the dragon\u2019s pool rises,\nThe sun rises at the phoenix gate.\nWhen will good fortune arrive,\nPeace and prosperity are hidden in signs.)MiniCPM-2.4B-DPO:\n\u9f99\u817e\u4e07\u91cc\u6c14\u5982\u8679\uff0c\n\u5e74\u5c81\u65b0\u6765\u4e50\u4e8b\u6d53\u3002\n\u5927\u5409\u826f\u8fb0\u582a\u5171\u8d4f\uff0c\n\u5409\u7965\u5982\u610f\u7b11\u989c\u7ea2\u3002\n(Dragons soar for thousands of miles, their energy like a\nrainbow,\nAs the new year begins, joyous occasions abound.\nAuspicious days are worth enjoying together,\nGood fortune and happiness bring rosy smiles.)\nFigure 21: Examples on instruction following. MiniCPM-2.4B-DPO successfully follows the\ninstructions and generates a poem with the correct tonal pattern and the first character of\neach sentence. However, MiniCPM-1.2B only generates the correct tonal pattern.\n31Emoji\nPrompt: \u8bf7\u7ed9\u6211\u4e00\u4e2a json\uff0c\u6bcf\u4e2akey\u662f\u4e00\u4e2a emoji\uff0c\u6bcf\u4e2avalue\u662f\u8fd9\u4e2aemoji\u5bf9\u5e94\u7684\u610f\u601d\uff0c\u7ed9\u51fa10\u4e2a\u5de6\u53f3\n(Please give me a JSON object where each key is an emoji and each value is the meaning of that emoji, providing\naround 10 examples.)\nMiniCPM-1.2B:\n\u5f53\u7136\u53ef\u4ee5\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b10\u4e2a\u5de6\u53f3emoji\u53ca\u5176\u542b\u4e49\n\u7684JSON\uff1a\n{\n\u201d\n \u201d: \u201d\u7b11\u5bb9\u201d,\n\u201d\n \u201d: \u201d\u7b11\u201d,\n\u201d\n \u201d: \u201d\u60ca\u53f9\u201d,\n\u201d\n \u201d: \u201d\u4eb2\u543b\u201d,\n\u201d\n \u201d: \u201d\u6076\u4f5c\u5267\u201d,\n\u201d\n \u201d: \u201d\u5931\u671b\u201d,\n\u201d\n \u201d:\u201d\u601d\u8003\u201d,\n\u201d\n \u201d: \u201d\u91d1\u94b1\u201d,\n\u201d\n \u201d: \u201d\u5e2e\u52a9\u201d,\n\u201d\n \u201d: \u201d\u5e9f\u7269\u201d\n}\n\u5e0c\u671b\u8fd9\u4e2aJSON\u5bf9\u4f60\u6709\u5e2e\u52a9\uff01\n(Of course, this is a JSON file containing around 10 emojis\nand their meanings:\n{\n\u201d\n \u201d: \u201dsmiling\u201d,\n\u201d\n \u201d: \u201dlaughing\u201d,\n\u201d\n \u201d: \u201damazing\u201d,\n\u201d\n \u201d: \u201dkissing\u201d,\n\u201d\n \u201d: \u201dmischief\u201d,\n\u201d\n \u201d: \u201ddisappointment\u201d,\n\u201d\n \u201d:\u201dthinking\u201d,\n\u201d\n \u201d: \u201dmoney\u201d,\n\u201d\n \u201d: \u201dhelp\u201d,\n\u201d\n \u201d: \u201drubbish\u201d\n}\nHope you found this JSON useful!)MiniCPM-2.4B-DPO:\n\u5f53\u7136\u53ef\u4ee5\uff01\u4ee5\u4e0b\u662f\u4e00\u4e2a\u5305\u542b10\u4e2aemoi\u53ca\u5176\u5bf9\u5e94\u610f\u4e49\n\u7684JSON\u5bf9\u8c61\uff1a\n{\n\u201d\n \u201d: \u201d\u8868\u793a\u6109\u5feb\u6216\u6ee1\u610f\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u5fae\u7b11\u548c\u559c\u60a6\u3002\u201d,\n\u201d\n \u201d: \u201d\u8868\u793a\u6124\u6012\u6216\u4e0d\u6ee1\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u60c5\u7eea\u7684\u6fc0\u52a8\u3002\u201d,\n\u201d\n \u201d: \u201d\u8868\u793a\u5927\u7b11\u6216\u5f00\u5fc3\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u7b11\u58f0\u548c\u6b22\u4e50\u3002\u201d,\n\u201d\n \u201d: \u201d\u8868\u793a\u641e\u7b11\u6216\u5c34\u5c2c\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u641e\u7b11\u7684\u573a\u666f\u6216\u5c34\n\u5c2c\u7684\u60c5\u51b5\u3002\u201d,\n\u201d\n \u201d: \u201d\u8868\u793a\u60ca\u8bb6\u6216\u9707\u60ca\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u5bf9\u67d0\u4e2a\u4e8b\u4ef6\u6216\u60c5\n\u7684\u5f3a\u70c8\u53cd\u5e94\u3002\u201d,\n\u201d\n \u201d: \u201d\u8868\u793a\u5931\u671b\u6216\u6cae\u4e27\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u5bf9\u67d0\u4e8b\u6216\u67d0\u4eba\u5931\n\u53bb\u5174\u8da3\u6216\u4fe1\u5fc3\u7684\u60c5\u7eea\u3002\u201d,\n\u201d\n \u201d:\u201d\u8868\u793a\u5fc3\u75db\u6216\u4f24\u5fc3\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u5bf9\u67d0\u4e8b\u6216\u67d0\u4eba\u9020\u6210\n\u75db\u82e6\u7684\u5f3a\u70c8\u611f\u53d7\u3002\u201d,\n\u201d\n \u201d: \u201d\u8868\u793a\u60ca\u559c\u6216\u5174\u594b\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u5bf9\u67d0\u4e8b\u6216\u67d0\u4eba\u7684\n\u53d1\u73b0\u6216\u53d1\u73b0\u7684\u60c5\u7eea\u3002\u201d,\n\u201d\n \u201d: \u201d\u8868\u793a\u9177\u6216\u65f6\u9ae6\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u81ea\u4fe1\u3001\u72ec\u7acb\u548c\u6f6e\u6d41\n\u7684\u6001\u5ea6\u3002\u201d,\n\u201d\n \u201d: \u201d\u8868\u793a\u559c\u6b22\u6216\u6b23\u8d4f\uff0c\u901a\u5e38\u7528\u4e8e\u8868\u8fbe\u5bf9\u67d0\u7269\u6216\u67d0\u4eba\u7684\n\u559c\u7231\u4e4b\u60c5\u3002\u201d\n}\n\u5e0c\u671b\u8fd9\u4e2aJSON\u5bf9\u8c61\u80fd\u6ee1\u8db3\u60a8\u7684\u9700\u6c42\uff01\u5982\u679c\u60a8\u6709\u4efb\u4f55\u95ee\n\u9898\uff0c\u8bf7\u968f\u65f6\u544a\u8bc9\u6211\u3002\n(Of course! Here is a JSON object containing 10 emojis and\ntheir corresponding meanings:\n{\n\u201d\n \u201d: \u201dIt means happy or satisfied, typically used to ex-\npress smiling and joy.\u201d,\n\u201d\n \u201d: \u201dIt means anger or dissatisfaction, typically used to\nexpress intense emotions.\u201d,\n\u201d\n \u201d: \u201dIt means laughing or happy, typically used to ex-\npress laughter and joy.\u201d,\n\u201d\n \u201d: \u201dIt means funny or awkward, typically used to ex-\npress humorous situations or awkwardness.\u201d,\n\u201d\n \u201d: \u201dIt means surprise or shock, typically used to ex-\npress a strong reaction to an event or situation.\u201d,\n\u201d\n \u201d: \u201dIt means disappointment or discouragement, typ-\nically used to express a loss of interest or confidence in\nsomething or someone.\u201d,\n\u201d\n \u201d: \u201dIt means heartache or sadness, typically used to\nexpress intense feelings of pain caused by something or\nsomeone.\u201d,\n\u201d\n \u201d: \u201dIt means surprise or excitement, typically used to\nexpress the emotion of discovery or finding something or\nsomeone.\u201d,\n\u201d\n \u201d: \u201dIt means cool or stylish, typically used to express\na confident, independent, and trendy attitude.\u201d,\n\u201d\n \u201d: \u201dIt means like or appreciate, typically used to ex-\npress affection or admiration for something or someone.\u201d\n}\nI hope this JSON object meets your needs! If you have any\nquestions, please feel free to let me know.)\nFigure 22: Examples on emoji. Both models understand emojis and structured data well,\nand accurately control the number of items implicitly. The DPO model tends to generate\nmore words.\n32"
}