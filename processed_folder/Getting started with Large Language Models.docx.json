{
    "filename": "Getting started with Large Language Models.docx",
    "type": "docx",
    "title": "",
    "authors": "Chris Quirk",
    "lastmodifiedtime": "2023-05-31T00:10:19+00:00",
    "text": "Getting started with Large Language Models\naka.ms/llm-start-here\nchrisq@microsoft.com, February 2023\nBackground  \nIn January 2023, everyone is buzzing about ChatGPT. This large language model can help design programming languages, help you write business plans, author piano compositions, and many more. This document is about how to get started.\n What are these large language models? \nMost (LLMs) today, including the GPT family of models from OpenAI, are word generators. You give them a prefix, and they keep generating tokens until they hit a limit or a pre-defined stop token. If I present the model with a prefix, it just keeps writing, almost like a test question.\nBecause they are trained on huge volumes of text, they know about a huge variety of things from the real world. To Microsoft AI: Artificial Intelligence Basics demonstrate, I\u2019ll show a prompt or prefix in normal text, followed by the GPT-3 prediction in green highlights. It has learned about internal combustion engines: \n\nAs well as natural language processing techniques:\n\nThe fluency and background here is impressive, though one can imagine how these completions and responses could be learned directly from the web \u2013 perhaps this answer or something like it was part of the training data, and then model has memorized this information.\nMore impressively, though, these models have also learned to work through information based on background context. For instance, they can solve word problems:\n\nThe capacity of these models to capture common sense information, world knowledge, and reason over local information in text has dramatically increased over the past 5 years, as the training data provided to the model has grown and the computational capacity and parameters of the models have increased.\nHow has this changed machine learning?\nA few years ago, if we wanted a system to do complex pattern recognition or prediction, we would need to find large volumes of training data. Mostly this would mean thousands, or millions, or even billions of input/output pairs used to train a predictor. Translation systems would learn from pre-translated text, image recognition systems would learn from annotated images, etc. However, each system was unique and bespoke. This processing of training a machine learned model has been a key recipe to successful ML approaches starting in the 1990s.\nAbout 5 years ago, our approach to machine learned tasks changed drastically with large, pre-trained models like ImageNet and BERT, that learned background information about images or language. Machine learned approaches would start with one of these pre-trained models, and then fine-tune the model parameters (make smaller updates from an initial model starting point) to make the desired prediction. This allowed us to use orders of magnitude less training data and to have models with a broader coverage of background knowledge.\nThe most recent advance has been to move to so-called few-shot or zero-shot training. Few-shot means that the model doesn\u2019t need thousands or even hundreds of training examples anymore \u2013 just a handful of examples are used. Sometimes the model doesn\u2019t even need a training instance anymore \u2013 zero-shot means that we just ask the model to do what we want, and it does it.\n \nThis prompt starts with an instruction, and then shows several examples of what is expected. Finally, the desired test input is provided. GPT successfully predicts a good completion. This seems like a bit of magic, but keep in mind that GPT was trained to mimic what people have written. The internet is full of documents where questions are answered, where reasoning is slowly worked out, where the chain-of-thought behind an answer is spelled out, and GPT has learned a lot from these cases. \nOften people will call the input to these models a prompt. Finding a good prompt that maximizes accuracy from large language model is called prompt engineering. Developing good prompts is not a clear science yet \u2013 currently people manually tune the text (including written instructions, background context, provided examples components, etc.) to optimize the quality of output. There are efforts to make prompt engineering into a standard optimization problem, but this is in flux. Furthermore, there are advantages to written prompts \u2013 they can be easily tuned or updated without knowing how to code or optimize.\nWhat is still tough?\nAlthough the landscape has drastically changed, some key ML problems have not disappeared, and other new issues have arisen. I\u2019m listing three issues here that are important, but this list is not comprehensive.\nEvaluation\nOne key challenge in machine learned systems is evaluation \u2013 did my model do a good job? Does it work well in general? Will it be okay for the next piece of data or user who comes along? This challenge still remains. Often times, we\u2019ll try out a few examples of GPT and find they work beautifully. However, sometimes they fail miserably.\nMost machine learned systems are evaluated on a test set. For many tasks, finding and building a good test set isn\u2019t easy. Sometimes we can use naturally occurring data for test (and maybe training too). For instance, human translators are paid to translate software documentation sentence by sentence. The resulting sentence pairs can be used to train and evaluate a system \u2013 if we\u2019re lucky, such data will be available, and we can compare system outputs to a human-produced answer. If we\u2019re not so lucky, we\u2019ll need to create or curate that dataset. This is very important to ensuring that a system works well.\nFurthermore, in many language-oriented examples, the system output and the human authored output may use totally different words but mean the same thing! For instance the two sentences \u201cI have a severe headache\u201d and \u201cmy head is throbbing with pain\u201d have zero words in common, but are very close in meaning. The language processing community has developed noisy but useful techniques to compare system outputs, but this is imperfect.\nKeep in mind that LLMs have transformed the process of developing and exploring a model, but clean and believable evaluation is a crucial and expensive part of an effective deployed system.\nFactual correctness\nA major headache is that these large language models generate very authentic looking prose (very few spelling or grammatical errors), but often time make up information that is not grounded in the real world. I\u2019ll ask GPT about myself:\n\nNone of this is true. I\u2019m not a football player or coach. Joe Brady is the offensive coordinator for the Panthers. The paragraph sounds very impressive \u2013 generating this kind of background pedigree might even get you elected to congress \u2013 but it\u2019s not true.\nFinding ways to identify and minimize this hallucination is a key problem area. Any downstream user needs to be cautious that the information from the model may be incorrect.\nResponsible AI\nThese models have learned many things from our training data, some desirable, some not. Amongst the model training data are buried explicitly problematic language (statements that are racist, etc.), as well as implicit biases (perhaps there are more occurrences of \u201che\u201d associated with \u201cdoctor\u201d, leading the model toward biases that humans make). When we use such a model to generate text or to guide decision making processes, it can potentially cause or amplify harm.\nI cannot do justice to this crucial and complex issue in a getting started guide, but I strongly encourage users of any advanced pattern recognition or machine learning technique (especially powerful language models like these) to carefully consider the implications, improvements, and potential harms of their application. One starting point is here: https://microsoft.sharepoint.com/teams/exdrai\nLet\u2019s build something\nTo pick a common task, say I want to summarize some data. I have a long body of text, and I\u2019d like to find a shorter form of it. I\u2019ll work with the Foundry Toolkit in these examples.\nEssential guidance for FHL and beyond\nOur obligations to safeguard Microsoft and customer data don\u2019t change even when doing FHL projects. Please keep the following in mind as you embark on your FHL awesomeness.\nWhile using external endpoints like ChatGPT hosted at openai.com (or any other non M365 endpoint):\nDo not send any employee personal data to external endpoints. This includes data from internal tools, data from your own usage of Microsoft products, your own productivity data (e.g. your emails), or even data voluntarily provided by other employees. \nDo not send any confidential Microsoft information to external endpoints.\nDo not send any data that identifies customers (OII) or users (EUII/EUPI) or that has customer content to external endpoints. \nOnly non-personal data, web data or synthetic data can be used against external endpoints.\nThere are internal endpoints like Foundry that are set up for experimentation. While using internal endpoints, keep the following guidelines in mind:\nNever send highly confidential or tented information, even to these internal endpoints.\nUse approved methods like Recife data or using Graph Explorer for getting your personal data.\nThe eyes-on or eyes-off guidance around specific types of data still holds true even when using internal endpoints. Please refer to the data classification taxonomy and the data handling standards for guidance around data handling policies.\nIf you have any questions or need clarifications, please reach out to your compliance, security or privacy champion.\nSimple prompt\nWe are going to create first a prompt that asks GPT to summarize. You can jump to a link here:\nhttps://foundrytoolkit.azurewebsites.net/playgroundv2?session=7f34caca-ec5a-452f-bb24-3c4d6fc2449c\nThe input to the model is as follows:\nSummarize below text in 50 words:\n \nI've been doing these pitch sessions for a year or two now and and this team and I always try to keep this like relatively informal, because next week is not only our FHL week, but it's also sort of the company wide hackathon and there should be an opportunity for people to do like crazy, interesting, unusual stuff to explore some new areas to really learn and hack and play around with things I guess one bit of feedback. I was just hearing today is that fix is a little bit tricky, right? Because fixes can sort of. Like feel a little bit doing your daily job just for an additional week. Of course, you're welcome to do that, but this should be a week that you have to use. However, you will to explore something completely different to just learn to create a new prototype of something. And so you and your whole management change should be getting encouraged to use this week in the way that you like. So if you have any feedback or issue about that, I do encourage you to take it up with your manager and so on. But I think the the feedback I generally hear across this org is everybody's excited about this, that.      \n\n\nThat\u2019s it \u2013 we just ask this model to summarize a given text. No training data, no fancy structure, no complex call structure \u2013 just textual input, and textual output\nWhen provided with that input, GPT produces the following output:\nThe speaker has been doing pitch sessions for a year or two, and they are trying to keep it informal. They want people to use the hackathon week to explore something new and different, and not just use it to do their regular job.\nIt\u2019s a pretty reasonable summary. You can play with the details too:\nAsk it to summarize in 10 words\nVary the input text\nExplore variations in the instruction\nIt\u2019s not always perfect, but keep in mind: this is not a model trained specifically to summarize. Rather, it has learned to consume text and follow generalized instructions. It does a pretty remarkable job.\nAdam Troy has developed a Prompt Playbook to define and illustrate some of the most effective approaches in prompt engineering today.\nPrompt template\nNow if we actually want to use this in practice, we most likely want a more generalized form of template \u2013 something where the text to summarize, and maybe even the length of the summary, are parameters that can be specified at runtime.\nThe approach that Foundry Toolkit has used is to introduce a templating strategy. In addition to fixed text, a prompt template can contain placeholders that are replaced on the fly. The above prompt can be rewritten as (https://foundrytoolkit.azurewebsites.net/playgroundv2?prompt=177ba41d-7758-4873-b8d4-68f4db469df6):\nSummarize below text in 50 words:\n{{Text}}\nThe value of the {{Text}} variable can be supplied at runtime. Foundry Toolkit uses JSON to provide these parameter values and the Handlebars language for templating. Other systems might use a different templating format or other variations. However, the notion of composing an input on the fly for GPT to operate over is powerful and commonly used these days.\nIt\u2019s also worth noting that GPT does not (yet!) know about these template languages. This is just preprocessing that a user of GPT does beforehand.\nThis is a good point to reemphasize the importance of evaluation. One could try lots of variations on the input \u2013 put the instruction after the text, changing the wording of the instruction itself, adding some delimiter about the text itself, providing some additional input/output examples as part of the prompt, etc. But would this improve the quality of summaries overall? To answer this sort of question, you\u2019ll need some kind of evaluation methodology and metrics, for instance a batch of known-good input/output pairs and some metric to compare human-authored and system-produced summaries.\n\nCalling the LLM API from Code\nA critical part to accessing LLM endpoints is the LLM API (aka.ms/llm-api-docs). The LLM API is an abstraction layer which makes the compliant GPTx endpoints available to scenarios using appropriate authentication and manages the scope of targeting to users and providing rate limiting leveraging Azure OAI.\nThe repo at aka.ms/lm-api-examples contains some code to get you started, including simple examples in Python and C# calling into an LLM API endpoint. (More examples will be coming soon!)\nYou can use your own Recife personal curated email data as prompts to get completions results from the OpenAI Endpoint. We have created a code-lab with a hands-on example on how to do just that: aka.ms/llm-studio-example.\nExample use cases\nThere are many possible ways to explore GPT models. Here are just a few ideas:\nAsking about your calendar. You can get a snapshot of your calendar from Microsoft Graph, serialize it out into text, then include it as part of a prompt for asking about past and future events. Finding effective means of presenting our Substrate data as part of a prompt is a key question for MSAI and Microsoft in the future.\nAuthoring documentation automatically. You can provide code and an outline about the goals of your document, then use GPT to help you construct the details of that documentation. Recent GPT models are surprisingly good at understanding code!\nIn line with this MSAI has joined the E+D wide Documentation Enhancement Program (DEP, aka.ms/msai-dep). Join forces using aka.ms/msai-dep/fhl/join to make your prototype part of the MSAI DEP.\nGenerate meeting agendas, based on meeting titles and attendees. Leveraging information from your user shard to gather information about attendees and background about a meeting title, you could explore authoring an agenda. This could be informed by past meetings with similar titles, information or topics associated with the people on the invite, and relevant documents. Gathering the data and establishing an evaluation could be an interesting challenge.\nGenerate diagrams from text in some simple markup language (e.g. mermaid). Sometimes a picture really is worth a thousand words. Can you generate a flowchart or system diagram automatically given a piece of text?\nText Generation\nProduce documentation from design specs, code, etc. (things that go into SDC)\nWrite TSG based on set of Teams, Stack Overflow questions and documentation.\nAutomate QOS, COGS, sprint or status report generation.\nCreate a text to YAML for compliant AML components or Polymer configurations using ChatGPT and custom VSCode plugins. Similar could be done for Polymer configurations (e.g. protobuf and other configurations). (POC: Amund.Tveit@microsoft.com)\nAgenda creation: Train ChatGPT to generate agendas for meetings based on the topics, attendees, and goals of the meeting.\nCustomized image generation: e.g. suggest slide layout/images but based on context and user\u2019s past slide show content\nSummarization\nQ&A over and summarize a document.\nQuestion generation off a document to validate reading comprehension\nQ&A over my inbox\nAsk questions about my calendar: when am I talking to X next?\nGenerate highlights/lowlights/risks from engineering reports or ADO status.\nCatch up with a chat or email thread you haven\u2019t read\nSummaries of stack overflow questions on a given topic\nGenerate a summary of what my meetings (tomorrow/next week) are about by leveraging calendar, people, roles, docs, LinkedIn, who+...\nSearch\nSearch for SDC docs using ChatGPT on SDC content\nHelping users find something relevant if their search has failed by multi-turn clarification of your query. The goal of the latter thing would be to get a trace of a search that would have failed, but now succeeded, and this could be used to improve our system.\nInternal tooling \nPipelines connecting semantic fabric or SWG with LLMs: e.g. prompt augmentation or fetching data for improved ChatGPT responses\nUsing (Chat)GPT to detect hallucinations/textual entailment through coaching. Given original text and generated text (could be from GPT or handcrafted), can we provide the necessary information in the prompts to determine whether the generated text is grounded in the original content.\nDevelop a full-fledged GPT-Annotator: pick a task like email classification, leverage GPT to generate an Annotation Guideline, use the Annotation Guideline as input and ask GPT to generate prompts for it to do synthetic +ve and -ve sample generation. Use GPT and the prompts from previous step to produce positive and negative examples. Assess via human judgement whether the synthetic dataset seems useful/consistent\nCreate a completely simulated environment for compliant AML and/or Polymer using ChatGPT similar to Linux terminal or Microsoft Graph simulations (see slide 35-38 in this deck: ChatGPT Usage Examples.pptx ) (POC: Amund.Tveit@microsoft.com)\nRecommendation\nWhat should I spend my time doing next? What documents I should read?\nCustom email triage: move emails to correct folders customized to users\u2019 directory structure (I.e. get rid of manual email rules)\nEvent recommendations: Train ChatGPT to recommend events to users based on their previous attendance, interests, and location.\nRecommendation of [movies using Movie Lens or Teams Chats or Outlook Emails] via GPT prompts that includes instructions (inject Substrate data for Teams/Outlook), actual question, negative and positive examples. Chat/Email \u2013 is this an important chat/email to focus on?\nSupport and resources\nYou can use the following support channels to get assistance using our platform.\nAsk in Stack Overflow: For general questions about the platform, how to use it, etc. Please use the substrate-llm-support tag in your question.\nReport an Issue: For agility blockers, general feedback, feature requests, and documentation change requests/feedback. Please use substrate-llm-support tag while reporting your issue.\nJoin AI Platform Users DL to receive the latest Substrate Intelligence Platform updates including LLMs.\nOther places to look for useful information:\nE+D AI Accelerator - Home (sharepoint.com)\nOPG FHL Talks on LLMs \nBuilding GPT-3 applications \u2014 beyond the prompt by Paulo Salem\nFAQs\nWhich OpenAI models are supported by Substrate LLM API?\nSubstrate LLM API supports 4 OpenAI models today:\nGPT-3: text-davinci-001\nGPT-3.5: text-davinci-002\nGPT-3.51: text-davinci-003\nChatGPT: text-chat-davinci-002\nThe list of available models will be kept up-to-date at this link aka.ms/inferencing/llm.\nHow much traffic can I direct to Substrate LLM API during FHL?\nWe are requesting teams to limit their usage of Substrate LLM API to prototyping scenarios during FHL (5-10 requests per hour per user). Teams that are interested in scaling their scenarios to bigger rings should reach out to Nitant Singh (nisi@) for next steps.\nI want to explore Large Language Models. How can I get started?\nHere are two tools you can use to explore Large Language Models:\nFoundry Toolkit\nAugmentation Loop Playground\nAnd of course, this guide!\n\n"
}