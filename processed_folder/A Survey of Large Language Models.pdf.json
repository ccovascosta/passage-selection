{
    "filename": "A Survey of Large Language Models.pdf",
    "type": "pdf",
    "title": "",
    "authors": "",
    "lastmodifiedtime": "D:20231127014456Z",
    "text": "1\nA Survey of Large Language Models\nWayne Xin Zhao, Kun Zhou*, Junyi Li*, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Y ang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie and Ji-Rong Wen\nAbstract \u2014Ever since the Turing Test was proposed in the 1950s, humans have explored the mastering of language intelligence\nby machine. Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a\nsignificant challenge to develop capable artificial intelligence (AI) algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving\nfrom statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-\ntraining Transformer models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP)\ntasks. Since the researchers have found that model scaling can lead to an improved model capacity, they further investigate the scaling\neffect by increasing the parameter scale to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these\nenlarged language models not only achieve a significant performance improvement, but also exhibit some special abilities ( e.g., in-\ncontext learning) that are not present in small-scale language models ( e.g., BERT). To discriminate the language models in different\nparameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size ( e.g.,\ncontaining tens or hundreds of billions of parameters). Recently, the research on LLMs has been largely advanced by both academia\nand industry, and a remarkable progress is the launch of ChatGPT (a powerful AI chatbot developed based on LLMs), which has\nattracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI\ncommunity, which would revolutionize the way how we develop and use AI algorithms. Considering this rapid technical progress, in this\nsurvey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular,\nwe focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Furthermore, we\nalso summarize the available resources for developing LLMs and discuss the remaining issues for future directions. This survey provides\nan up-to-date review of the literature on LLMs, which can be a useful resource for both researchers and engineers.\nIndex Terms \u2014Large Language Models; Emergent Abilities; Adaptation Tuning; Utilization; Alignment; Capacity Evaluation\n\u2726\n1 I NTRODUCTION\n\u201cThe limits of my language mean the limits of my world.\u201d\n\u2014Ludwig Wittgenstein\nLANGUAGE is a prominent ability in human beings to\nexpress and communicate, which develops in early\nchildhood and evolves over a lifetime [3, 4]. Machines,\nhowever, cannot naturally grasp the abilities of understand-\ning and communicating in the form of human language,\nunless equipped with powerful artificial intelligence (AI)\nalgorithms. It has been a longstanding research challenge\nto achieve this goal, to enable machines to read, write, and\ncommunicate like humans [5].\nTechnically, language modeling (LM) is one of the major\napproaches to advancing language intelligence of machines.\nIn general, LM aims to model the generative likelihood\nof word sequences, so as to predict the probabilities of\n\u2022Version: v13 (major update on November 23, 2023).\n\u2022GitHub link: https://github.com/RUCAIBox/LLMSurvey\n\u2022Chinese version link: https://github.com/RUCAIBox/LLMSurvey/blob/\nmain/assets/LLM Survey Chinese.pdf\n\u2022* K. Zhou and J. Li contribute equally to this work.\n\u2022The authors are mainly with Gaoling School of Artificial Intelligence and\nSchool of Information, Renmin University of China, Beijing, China; Jian-\nYun Nie is with DIRO, Universit\u00b4 e de Montr\u00b4 eal, Canada.\nContact e-mail: batmanfly@gmail.com\n\u2022The authors of this survey paper reserve all the copyrights of the fig-\nures/tables, and any use of these materials for publication purpose must be\nofficially granted by the survey authors.future (or missing) tokens. The research of LM has received\nextensive attention in the literature, which can be divided\ninto four major development stages:\n\u2022Statistical language models (SLM) . SLMs [6\u20139] are de-\nveloped based on statistical learning methods that rose in\nthe 1990s. The basic idea is to build the word prediction\nmodel based on the Markov assumption, e.g., predicting the\nnext word based on the most recent context. The SLMs with\na fixed context length nare also called n-gram language\nmodels, e.g., bigram and trigram language models. SLMs\nhave been widely applied to enhance task performance\nin information retrieval (IR) [10, 11] and natural language\nprocessing (NLP) [12\u201314]. However, they often suffer from\nthe curse of dimensionality: it is difficult to accurately\nestimate high-order language models since an exponential\nnumber of transition probabilities need to be estimated.\nThus, specially designed smoothing strategies such as back-\noff estimation [15] and Good\u2013Turing estimation [16] have\nbeen introduced to alleviate the data sparsity problem.\n\u2022Neural language models (NLM) . NLMs [1, 17, 18] charac-\nterize the probability of word sequences by neural networks,\ne.g., multi-layer perceptron (MLP) and recurrent neural net-\nworks (RNNs). As a remarkable contribution, the work in\n[1] introduced the concept of distributed representation of\nwords and built the word prediction function conditioned\non the aggregated context features ( i.e., the distributed\nword vectors). By extending the idea of learning effective\nfeatures for text data, a general neural network approacharXiv:2303.18223v13  [cs.CL]  24 Nov 20232\n/uni00000015/uni00000013/uni00000014/uni0000001b /uni00000015/uni00000013/uni00000014/uni0000001c /uni00000015/uni00000013/uni00000015/uni00000013 /uni00000015/uni00000013/uni00000015/uni00000014 /uni00000015/uni00000013/uni00000015/uni00000015 /uni00000015/uni00000013/uni00000015/uni00000016\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000014\n/uni00000025/uni00000028/uni00000035/uni00000037/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000015/uni00000037/uni00000018/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000016/uni00000026/uni00000052/uni00000047/uni00000048/uni0000005b/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000015/uni00000013/uni00000015/uni00000013 /uni00000015/uni00000013/uni00000015/uni00000014 /uni00000015/uni00000013/uni00000015/uni00000015 /uni00000015/uni00000013/uni00000015/uni00000016\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000013/uni00000015/uni00000018/uni00000013/uni00000018/uni00000013/uni00000013/uni0000001a/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000018/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000014/uni0000001a/uni00000018/uni00000013\n/uni00000037/uni00000018 /uni0000002a/uni00000033/uni00000037/uni00000010/uni00000016/uni00000026/uni00000052/uni00000047/uni00000048/uni0000005b/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n(a) Query=\u201dLanguage Model\u201d\n/uni00000015/uni00000013/uni00000014/uni0000001b /uni00000015/uni00000013/uni00000014/uni0000001c /uni00000015/uni00000013/uni00000015/uni00000013 /uni00000015/uni00000013/uni00000015/uni00000014 /uni00000015/uni00000013/uni00000015/uni00000015 /uni00000015/uni00000013/uni00000015/uni00000016\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013\n/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000014\n/uni00000025/uni00000028/uni00000035/uni00000037/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000015/uni00000037/uni00000018/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000016/uni00000026/uni00000052/uni00000047/uni00000048/uni0000005b/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017\n/uni00000015/uni00000013/uni00000015/uni00000013 /uni00000015/uni00000013/uni00000015/uni00000014 /uni00000015/uni00000013/uni00000015/uni00000015 /uni00000015/uni00000013/uni00000015/uni00000016\n/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000013/uni00000015/uni00000018/uni00000013/uni00000018/uni00000013/uni00000013/uni0000001a/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000018/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000014/uni0000001a/uni00000018/uni00000013\n/uni00000037/uni00000018 /uni0000002a/uni00000033/uni00000037/uni00000010/uni00000016/uni00000026/uni00000052/uni00000047/uni00000048/uni0000005b/uni0000002c/uni00000051/uni00000056/uni00000057/uni00000055/uni00000058/uni00000046/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni0000002f/uni0000002f/uni00000044/uni00000030/uni00000024/uni0000002a/uni00000033/uni00000037/uni00000010/uni00000017 (b) Query=\u201dLarge Language Model\u201d\nFig. 1: The trends of the cumulative numbers of arXiv papers that contain the keyphrases \u201c language model \u201d (since June 2018)\nand \u201c large language model \u201d (since October 2019), respectively. The statistics are calculated using exact match by querying\nthe keyphrases in title or abstract by months. We set different x-axis ranges for the two keyphrases, because \u201clanguage\nmodels\u201d have been explored at an earlier time. We label the points corresponding to important landmarks in the research\nprogress of LLMs. A sharp increase occurs after the release of ChatGPT: the average number of published arXiv papers\nthat contain \u201c large language model \u201d in title or abstract goes from 0.40 per day to 8.58 per day (Figure 1(b)).\nStatistical LMNeural LMPre-trained LMLLMTask solvingcapacity 1990s201320182020Word2vec (NPLM)!NLPSStatic word representationsNeural context modeling Solve typical NLP tasksn-gram modelsStatistical methods  Probability estimation Assist in specific tasksELMO!BERT!GPT-1/2Context-aware representationsPre-training + fine-tuningSolve various NLP tasksGPT-3/4!ChatGPT!ClaudeScaling language modelsPrompt based completionSolve various real-world tasksGeneral-purpose task solverSpecific task helperTask-agnostic feature learnerTransferable NLP task solver\nFig. 2: An evolution process of the four generations of language models (LM) from the perspective of task solving capacity.\nNote that the time period for each stage may not be very accurate, and we set the time mainly according to the publish\ndate of the most representative studies at each stage. For neural language models, we abbreviate the paper titles of\ntwo representative studies to name the two approaches: NPLM [1] (\u201c A neural probabilistic language model \u201d) and NLPS [2]\n(\u201cNatural language processing (almost) from scratch \u201d). Due to the space limitation, we don\u2019t list all representative studies in\nthis figure.\nwas developed to build a unified, end-to-end solution for\nvarious NLP tasks [2]. Furthermore, word2vec [19, 20] was\nproposed to build a simplified shallow neural network\nfor learning distributed word representations, which were\ndemonstrated to be very effective across a variety of NLP\ntasks. These studies have initiated the use of language\nmodels for representation learning (beyond word sequence\nmodeling), having an important impact on the field of NLP .\n\u2022Pre-trained language models (PLM) . As an early at-\ntempt, ELMo [21] was proposed to capture context-aware\nword representations by first pre-training a bidirectional\nLSTM (biLSTM) network (instead of learning fixed word\nrepresentations) and then fine-tuning the biLSTM network\naccording to specific downstream tasks. Furthermore, based\non the highly parallelizable Transformer architecture [22]\nwith self-attention mechanisms, BERT [23] was proposed bypre-training bidirectional language models with specially\ndesigned pre-training tasks on large-scale unlabeled cor-\npora. These pre-trained context-aware word representations\nare very effective as general-purpose semantic features,\nwhich have largely raised the performance bar of NLP\ntasks. This study has inspired a large number of follow-up\nwork, which sets the \u201c pre-training and fine-tuning \u201d learning\nparadigm. Following this paradigm, a great number of stud-\nies on PLMs have been developed, introducing either differ-\nent architectures [24, 25] ( e.g., GPT-2 [26] and BART [24]) or\nimproved pre-training strategies [27\u201329]. In this paradigm, it\noften requires fine-tuning the PLM for adapting to different\ndownstream tasks.\n\u2022Large language models (LLM) . Researchers find that\nscaling PLM ( e.g., scaling model size or data size) often\nleads to an improved model capacity on downstream tasks3\n(i.e.,following the scaling law [30]). A number of studies\nhave explored the performance limit by training an ever\nlarger PLM ( e.g., the 175B-parameter GPT-3 and the 540B-\nparameter PaLM). Although scaling is mainly conducted\nin model size (with similar architectures and pre-training\ntasks), these large-sized PLMs display different behaviors\nfrom smaller PLMs ( e.g., 330M-parameter BERT and 1.5B-\nparameter GPT-2) and show surprising abilities (called emer-\ngent abilities [31]) in solving a series of complex tasks. For\nexample, GPT-3 can solve few-shot tasks through in-context\nlearning , whereas GPT-2 cannot do well. Thus, the research\ncommunity coins the term \u201c large language models (LLM) \u201d1\nfor these large-sized PLMs [32\u201335], which attract increasing\nresearch attention (See Figure 1). A remarkable application\nof LLMs is ChatGPT2that adapts the LLMs from the GPT\nseries for dialogue, which presents an amazing conversation\nability with humans. We can observe a sharp increase of the\narXiv papers that are related to LLMs after the release of\nChatGPT in Figure 1.\nAs discussed before, language model is not a new tech-\nnical concept specially for LLMs, but has evolved with the\nadvance of artificial intelligence over the decades. Early lan-\nguage models mainly aim to model and generate text data,\nwhile latest language models ( e.g., GPT-4) focus on complex\ntask solving. From language modeling totask solving , it is an\nimportant leap in scientific thinking, which is the key to\nunderstand the development of language models in the re-\nsearch history. From the perspective of task solving, the four\ngenerations of language models have exhibited different lev-\nels of model capacities. In Figure 2, we describe the evolu-\ntion process of language models in terms of the task solving\ncapacity. At first, statistical language models mainly assisted\nin some specific tasks ( e.g., retrieval or speech tasks), in\nwhich the predicted or estimated probabilities can enhance\nthe performance of task-specific approaches. Subsequently,\nneural language models focused on learning task-agnostic\nrepresentations ( e.g., features), aiming to reduce the efforts\nfor human feature engineering. Furthermore, pre-trained\nlanguage models learned context-aware representations that\ncan be optimized according to downstream tasks. For the\nlatest generation of language model, LLMs are enhanced by\nexploring the scaling effect on model capacity, which can be\nconsidered as general-purpose task solvers. To summarize,\nin the evolution process, the task scope that can be solved\nby language models have been greatly extended, and the\ntask performance attained by language models have been\nsignificantly enhanced.\nIn the existing literature, PLMs have been widely dis-\ncussed and surveyed [36\u201339], while LLMs are seldom re-\nviewed in a systematic way. To motivate our survey, we first\nhighlight three major differences between LLMs and PLMs.\nFirst, LLMs display some surprising emergent abilities that\nmay not be observed in previous smaller PLMs. These abili-\nties are key to the performance of language models on com-\nplex tasks, making AI algorithms unprecedently powerful\nand effective. Second, LLMs would revolutionize the way\nthat humans develop and use AI algorithms. Unlike small\n1. Note that a LLM is not necessarily more capable than a small PLM,\nand emergent abilities may not occur in some LLMs.\n2. https://openai.com/blog/chatgpt/PLMs, the major approach to accessing LLMs is through\nthe prompting interface ( e.g., GPT-4 API). Humans have to\nunderstand how LLMs work and format their tasks in a way\nthat LLMs can follow. Third, the development of LLMs no\nlonger draws a clear distinction between research and en-\ngineering. The training of LLMs requires extensive practical\nexperiences in large-scale data processing and distributed\nparallel training. To develop capable LLMs, researchers\nhave to solve complicated engineering issues, working with\nengineers or being engineers.\nNowadays, LLMs are posing a significant impact on\nthe AI community, and the advent of ChatGPT and GPT-4\nleads to the rethinking of the possibilities of artificial general\nintelligence (AGI). OpenAI has published a technical article\nentitled \u201c Planning for AGI and beyond \u201d, which discusses\nthe short-term and long-term plans to approach AGI [40],\nand a more recent paper has argued that GPT-4 might be\nconsidered as an early version of an AGI system [41]. The\nresearch areas of AI are being revolutionized by the rapid\nprogress of LLMs. In the field of NLP , LLMs can serve as a\ngeneral-purpose language task solver (to some extent), and\nthe research paradigm has been shifting towards the use\nof LLMs. In the field of IR, traditional search engines are\nchallenged by the new information seeking way through AI\nchatbots ( i.e.,ChatGPT), and New Bing3presents an initial\nattempt that enhances the search results based on LLMs. In\nthe field of CV , the researchers try to develop ChatGPT-like\nvision-language models that can better serve multimodal\ndialogues [42\u201345], and GPT-4 [46] has supported multi-\nmodal input by integrating the visual information. This new\nwave of technology would potentially lead to a prosperous\necosystem of real-world applications based on LLMs. For\ninstance, Microsoft 365 is being empowered by LLMs ( i.e.,\nCopilot) to automate the office work, and OpenAI supports\nthe use of plugins in ChatGPT for implementing special\nfunctions.\nDespite the progress and impact, the underlying prin-\nciples of LLMs are still not well explored. Firstly, it is\nmysterious why emergent abilities occur in LLMs, instead of\nsmaller PLMs. As a more general issue, there lacks a deep,\ndetailed investigation of the key factors that contribute to\nthe superior abilities of LLMs. It is important to study when\nand how LLMs obtain such abilities [47]. Although there are\nsome meaningful discussions about this problem [31, 47],\nmore principled investigations are needed to uncover the\n\u201csecrets \u201c of LLMs. Secondly, it is difficult for the research\ncommunity to train capable LLMs. Due to the huge de-\nmand of computation resources, it is very costly to carry\nout repetitive, ablating studies for investigating the effect\nof various strategies for training LLMs. Indeed, LLMs are\nmainly trained by industry, where many important training\ndetails ( e.g., data collection and cleaning) are not revealed\nto the public. Thirdly, it is challenging to align LLMs with\nhuman values or preferences. Despite the capacities, LLMs\nare also likely to produce toxic, fictitious, or harmful con-\ntents. It requires effective and efficient control approaches\nto eliminating the potential risk of the use of LLMs [46].\nFaced with both opportunities and challenges, it needs\nmore attention on the research and development of LLMs. In\n3. https://www.bing.com/new4\norder to provide a basic understanding of LLMs, this survey\nconducts a literature review of the recent advances in LLMs\nfrom four major aspects, including pre-training (how to pre-\ntrain a capable LLM), adaptation (how to effectively adapt\npre-trained LLMs for better use), utilization (how to use\nLLMs for solving various downstream tasks) and capability\nevaluation (how to evaluate the abilities of LLMs and existing\nempirical findings). We thoroughly comb the literature and\nsummarize the key findings, techniques, and methods of\nLLMs. For this survey, we also create a GitHub project\nwebsite by collecting the supporting resources for LLMs, at\nthe link https://github.com/RUCAIBox/LLMSurvey. We\nare also aware of several related review articles on PLMs\nor LLMs [32, 36, 38, 39, 43, 48\u201354]. These papers either\ndiscuss PLMs or some specific (or general) aspects of LLMs.\nCompared with them, we focus on the techniques and\nmethods to develop and use LLMs and provide a relatively\ncomprehensive reference to important aspects of LLMs.\nThe remainder of this survey is organized as follows:\nSection 2 introduces the background for LLMs and the evo-\nlution of GPT-series models, followed by the summarization\nof available resources for developing LLMs in Section 3.\nSections 4, 5, 6, and 7 review and summarize the recent\nprogress from the four aspects of pre-training, adaptation,\nutilization, and capacity evaluation, respectively. Then, Sec-\ntion 8 discusses the practical guide for prompt design,\nand Section 9 reviews the applications of LLMs in several\nrepresentative domains. Finally, we conclude the survey in\nSection 10 by summarizing the major findings and discuss\nthe remaining issues for future work.\n2 O VERVIEW\nIn this section, we present an overview about the back-\nground of LLMs and then summarize the technical evolu-\ntion of the GPT-series models.\n2.1 Background for LLMs\nTypically, large language models (LLMs) refer to Transformer\nlanguage models that contain hundreds of billions (or\nmore) of parameters4, which are trained on massive text\ndata [32], such as GPT-3 [55], PaLM [56], Galactica [35],\nand LLaMA [57]. LLMs exhibit strong capacities to un-\nderstand natural language and solve complex tasks (via\ntext generation). To have a quick understanding of how\nLLMs work, this part introduces the basic background for\nLLMs, including scaling laws, emergent abilities and key\ntechniques.\nFormulation of Scaling Laws for LLMs . Currently, LLMs\nare mainly built upon the Transformer architecture [22],\nwhere multi-head attention layers are stacked in a very\ndeep neural network. Existing LLMs adopt similar Trans-\nformer architectures and pre-training objectives ( e.g., lan-\nguage modeling) as small language models. However, LLMs\nsignificantly extend the model size, data size, and total\n4. In existing literature, there is no formal consensus on the minimum\nparameter scale for LLMs, since the model capacity is also related to\ndata size and total compute. In this survey, we take a slightly loose\ndefinition of LLMs, and mainly focus on discussing language models\nwith a model size larger than 10B.compute (orders of magnification). Extensive research has\nshown that scaling can largely improve the model capacity\nof LLMs [26, 55, 56]. Thus, it is useful to establish a quantita-\ntive approach to characterizing the scaling effect. Next, we\nintroduce two representative scaling laws for Transformer\nlanguage models [30, 34].\n\u2022KM scaling law5. In 2020, Kaplan et al. [30] (the OpenAI\nteam) firstly proposed to model the power-law relationship\nof model performance with respective to three major factors,\nnamely model size ( N), dataset size ( D), and the amount of\ntraining compute ( C), for neural language models. Given\na compute budget c, they empirically presented three basic\nformulas for the scaling law6:\nL(N) =\u0012Nc\nN\u0013\u03b1N\n, \u03b1 N\u223c0.076, Nc\u223c8.8\u00d71013(1)\nL(D) =\u0012Dc\nD\u0013\u03b1D\n, \u03b1 D\u223c0.095, Dc\u223c5.4\u00d71013\nL(C) =\u0012Cc\nC\u0013\u03b1C\n, \u03b1 C\u223c0.050, Cc\u223c3.1\u00d7108\nwhere L(\u00b7)denotes the cross entropy loss in nats, and\na follow-up study [58] from OpenAI has shown that the\nlanguage modeling loss can be decomposed into two parts,\nnamely irreducible loss (the entropy of the true data distri-\nbution) and reducible loss (an estimate of the KL divergence\nbetween the true and model distributions). The three laws\nwere derived by fitting the model performance with varied\ndata sizes (22M to 23B tokens), model sizes (768M to 1.5B\nnon-embedding parameters) and training compute, under\nsome assumptions ( e.g., the analysis of one factor should\nbe not bottlenecked by the other two factors). They showed\nthat the model performance has a strong dependence rela-\ntion on the three factors.\n\u2022Chinchilla scaling law . As another representative study,\nHoffmann et al. [34] (the Google DeepMind team) proposed\nan alternative form for scaling laws to instruct the compute-\noptimal training for LLMs. They conducted rigorous exper-\niments by varying a larger range of model sizes (70M to\n16B) and data sizes (5B to 500B tokens), and fitted a similar\nscaling law yet with different coefficients as below [34]:\nL(N, D ) =E+A\nN\u03b1+B\nD\u03b2, (2)\nwhere E= 1.69, A= 406 .4, B= 410 .7,\u03b1= 0.34and\n\u03b2= 0.28. By optimizing the loss L(N, D )under the con-\nstraint C\u22486ND, they showed that the optimal allocation\nof compute budget to model size and data size can be\nderived as follows:\nNopt(C) =G\u0012C\n6\u0013a\n, D opt(C) =G\u22121\u0012C\n6\u0013b\n, (3)\nwhere a=\u03b1\n\u03b1+\u03b2,b=\u03b2\n\u03b1+\u03b2andGis a scaling coefficient that\ncan be computed by A,B,\u03b1and\u03b2. As analyzed in [34],\n5. Since there was not a model trained following this law in the\noriginal paper, we took the last names of the two co-first authors to\nname this scaling law.\n6. Here, Nc,Dcand Ccare measured in the number of non-\nembedding parameters, the number of training tokens and the number\nof FP-days, respectively. According to the original paper [30], CcandC\nshould be denoted by Cmin\nc andCmin, corresponding to the optimal\nuse of compute. We use the simplified notations for ease of discussions.5\ngiven an increase in compute budget, the KM scaling law\nfavors a larger budget allocation in model size than the data\nsize, while the Chinchilla scaling law argues that the two\nsizes should be increased in equal scales, i.e.,having similar\nvalues for aandbin Equation (3).\nDiscussion on Scaling Laws . After introducing the formu-\nlations, we continue to discuss scaling law in the following\ntwo aspects, to enhance its understanding:\n\u2022Predictable scaling . In practice, scaling law can be used\nto instruct the training of LLMs, and it has been proven\nfeasible to reliably estimate the performance of larger mod-\nels based on that of smaller models, called predictable scal-\ning [46]. The benefits of predictable scaling for training\nLLMs are mainly twofold. Firstly, for large models, it is\ninfeasible to rigorously examine various training tricks or\nvariants, and it would be very helpful if experiences gained\nfrom small models could also apply to large models. For\ninstance, small proxy models can be trained to find the\noptimal schedule of the data mixture for large models [59].\nSecondly, the training of large-scale models takes a long\ntime, often suffering from issues such as training loss spike,\nand scaling law can be employed to monitor the training\nstatus of LLMs, e.g.,identifying abnormal performance at an\nearly time. Despite that scaling law characterizes a smooth\ntrend of performance increase (or loss decrease), it also\nindicates that diminishing returns7might occur as model\nscaling. An empirical study [58] from the OpenAI team\nhas shown that representation quality or semantic content\ncan still effectively improve even if approaching the point\nof diminishing returns ( i.e., approaching the irreducible\nloss) [58]. This finding suggests that training large models\nare promising for improving the performance of down-\nstream tasks. To further explore scaling effect, a potential\nissue is that the amount of available data for training LLMs\nis actually limited. With the ever-increasing model scale, the\npublic text data would be soon \u201cexhausted\u201d for LLMs [60].\nThus, it will be meaningful to study how scaling laws apply\nto a data-constrained regime [61], where data repetition or\naugmentation might be useful to alleviate data scarcity.\n\u2022Task-level predictability . Existing research of scaling laws\nare mostly conducted in terms of language modeling loss\n(e.g., per-token cross-entropy loss in nats [30]), while in\npractice we are more concerned about the performance of\nLLMs on actual tasks. Thus, a basic problem is that how\nthe decrease of language modeling loss translates into the\nimprovement of task performance [58]. Intuitively, a model\nwith a smaller language modeling loss tends to yield a\nbetter performance on downstream tasks, since language\nmodeling loss can be considered as a general measure of\nthe overall model capacity. GPT-4 [46] has reported that\nsome capabilities ( e.g., coding ability) can be accurately\npredicted via scaling law. Despite that, readers should be\naware that a direct decrease in language modeling loss does\nnot always indicate an improvement of model performance\non downstream tasks. Specially, the phenomenon of inverse\nscaling would occur for some tasks, where task performance\nsurprisingly becomes worse as the language modeling loss\ndecreases [62]. Overall, it is more difficult to explore and\n7. https://en.wikipedia.org/wiki/Diminishing returnscharacterize task-level scaling laws, since it might be also\ndependent on task-related information (task metric, task\ndifficulty, etc.). Furthermore, some capacities ( e.g.,in-context\nlearning [55]) are unpredictable according to the scaling law,\nwhich can be observed only when the model size exceeds a\ncertain level (as discussed below).\nEmergent Abilities of LLMs . In the literature [31], emergent\nabilities of LLMs are formally defined as \u201cthe abilities that\nare not present in small models but arise in large models\u201d,\nwhich is one of the most prominent features that distin-\nguish LLMs from previous PLMs. It further introduces a\nnotable characteristic when emergent abilities occur [31]:\nperformance rises significantly above random when the\nscale reaches a certain level. By analogy, such an emergent\npattern has close connections with the phenomenon of phase\ntransition in physics [31, 63]. In principle, emergent abilities\ncan be defined in relation to some complex tasks [31, 64],\nwhile we are more concerned with general abilities that\ncan be applied to solve a variety of tasks. Here, we briefly\nintroduce three typical emergent abilities for LLMs and\nrepresentative models that possess such an ability8.\n\u2022In-context learning. The in-context learning (ICL) ability\nis formally introduced by GPT-3 [55]: assuming that the\nlanguage model has been provided with a natural language\ninstruction and/or several task demonstrations, it can gen-\nerate the expected output for the test instances by com-\npleting the word sequence of input text, without requiring\nadditional training or gradient update9. Among the GPT-\nseries models, the 175B GPT-3 model exhibited a strong ICL\nability in general, but not the GPT-1 and GPT-2 models. Such\nan ability also depends on the specific downstream task. For\nexample, the ICL ability can emerge on the arithmetic tasks\n(e.g., the 3-digit addition and subtraction) for the 13B GPT-3,\nbut 175B GPT-3 even cannot work well on the Persian QA\ntask [31].\n\u2022Instruction following. By fine-tuning with a mixture of\nmulti-task datasets formatted via natural language descrip-\ntions (called instruction tuning ), LLMs are shown to perform\nwell on unseen tasks that are also described in the form\nof instructions [28, 66, 67]. With instruction tuning, LLMs\nare enabled to follow the task instructions for new tasks\nwithout using explicit examples, thus having an improved\ngeneralization ability. According to the experiments in [67],\ninstruction-tuned LaMDA-PT [68] started to significantly\noutperform the untuned one on unseen tasks when the\nmodel size reached 68B, but not for 8B or smaller model\nsizes. A recent study [69] found that a model size of 62B is\nat least required for PaLM to perform well on various tasks\nin four evaluation benchmarks ( i.e.,MMLU, BBH, TyDiQA\nand MGSM), though a much smaller size might suffice for\nsome specific tasks ( e.g., MMLU).\n\u2022Step-by-step reasoning. For small language models, it\nis usually difficult to solve complex tasks that involve\n8. It is difficult to accurately examine the critical size for emergent\nabilities of LLMs ( i.e.,the minimum size to possess an ability), since it\nmight vary for different models or tasks. Also, existing studies often\ntest emergent abilities on very limited model sizes for a specific LLM.\nFor example, PaLM is often tested with three sizes of 8B, 62B and 540B.\nIt is unclear about the model performance of the untested sizes.\n9. In a recent study [65], it also shows that in-context learning implic-\nitly performs meta-optimization through the attention mechanism.6\nmultiple reasoning steps, e.g., mathematical word problems.\nIn contrast, with the chain-of-thought (CoT) prompting\nstrategy [33], LLMs can solve such tasks by utilizing the\nprompting mechanism that involves intermediate reasoning\nsteps for deriving the final answer. This ability is speculated\nto be potentially obtained by training on code [33, 47]. An\nempirical study [33] has shown that CoT prompting can\nbring performance gains (on arithmetic reasoning bench-\nmarks) when applied to PaLM and LaMDA variants with\na model size larger than 60B, while its advantage over\nthe standard prompting becomes more evident when the\nmodel size exceeds 100B. Furthermore, the performance\nimprovement with CoT prompting seems to be also varied\nfor different tasks, e.g., GSM8K >MAWPS >SWAMP for\nPaLM [33].\nHow Emergent Abilities Relate to Scaling Laws . In existing\nliterature [30, 31, 34], scaling laws and emergent abilities\nprovide two perspectives to understand the advantage of\nlarge models over small models. In general, scaling law\n(often measured by language modeling loss ) describes pre-\ndictable performance relation with the potential effect of\ndiminishing returns, while emergent abilities (often mea-\nsured by task performance ) are unpredictable but very prof-\nitable once such abilities actually emerge. Since the two\nperspectives reflect different performance trends (continu-\nous improvement v.s.sharp performance leap), they might\nlead to misaligned findings or observations. There are also\nextensive debates on the rationality of emergent abilities.\nA popular speculation is that emergent abilities might be\npartially attributed to the evaluation setting for special tasks\n(e.g., the discontinuous evaluation metrics) [70, 71]: when\nevaluation metrics are altered accordingly, the sharpness of\nthe emergent ability curve would disappear. However, the\nperformance of LLMs on most tasks are perceived by users\nnaturally in a discontinuous way. For instance, end users\nprefer a reliable code generated by LLMs that can success-\nfully pass the test case, but are less interested in selecting a\nbetter code with fewer errors between two failed ones. More\nrecently, a study [72] proposes a new evaluation setting\nthat can enlarge the resolution of task metrics, making task\nperformance more predictable. Despite these efforts, more\nfundamental research ( e.g., grokking10) about the working\nmechanism of LLMs is still in need to understand the emer-\ngence of certain abilities. The subtle relation between scaling\nlaw and emergent abilities can be explained by analogy with\nthe ability acquisition of human11. Take the speaking ability\nas an example. For children, language development (espe-\ncially infants) can be also considered as a multi-level process\nwhere \u201cemergent abilities\u201d occur. Specially, the language\nability would relatively stable within a time interval, but\nqualitative change only occurs when evolving into another\nability level ( e.g., from speaking simple words to speaking\nsimple sentences). Such a learning process is essentially not\nsmooth and stable (i.e.,language ability does not develop at\na constant rate over time), though a child actually grows\n10. Grokking refers that \u201ca pattern in the data, improving generaliza-\ntion performance from random chance level to perfect generalization\u201d,\nquoted from the original paper [73].\n11. This explanation is only for ease of understanding, and there is\nnot direct evidence to connect the two points.every day. It is interesting that young parents would be often\nsurprised by unexpected progress of the speaking ability\nexhibited by their babies.\nKey Techniques for LLMs . It has been a long way that\nLLMs evolve into the current state: general and capable\nlearners. In the development process, a number of impor-\ntant techniques are proposed, which largely improve the\ncapacity of LLMs. Here, we briefly list several important\ntechniques that (potentially) lead to the success of LLMs, as\nfollows.\n\u2022Scaling . As discussed in previous parts, there exists\nan evident scaling effect in Transformer language mod-\nels: larger model/data sizes and more training compute\ntypically lead to an improved model capacity [30, 34]. As\ntwo representative models, GPT-3 and PaLM explored the\nscaling limits by increasing the model size to 175B and\n540B, respectively. Since compute budget is usually limited,\nscaling laws can be further employed to conduct a more\ncompute-efficient allocation of the compute resources. For\nexample, Chinchilla (with more training tokens) outper-\nforms its counterpart model Gopher (with a larger model\nsize) by increasing the data scale with the same compute\nbudget [34]. In addition, data scaling should be with careful\ncleaning process, since the quality of pre-training data plays\na key role in the model capacity.\n\u2022Training . Due to the huge model size, it is very chal-\nlenging to successfully train a capable LLM. Distributed\ntraining algorithms are needed to learn the network param-\neters of LLMs, in which various parallel strategies are of-\nten jointly utilized. To support distributed training, several\noptimization frameworks have been released to facilitate\nthe implementation and deployment of parallel algorithms,\nsuch as DeepSpeed [74] and Megatron-LM [75\u201377]. Also, op-\ntimization tricks are also important for training stability and\nmodel performance, e.g., restart to overcome training loss\nspike [56] and mixed precision training [78]. More recently,\nGPT-4 [46] proposes to develop special infrastructure and\noptimization methods that reliably predict the performance\nof large models with much smaller models.\n\u2022Ability eliciting . After being pre-trained on large-scale\ncorpora, LLMs are endowed with potential abilities as\ngeneral-purpose task solvers. These abilities might not be\nexplicitly exhibited when LLMs perform some specific tasks.\nAs the technical approach, it is useful to design suitable task\ninstructions or specific in-context learning strategies to elicit\nsuch abilities. For instance, chain-of-thought prompting has\nbeen shown to be useful to solve complex reasoning tasks\nby including intermediate reasoning steps. Furthermore,\nwe can perform instruction tuning on LLMs with task\ndescriptions expressed in natural language, for improving\nthe generalizability of LLMs on unseen tasks. These eliciting\ntechniques mainly correspond to the emergent abilities of\nLLMs, which may not show the same effect on small lan-\nguage models.\n\u2022Alignment tuning . Since LLMs are trained to capture\nthe data characteristics of pre-training corpora (including\nboth high-quality and low-quality data), they are likely to\ngenerate toxic, biased, or even harmful content for humans.\nIt is necessary to align LLMs with human values, e.g., helpful ,\nhonest , and harmless . For this purpose, InstructGPT [66]7\ndesigns an effective tuning approach that enables LLMs to\nfollow the expected instructions, which utilizes the tech-\nnique of reinforcement learning with human feedback [66, 79].\nIt incorporates human in the training loop with elaborately\ndesigned labeling strategies. ChatGPT is indeed developed\non a similar technique to InstructGPT, which shows a strong\nalignment capacity in producing high-quality, harmless re-\nsponses, e.g., rejecting to answer insulting questions.\n\u2022Tools manipulation . In essence, LLMs are trained as text\ngenerators over massive plain text corpora, thus performing\nless well on the tasks that are not best expressed in the\nform of text ( e.g., numerical computation). In addition, their\ncapacities are also limited to the pre-training data, e.g., the\ninability to capture up-to-date information. To tackle these\nissues, a recently proposed technique is to employ external\ntools to compensate for the deficiencies of LLMs [80, 81].\nFor example, LLMs can utilize the calculator for accurate\ncomputation [80] and employ search engines to retrieve\nunknown information [81]. More recently, ChatGPT has\nenabled the mechanism of using external plugins (existing\nor newly created apps)12, which are by analogy with the\n\u201ceyes and ears \u201d of LLMs. Such a mechanism can broadly\nexpand the scope of capacities for LLMs.\nIn addition, many other factors ( e.g., the upgrade of\nhardware) also contribute to the success of LLMs. Currently,\nwe limit our discussion to the major technical approaches\nand key findings for developing LLMs.\n2.2 Technical Evolution of GPT-series Models\nDue to the excellent capacity in communicating with hu-\nmans, ChatGPT has ignited the excitement of the AI com-\nmunity since its release. ChatGPT is developed based on the\npowerful GPT model with specially optimized conversation\ncapacities. Considering the ever-growing interest in Chat-\nGPT and GPT models, we add a special discussion about the\ntechnical evolution of the GPT-series models, to briefly sum-\nmarize the progress how they have been developed in the\npast years. Meanwhile, we drew a schematic diagram de-\npicting the technological evolution of the GPT-series models\nin Figure 4. The basic principle underlying GPT models is\nto compress the world knowledge into the decoder-only\nTransformer model by language modeling, such that it can\nrecover (or memorize) the semantics of world knowledge\nand serve as a general-purpose task solver. Two key points\nto the success are (I) training decoder-only Transformer\nlanguage models that can accurately predict the next word\nand (II) scaling up the size of language models . Overall, the\nresearch of OpenAI on LLMs can be roughly divided into\nthe following stages13.\nEarly Explorations . According to one interview with Ilya\nSutskever14(a co-founder and chief scientist of OpenAI),\nthe idea of approaching intelligent systems with language\n12. https://openai.com/blog/chatgpt-plugins\n13. Note that the discussion of this part can be somewhat subjective.\nThe overall viewpoints and summaries are made based on the under-\nstanding of the survey authors by reading the papers, blog articles,\ninterview reports and APIs released by OpenAI.\n14. https://hackernoon.com/an-interview-with-ilya-sutskever-co-\nfounder-of-openaimodels was already explored in the early days of Ope-\nnAI, while it was attempted with recurrent neural net-\nworks (RNN) [121]. With the advent of Transformer, OpenAI\ndeveloped two initial GPT models, namely GPT-1 [122] and\nGPT-2 [26], which can be considered as the foundation to\nmore powerful models subsequently i.e.,GPT-3 and GPT-4.\n\u2022GPT-1 . In 2017, the Transformer model [22] was intro-\nduced by Google, and the OpenAI team quickly adapted\ntheir language modeling work to this new neural network\narchitecture. They released the first GPT model in 2018,\ni.e., GPT-1 [122], and coined the abbreviation term GPT\nas the model name, standing for Generative Pre-Training .\nGPT-1 was developed based on a generative, decoder-only\nTransformer architecture, and adopted a hybrid approach of\nunsupervised pretraining and supervised fine-tuning. GPT-\n1 has set up the core architecture for the GPT-series models\nand established the underlying principle to model natural\nlanguage text, i.e.,predicting the next word.\n\u2022GPT-2 . Following a similar architecture of GPT-1,\nGPT-2 [26] increased the parameter scale to 1.5B, which\nwas trained with a large webpage dataset WebText. As\nclaimed in the paper of GPT-2, it sought to perform\ntasks via unsupervised language modeling, without explicit\nfine-tuning using labeled data. To motivate the approach,\nthey introduced a probabilistic form for multi-task solving,\ni.e.,p(output |input, task )(similar approaches have been\nadopted in [123]), which predicts the output conditioned on\nthe input and task information. To model this conditional\nprobability, language text can be naturally employed as a\nunified way to format input, output and task information.\nIn this way, the process of solving a task can be cast as a\nword prediction problem for generating the solution text.\nFurther, they introduced a more formal claim for this idea:\n\u201cSince the (task-specific) supervised objective is the same\nas the unsupervised (language modeling) objective but only\nevaluated on a subset of the sequence, the global minimum\nof the unsupervised objective is also the global minimum\nof the supervised objective (for various tasks)\u201d [26]15. A\nbasic understanding of this claim is that each (NLP) task\ncan be considered as the word prediction problem based\non a subset of the world text. Thus, unsupervised language\nmodeling could be capable in solving various tasks, if it was\ntrained to have sufficient capacity in recovering the world\ntext. These early discussion in GPT-2\u2019s paper echoed in the\ninterview of Ilya Sutskever by Jensen Huang: \u201cWhat the\nneural network learns is some representation of the process\nthat produced the text. This text is actually a projection of\nthe world...the more accurate you are in predicting the next\nword, the higher the fidelity, the more resolution you get in\nthis process...\u201d16.\nCapacity Leap . Although GPT-2 is intended to be an \u201cun-\nsupervised multitask learner\u201d, it overall has an inferior\nperformance compared with supervised fine-tuning state-\nof-the-art methods. Because it has a relatively small model\nsize, it has been widely fine-tuned in downstream tasks,\nespecially the dialog tasks [124, 125]. Based on GPT-2, GPT-3\n15. To better understand this sentence, we put some explanation\nwords in parentheses.\n16. https://lifearchitect.ai/ilya/8\nTABLE 1: Statistics of large language models (having a size larger than 10B in this survey) in recent years, including the\ncapacity evaluation, pre-training data scale (either in the number of tokens or storage size) and hardware resource costs.\nIn this table, we only include LLMs with a public paper about the technical details. Here, \u201cRelease Time\u201d indicates the\ndate when the corresponding paper was officially released. \u201cPublicly Available\u201d means that the model checkpoints can be\npublicly accessible while \u201cClosed Source\u201d means the opposite. \u201cAdaptation\u201d indicates whether the model has been with\nsubsequent fine-tuning: IT denotes instruction tuning and RLHF denotes reinforcement learning with human feedback.\n\u201cEvaluation\u201d indicates whether the model has been evaluated with corresponding abilities in their original paper: ICL\ndenotes in-context learning and CoT denotes chain-of-thought. \u201c*\u201d denotes the largest publicly available version.\nAdaptation EvaluationModelRelease\nTimeSize\n(B)Base\nModel IT RLHFPre-train\nData ScaleLatest Data\nTimestampHardware\n(GPUs / TPUs)Training\nTime ICL CoT\nT5 [82] Oct-2019 11 - - - 1T tokens Apr-2019 1024 TPU v3 - \u2713 -\nmT5 [83] Oct-2020 13 - - - 1T tokens - - - \u2713 -\nPanGu- \u03b1[84] Apr-2021 13* - - - 1.1TB - 2048 Ascend 910 - \u2713 -\nCPM-2 [85] Jun-2021 198 - - - 2.6TB - - - - -\nT0 [28] Oct-2021 11 T5 \u2713 - - - 512 TPU v3 27 h \u2713 -\nCodeGen [86] Mar-2022 16 - - - 577B tokens - - - \u2713 -\nGPT-NeoX-20B [87] Apr-2022 20 - - - 825GB - 96 40G A100 - \u2713 -\nTk-Instruct [88] Apr-2022 11 T5 \u2713 - - - 256 TPU v3 4 h \u2713 -\nUL2 [89] May-2022 20 - - - 1T tokens Apr-2019 512 TPU v4 - \u2713 \u2713\nOPT [90] May-2022 175 - - - 180B tokens - 992 80G A100 - \u2713 -\nNLLB [91] Jul-2022 54.5 - - - - - - - \u2713 -\nCodeGeeX [92] Sep-2022 13 - - - 850B tokens - 1536 Ascend 910 60 d \u2713 -\nGLM [93] Oct-2022 130 - - - 400B tokens - 768 40G A100 60 d \u2713 -\nFlan-T5 [69] Oct-2022 11 T5 \u2713 - - - - - \u2713 \u2713\nBLOOM [78] Nov-2022 176 - - - 366B tokens - 384 80G A100 105 d \u2713 -\nmT0 [94] Nov-2022 13 mT5 \u2713 - - - - - \u2713 -\nGalactica [35] Nov-2022 120 - - - 106B tokens - - - \u2713 \u2713\nBLOOMZ [94] Nov-2022 176 BLOOM \u2713 - - - - - \u2713 -\nOPT-IML [95] Dec-2022 175 OPT \u2713 - - - 128 40G A100 - \u2713 \u2713\nLLaMA [57] Feb-2023 65 - - - 1.4T tokens - 2048 80G A100 21 d \u2713 -\nPythia [96] Apr-2023 12 - - - 300B tokens - 256 40G A100 - \u2713 -\nCodeGen2 [97] May-2023 16 - - - 400B tokens - - - \u2713 -\nStarCoder [98] May-2023 15.5 - - - 1T tokens - 512 40G A100 - \u2713 \u2713\nLLaMA2 [99] Jul-2023 70 - \u2713 \u2713 2T tokens - 2000 80G A100 - \u2713 -\nBaichuan2 [100] Sep-2023 13 - \u2713 \u2713 2.6T tokens - 1024 A800 - \u2713 -\nQWEN [101] Sep-2023 14 - \u2713 \u2713 3T tokens - - - \u2713 -\nFLM [102] Sep-2023 101 - \u2713 - 311B tokens - 192 A800 22 d \u2713 -Publicly\nAvailable\nSkywork [103] Oct-2023 13 - - - 3.2T tokens - 512 80G A800 - \u2713 -\nGPT-3 [55] May-2020 175 - - - 300B tokens - - - \u2713 -\nGShard [104] Jun-2020 600 - - - 1T tokens - 2048 TPU v3 4 d - -\nCodex [105] Jul-2021 12 GPT-3 - - 100B tokens May-2020 - - \u2713 -\nERNIE 3.0 [106] Jul-2021 10 - - - 375B tokens - 384 V100 - \u2713 -\nJurassic-1 [107] Aug-2021 178 - - - 300B tokens - 800 GPU - \u2713 -\nHyperCLOVA [108] Sep-2021 82 - - - 300B tokens - 1024 A100 13.4 d \u2713 -\nFLAN [67] Sep-2021 137 LaMDA-PT \u2713 - - - 128 TPU v3 60 h \u2713 -\nYuan 1.0 [109] Oct-2021 245 - - - 180B tokens - 2128 GPU - \u2713 -\nAnthropic [110] Dec-2021 52 - - - 400B tokens - - - \u2713 -\nWebGPT [81] Dec-2021 175 GPT-3 - \u2713 - - - - \u2713 -\nGopher [64] Dec-2021 280 - - - 300B tokens - 4096 TPU v3 920 h \u2713 -\nERNIE 3.0 Titan [111] Dec-2021 260 - - - - - - - \u2713 -\nGLaM [112] Dec-2021 1200 - - - 280B tokens - 1024 TPU v4 574 h \u2713 -\nLaMDA [68] Jan-2022 137 - - - 768B tokens - 1024 TPU v3 57.7 d - -\nMT-NLG [113] Jan-2022 530 - - - 270B tokens - 4480 80G A100 - \u2713 -\nAlphaCode [114] Feb-2022 41 - - - 967B tokens Jul-2021 - - - -\nInstructGPT [66] Mar-2022 175 GPT-3 \u2713 \u2713 - - - - \u2713 -\nChinchilla [34] Mar-2022 70 - - - 1.4T tokens - - - \u2713 -\nPaLM [56] Apr-2022 540 - - - 780B tokens - 6144 TPU v4 - \u2713 \u2713\nAlexaTM [115] Aug-2022 20 - - - 1.3T tokens - 128 A100 120 d \u2713 \u2713\nSparrow [116] Sep-2022 70 - - \u2713 - - 64 TPU v3 - \u2713 -\nWeLM [117] Sep-2022 10 - - - 300B tokens - 128 A100 40G 24 d \u2713 -\nU-PaLM [118] Oct-2022 540 PaLM - - - - 512 TPU v4 5 d \u2713 \u2713\nFlan-PaLM [69] Oct-2022 540 PaLM \u2713 - - - 512 TPU v4 37 h \u2713 \u2713\nFlan-U-PaLM [69] Oct-2022 540 U-PaLM \u2713 - - - - - \u2713 \u2713\nGPT-4 [46] Mar-2023 - - \u2713 \u2713 - - - - \u2713 \u2713\nPanGu- \u03a3[119] Mar-2023 1085 PanGu- \u03b1 - - 329B tokens - 512 Ascend 910 100 d \u2713 -Closed\nSource\nPaLM2 [120] May-2023 16 - \u2713 - 100B tokens - - - \u2713 \u27139\n2020\n20232021\n1-4\n5-8\n9-10\n1-3\n7-10\n1 1-12\nT5\nGPT -3\nWebGPT\nBLOOMZ\nGalaticamT0\n2019\nFLAN\nInstructGPT\nGPT -NeoX-20BCodeGen\nOPT\nOPT -IML\nMT-NLGT0\nTk-Instruct\nGPT -4\nGShard\nUL2\nPaLM Flan-T5\nFlan-PaLMSparr ow\nChatGPT\nErnie 3.0 Titan\nYuan 1.0\nGopher\nGLaMmT5\n PanGu- \ud835\udec2\nPLUG\nLaMDA\nCPM-2\nHyperCLOV ACodexJurassic-1\nErnie 3.0\nAnthr opic\nNLLBCoher eLuminousYaLM1 1-12\n2022\nGLM\nAlexaTMBLOOM\nWeLM\nAlphaCode\nChinchilla\nCodeGeeX\nFalconCodeGen2\n5-8\nLLaMA2\nStarCoder\nPaLM2BaichuanRWKV MPTInternLM\nXVERSEQWEN\nSkywork\n9-1 1Publicly Available\n4-6\n1-4\nLLaMAPanGu-\u03a3\nBardPythia\nVicuna\n Baichuan2\nAquila2\nGrok-1FLM\nFig. 3: A timeline of existing large language models (having a size larger than 10B) in recent years. The timeline was\nestablished mainly according to the release date ( e.g., the submission date to arXiv) of the technical paper for a model. If\nthere was not a corresponding paper, we set the date of a model as the earliest time of its public release or announcement.\nWe mark the LLMs with publicly available model checkpoints in yellow color. Due to the space limit of the figure, we only\ninclude the LLMs with publicly reported evaluation results.\nGPT-1\n2018.06\ndecoder -only architecture\ngenerative pre -trainingGPT-2\n2019.02\nunsupervised multitask learner\nscaling the model sizein-context learning\nexploring scaling limitscode pre -training\ngpt-3.5-turbo\n2023.03\nexcellent comprehensive abilitytext-davinci -002\n2022.03\ninstruction followingcode -davinci -002\n2022.03\ncapable code model+code\n+chat +RLHF +instructionCodex\n2021.07GPT-3\n2020.05GPT-4\n2023.03GPT-3.5\n2022.03\ntext-davinci -003\n2022.09\nhuman alignmentGPT-4 Turbo\n2023.09\nlonger context window\nGPT-4 Turbo with vision\n2023.09\nmultimodal abilityChatGPT\nstrong reasoning ability\nFig. 4: A brief illustration for the technical evolution of GPT-series models. We plot this figure mainly based on the papers,\nblog articles and official APIs from OpenAI. Here, solid lines denote that there exists an explicit evidence ( e.g., the official\nstatement that a new model is developed based on a base model) on the evolution path between two models, while dashed\nlines denote a relatively weaker evolution relation.\ndemonstrates a key capacity leap by scaling of the (nearly\nsame) generative pre-training architecture.\n\u2022GPT-3 . GPT-3 [55] was released in 2020, which scaled\nthe model parameters to an ever larger size of 175B. In\nthe GPT-3\u2019s paper, it formally introduced the concept of\nin-context learning (ICL)17, which utilizes LLMs in a few-\nshot or zero-shot way. ICL can teach (or instruct) LLMs to\nunderstand the tasks in the form of natural language text.\nWith ICL, the pre-training and utilization of LLMs converge\nto the same language modeling paradigm: pre-training pre-\ndicts the following text sequence conditioned on the context,\nwhile ICL predicts the correct task solution, which can be\nalso formatted as a text sequence, given the task description\n17. GPT-2 essentially used ICL for unsupervised task learning,\nthough it wasn\u2019t called ICL at that time.and demonstrations. GPT-3 not only demonstrates very ex-\ncellent performance in a variety of NLP tasks, but also on a\nnumber of specially designed tasks that require the abilities\nof reasoning or domain adaptation. Although the GPT-3\u2019s\npaper does not explicitly discuss the emergent abilities of\nLLMs, we can observe large performance leap that might\ntranscend the basic scaling law [30], e.g., larger models have\nsignificantly stronger ICL ability (illustrated in the original\nFigure 1.2 of the GPT-3\u2019s paper [55]). Overall, GPT-3 can be\nviewed as a remarkable landmark in the journey evolving\nfrom PLMs to LLMs. It has empirically proved that scaling\nthe neural networks to a significant size can lead to a huge\nincrease in model capacity.\nCapacity Enhancement . Due to the strong capacities, GPT-\n3 has been the base model to develop even more capable10\nLLMs for OpenAI. Overall, OpenAI has explored two major\napproaches to further improving the GPT-3 model, i.e.,train-\ning on code data and alignment with human preference,\nwhich are detailed as follows.\n\u2022Training on code data . A major limitation of the original\nGPT-3 model (pre-trained on plain text) lies in the lack of\nthe reasoning ability on complex tasks, e.g., completing the\ncode and solving math problems. To enhance this ability,\nCodex [105] was introduced by OpenAI in July 2021, which\nwas a GPT model fine-tuned on a large corpus of GitHub\ncode. It demonstrated that Codex can solve very difficult\nprogramming problems, and also lead to a significant per-\nformance improvement in solving math problems [126].\nFurther, a contrastive approach [127] to training text and\ncode embedding was reported in January 2022, which was\nshown to improve a series of related tasks ( i.e., linear-\nprobe classification, text search and code search). Actually,\nthe GPT-3.5 models are developed based on a code-based\nGPT model ( i.e.,code-davinci-002 ), which indicates that\ntraining on code data is a very useful practice to improve\nthe model capacity of GPT models, especially the reasoning\nability. Furthermore, there is also a speculation that train-\ning on code data can greatly increase the chain-of-thought\nprompting abilities of LLMs [47], while it is still worth\nfurther investigation with more thorough verification.\n\u2022Human alignment . The related research of human\nalignment can be dated back to the year 2017 (or earlier)\nfor OpenAI: a blog article entitled \u201clearning from human\npreferences\u201d18was posted on the OpenAI blog describing\na work that applied reinforcement learning (RL) to learn\nfrom the preference comparisons annotated by humans [79]\n(similar to the reward training step in the aligning algorithm\nof InstructGPT in Figure 12). Shortly after the release of this\nRL paper [79], the paper of the Proximal Policy Optimiza-\ntion (PPO) [128] was published in July 2017, which now has\nbeen the foundational RL algorithm for learning from hu-\nman preferences [66]. Later in January 2020, GPT-2 was fine-\ntuned using the aforementioned RL algorithms [79, 128],\nwhich leveraged human preferences to improve the capac-\nities of GPT-2 on NLP tasks. In the same year, another\nwork [129] trained a summarization model for optimizing\nhuman preferences in a similar way. Based on these prior\nwork, InstructGPT [66] was proposed in January 2022 to\nimprove the GPT-3 model for human alignment, which\nformally established a three-stage reinforcement learning from\nhuman feedback (RLHF) algorithm. Note that it seems that\nthe wording of \u201c instruction tuning \u201d has seldom been used in\nOpenAI\u2019s paper and documentation, which is substituted by\nsupervised fine-tuning on human demonstrations (i.e.,the first\nstep of the RLHF algorithm [66]). In addition to improving\nthe instruction following capacity, the RLHF algorithm is\nparticularly useful to mitigate the issues of generating harm\nor toxic content for LLMs, which is key to the safe deploy-\nment of LLMs in practice. OpenAI describes their approach\nto alignment research in a technical article [130], which\nhas summarized three promising directions: \u201ctraining AI\nsystems to use human feedback, to assist human evaluation\nand to do alignment research\u201d.\nThese enhancement techniques lead to the improved\n18. https://openai.com/research/learning-from-human-preferencesGPT-3 models with stronger capacities, which are called\nGPT-3.5 models by OpenAI (see the discussion about the\nOpenAI API in Section 3.1).\nThe Milestones of Language Models . Based on all the ex-\nploration efforts, two major milestones have been achieved\nby OpenAI, namely ChatGPT [131] and GPT-4 [46], which\nhave largely raised the capacity bar of existing AI systems.\n\u2022ChatGPT . In November 2022, OpenAI released the\nconversation model ChatGPT, based on the GPT models\n(GPT-3.5 and GPT-4). As the official blog article intro-\nduced [131], ChatGPT was trained in a similar way as\nInstructGPT (called \u201ca sibling model to InstructGPT\u201d in the\noriginal post), while specially optimized for dialogue. They\nreported a difference between the training of ChatGPT and\nInstructGPT in the data collection setup: human-generated\nconversations (playing both the roles of user and AI) are\ncombined with the InstructGPT dataset in a dialogue format\nfor training ChatGPT. ChatGPT exhibited superior capaci-\nties in communicating with humans: possessing a vast store\nof knowledge, skill at reasoning on mathematical problems,\ntracing the context accurately in multi-turn dialogues, and\naligning well with human values for safe use. Later on, the\nplugin mechanism has been supported in ChatGPT, which\nfurther extends the capacities of ChatGPT with existing tools\nor apps. So far, it seems to be the ever most powerful chatbot\nin the AI history. The launch of ChatGPT has a significant\nimpact on the AI research in the future, which sheds light\non the exploration of human-like AI systems.\n\u2022GPT-4 . As another remarkable progress, GPT-4 [46] was\nreleased in March 2023, which extended the text input to\nmultimodal signals. Overall, GPT-4 has stronger capacities\nin solving complex tasks than GPT-3.5, showing a large\nperformance improvement on many evaluation tasks. A re-\ncent study [41] investigated the capacities of GPT-4 by con-\nducting qualitative tests with human-generated problems,\nspanning a diverse range of difficult tasks, and showed\nthat GPT-4 can achieve more superior performance than\nprior GPT models such as ChatGPT. Furthermore, GPT-4\nresponds more safely to malicious or provocative queries,\ndue to a six-month iterative alignment (with an additional\nsafety reward signal in the RLHF training). In the technical\nreport, OpenAI has emphasized how to safely develop\nGPT-4 and applied a number of intervention strategies to\nmitigate the possible issues of LLMs, such as hallucinations,\nprivacy and overreliance. For example, they introduced the\nmechanism called red teaming [132] to reduce the harm or\ntoxic content generation. As another important aspect, GPT-\n4 has been developed on a well-established deep learning\ninfrastructure with improved optimization methods. They\nintroduced a new mechanism called predictable scaling that\ncan accurately predict the final performance with a small\nproportion of compute during model training.\n\u2022GPT-4V , GPT-4 turbo, and beyond . Based on the work\ndone for GPT-4 [46], OpenAI further released GPT-4V in\nSeptember 2023, which focused on the safe deployment of\nthe vision capabilities of GPT-4. In the GPT-4V\u2019s system\ncard [133], it has extensively discussed the assessment and\nmitigation of risks related to visually augmented inputs.\nSpecially, GPT-4V exhibited strong vision capacities in var-\nious application scenarios, showing the great potential as11\na powerful multimodal learning system. More recently, in\nNovember 2023, OpenAI released an upgraded generation\nof GPT-4 model at DevDay, named GPT-4 Turbo , with a\nseries of technical improvements. GPT-4 Turbo is featured\nby the improved model capacity (more capable than GPT-\n4), the extended knowledge source (up to April 2023),\nlong context window (up to 128k tokens), optimized model\nperformance (cheaper price), and other useful functional-\nity updates (function call, reproducible outputs, etc.). At\nthe same time, Assistants API was launched to ease the\nrapid development of agent-like assistants. With this API,\ndevelopers can easily create goal-oriented assistants within\ntheir applications, by leveraging specific instruction, extra\nknowledge and tool use. Furthermore, multimodal capaci-\nties (see, hear, and speak) were also enhanced in this new\nrelease, supported by GPT-4 Turbo with vision, DALL\u00b7E 3,\nText-to-speech (TTS), and Listen to voice samples. These\nimprovements have greatly extended the capacity scope and\nenhanced the task performance of GPT models. More impor-\ntantly, the application ecosystem will be greatly strength-\nened with the technology upgrade in improved models,\nAPIs, and functionalities.\nDespite the huge progress, there are still limitations with\nthese superior LLMs, e.g., generating hallucinations with\nfactual errors or potentially risky response within some\nspecific context [46]. More limitations or issues of LLMs will\nbe discussed in Section 7. It poses long-standing research\nchallenges to develop more capable, safer LLMs. From\nthe perspective of engineering, OpenAI has adopted an\niterative deployment strategy [134] to develop the models\nand products by following a five-stage development and\ndeployment life-cycle, which aims to effectively reduce the\npotential risks of using the models. In the following, we\nwill dive into the technical details in order to have a specific\nunderstanding of how they have been developed.\n3 R ESOURCES OF LLM S\nIt is by no means an easy job to develop or reproduce LLMs,\nconsidering the challenging technical issues and huge de-\nmands of computation resources. A feasible way is to learn\nexperiences from existing LLMs and reuse publicly avail-\nable resources for incremental development or experimental\nstudy. In this section, we briefly summarize the publicly\navailable resources for developing LLMs, including model\ncheckpoints (or APIs), corpora and libraries.\n3.1 Publicly Available Model Checkpoints or APIs\nGiven the huge cost of model pre-training, well-trained\nmodel checkpoints are critical to the study and development\nof LLMs for the research community. Since the parameter\nscale is a key factor to consider for using LLMs, we cate-\ngorize these public models into two scale levels ( i.e., tens\nof billions of parameters and hundreds of billions of parameters ),\nwhich is useful for users to identify the suitable resources ac-\ncording to their resource budget. In addition, for inference,\nwe can directly employ public APIs to perform our tasks,\nwithout running the model locally. Next, we introduce the\npublicly available model checkpoints and APIs.Models with Tens of Billions of Parameters . Most of the\nmodels in this category have a parameter scale ranging from\n10B to 20B, except LLaMA [57] and LLaMA2 [99] (con-\ntaining 70B parameters in the largest version), NLLB [91]\n(containing 54.5B parameters in the largest version), and\nFalcon [135] (containing 40B parameters in the largest ver-\nsion). Other models within this range include mT5 [83],\nPanGu- \u03b1[84], T0 [28], GPT-NeoX-20B [87], CodeGen [86],\nUL2 [89], Flan-T5 [69], and mT0 [94]. Among them, Flan-\nT5 (11B version) can serve as a premier model for re-\nsearch on instruction tuning, since it explores the instruction\ntuning from three aspects [69]: increasing the number of\ntasks, scaling the model size, and fine-tuning with chain-of-\nthought prompting data. Besides, CodeGen (11B version), as\nan autoregressive language model designed for generating\ncode, can be considered as a good candidate for exploring\nthe code generation ability. It also introduces a new bench-\nmark MTPB [86] specially for multi-turn program synthesis,\nwhich is composed by 115 expert-generated problems. To\nsolve these problems, it requires LLMs to acquire sufficient\nprogramming knowledge ( e.g., math, array operations, and\nalgorithms). More recently, CodeGen2 [97] has been released\nto explore the impact of choices in model architecture,\nlearning algorithms, and data distributions on the model. As\nanother LLM specialized in coding abilities, StarCoder [98]\nhas also achieved excellent results. As for multilingual tasks,\nmT0 (13B version) might be a good candidate model, which\nhas been fine-tuned on multilingual tasks with multilingual\nprompts. Furthermore, PanGu- \u03b1[84] shows good perfor-\nmance in Chinese downstream tasks in zero-shot or few-\nshot settings, which is developed based on the deep learn-\ning framework MindSpore [136]. Note that PanGu- \u03b1[84]\nholds multiple versions of models (up to 200B parameters),\nwhile the largest public version has 13B parameters. As\na popular LLM, LLaMA (65B version) [57], which contains\napproximately five times as many parameters as other mod-\nels, has exhibited superior performance in tasks related to\ninstruction following. Compared to LLaMA, LLaMA2 [99]\nhas made more explorations in reinforcement learning from\nhuman feedback (RLHF) and developed a chat-oriented\nversion called LLaMA-chat , which generally outperforms ex-\nisting open-source models across a range of helpfulness and\nsafety benchmarks. Due to the openness and effectiveness,\nLLaMA has attracted significant attention from the research\ncommunity, and many efforts [137\u2013140] have been devoted\nto fine-tuning or continually pre-training its different model\nversions for implementing new models or tools. More\nrecently, Falcon [135], as another open-source LLM, has also\nachieved very excellent performance on open benchmarks.\nIt is featured by a more careful data cleaning process to\nprepare the pre-training data (with a publicly shared dataset\nRefinedWeb [141]). Typically, pre-training models at this\nscale require hundreds or even thousands of GPUs or TPUs.\nFor instance, GPT-NeoX-20B uses 12 supermicro servers,\neach equipped with 8 NVIDIA A100-SXM4-40GB GPUs,\nwhile LLaMA utilizes 2,048 A100-80G GPUs as reported\nin their original publications. To accurately estimate the\ncomputation resources needed, it is suggested to use the\nmetrics measuring the number of involved computations\nsuch as FLOPS (i.e.,FLoating point number Operations Per\nSecond) [30].12\nLLaMA\nBenT saoBaize\nKoalaZiyaBELLE\nLLaMA\nAdapterGuanacoAlpaca\nLora\nLawyer\nLLaMA+ chat data\n+ task dataLLaV A\nInstructBLIPY ulan-Chat\n+ task data\nMultimodal models+ task dataData inheritanceModel inheritance\nV icuna\nAlpacaPanda\nPandaGPTCornucopiaChinese\nLLaMA\nT aoLi\n+ chat data+ chat data\n+ task data\nChinese\nAlpaca\nChatMed+ synthetic dataChinese\nV icuna\nLinly-Chinese-LLaMAOpen-Chinese-LLaMA\n+ task data\nLA WGPT\nRLHF\nPKU-Beaver\nChatbridgeOpenFlamingo\nV isionLLMMiniGPT -4Goat\nQiZhenGPT+ chat data\nBiLLa\n+ task data\nMath\n Finance\nContinue pr e-training\nInstruction\ntuning\nLaw Bilingualism Education MedicineParameter -efficient fine-tuning\nFull parameter  fine-tuning\n+ chinese data\n+ synthetic data\n+ Alpaca data\nFig. 5: An evolutionary graph of the research work conducted on LLaMA. Due to the huge number, we cannot include all\nthe LLaMA variants in this figure, even much excellent work. To support incremental update, we share the source file of\nthis figure, and welcome the readers to include the desired models by submitting the pull requests on our GitHub page.\nModels with Hundreds of Billions of Parameters . For\nmodels in this category, only a handful of models have been\npublicly released. For example, OPT [90], OPT-IML [95],\nBLOOM [78], and BLOOMZ [94] have nearly the same num-\nber of parameters as GPT-3 (175B version), while GLM [93]\nand Galactica [35] have 130B and 120B parameters, re-\nspectively. Among them, OPT (175B version), with the\ninstruction-tuned version OPT-IML, has been specially mo-\ntivated for open sharing, which aims to enable researchers\nto carry out reproducible research at scale. For research\nin cross-lingual generalization, BLOOM (176B version) and\nBLOOMZ (176B version) can be used as base models, due to\nthe competence in multilingual language modeling tasks.\nAs a bilingual LLM, GLM has also provided a popular\nsmall-sized Chinese chat model ChatGLM2-6B (a updated\nversion for ChatGLM-6B), which is featured with many\nimprovements in efficiency and capacity ( e.g., quantization,\n32K-length context, fast inference rate). Models of this scale\ntypically require thousands of GPUs or TPUs to train. For\ninstance, OPT (175B version) used 992 A100-80GB GPUs,\nwhile GLM (130B version) used a cluster of 96 NVIDIA\nDGX-A100 (8x40G) GPU nodes.\nLLaMA Model Family . The collection of LLaMA mod-\nels [57] were introduced by Meta AI in February, 2023,\nconsisting of four sizes (7B, 13B, 30B and 65B). Since\nreleased, LLaMA has attracted extensive attention from\nboth research and industry communities. LLaMA mod-els have achieved very excellent performance on various\nopen benchmarks, which have become the most popu-\nlar open language models thus far. A large number of\nresearchers have extended LLaMA models by either in-\nstruction tuning or continual pretraining. In particular, in-\nstruction tuning LLaMA has become a major approach\nto developing customized or specialized models, due to\nthe relatively low computational costs. To effectively adapt\nLLaMA models in non-English languages, it often needs to\nextend the original vocabulary (trained mainly on English\ncorpus) or fine-tune it with instructions or data in the\ntarget language. Among these extended models, Stanford\nAlpaca [142] is the first open instruct-following model\nfine-tuned based on LLaMA (7B). It is trained by 52K\ninstruction-following demonstrations generated via self-\ninstruct [143] using text-davinci-003 . The instruction\ndata, named Alpaca-52K , and training code have been ex-\ntensively adopted in subsequent work, such as Alpaca-\nLoRA [144] (a reproduction of Stanford Alpaca using\nLoRA [145]), Koala [146], and BELLE [147]. In addition, Vi-\ncuna [138] is another popular LLaMA variant, trained upon\nuser-shared conversations collected from ShareGPT [148].\nDue to the excellent performance and availability of the\nLLaMA model family, many multimodal models incorpo-\nrate them as the base language models, to achieve strong\nlanguage understanding and generation abilities. Compared\nwith other variants, Vicuna is more preferred in multimodal13\nlanguage models, which have led to the emergence of a va-\nriety of popular models, including LLaVA [149], MiniGPT-\n4 [150], InstructBLIP [151], and PandaGPT [152]. The re-\nlease of LLaMA has greatly advanced the research progress\nof LLMs. To summarize the research work conducted on\nLLaMA, we present a brief evolutionary graph in Figure 5.\nPublic API of LLMs . Instead of directly using the model\ncopies, APIs provide a more convenient way for common\nusers to use LLMs, without the need of running the model\nlocally. As a representative interface for using LLMs, the\nAPIs for the GPT-series models [46, 55, 66, 105] have\nbeen widely used for both academia and industry19.\nOpenAI has provided seven major interfaces to the models\nin GPT-3 series: ada,babbage ,curie ,davinci (the\nmost powerful version in GPT-3 series), text-ada-001 ,\ntext-babbage-001 , and text-curie-001 . Among\nthem, the first four interfaces can be further fine-\ntuned on the host server of OpenAI. In particular,\nbabbage ,curie , and davinci correspond to the\nGPT-3 (1B), GPT-3 (6.7B), and GPT-3 (175B) models,\nrespectively [55]. In addition, there are also two APIs\nrelated to Codex [105], called code-cushman-001 (a\npowerful and multilingual version of the Codex (12B) [105])\nand code-davinci-002 . Further, GPT-3.5 series\ninclude one base model code-davinci-002 and\nthree enhanced versions, namely text-davinci-002 ,\ntext-davinci-003 , and gpt-3.5-turbo . As more\npowerful alternatives, in this year, OpenAI has released\nthe model interfaces for GPT-4 series, including gpt-4 ,\ngpt-4-32k ,gpt-4-1106-preview (i.e., GPT-4 Turbo)\nand gpt-4-vision-preview (i.e., GPT-4 Turbo with\nvision, a multimodal model). It is worth noting that OpenAI\nhas been maintaining and upgrading these model interfaces\n(gpt-3.5-turbo ,gpt-4 ,gpt-4-32k ), so the API name\nwill actually point to the latest version. Currently, ChatGPT\ncan be powered by either GPT-3.5 or GPT-4 models. Overall,\none select the suitable model interface based on the specific\napplication scenarios and response requirements. The\ndetailed usage can be found on their project websites20.\nTABLE 2: Statistics of commonly-used data sources.\nCorpora Size Source Latest Update Time\nBookCorpus [153] 5GB Books Dec-2015\nGutenberg [154] - Books Dec-2021\nC4 [82] 800GB CommonCrawl Apr-2019\nCC-Stories-R [155] 31GB CommonCrawl Sep-2019\nCC-NEWS [27] 78GB CommonCrawl Feb-2019\nREALNEWs [156] 120GB CommonCrawl Apr-2019\nOpenWebText [157] 38GB Reddit links Mar-2023\nPushift.io [158] 2TB Reddit links Mar-2023\nWikipedia [159] 21GB Wikipedia Mar-2023\nBigQuery [160] - Codes Mar-2023\nthe Pile [161] 800GB Other Dec-2020\nROOTS [162] 1.6TB Other Jun-2022\n3.2 Commonly Used Corpora for Pre-training\nIn contrast to earlier PLMs, LLMs which consist of a signifi-\ncantly larger number of parameters require a higher volume\n19. https://platform.openai.com/docs/api-reference/introduction\n20. https://platform.openai.com/docs/models/overviewof training data that covers a broad range of content. For\nthis need, there are increasingly more accessible training\ndatasets that have been released for research. In this section,\nwe will briefly summarize several widely used corpora for\ntraining LLMs. Based on their content types, we catego-\nrize these corpora into six groups: Books, CommonCrawl,\nReddit links, Wikipedia, Code, and others.\nBooks. BookCorpus [153] is a commonly used dataset in\nprevious small-scale models ( e.g.,GPT [122] and GPT-2 [26]),\nconsisting of over 11,000 books covering a wide range of\ntopics and genres ( e.g., novels and biographies). Another\nlarge-scale book corpus is Project Gutenberg [154], consist-\ning of over 70,000 literary books including novels, essays,\npoetry, drama, history, science, philosophy, and other types\nof works in the public domain. It is currently one of the\nlargest open-source book collections, which is used in train-\ning of MT-NLG [113] and LLaMA [57]. As for Books1 [55]\nand Books2 [55] used in GPT-3 [55], they are much larger\nthan BookCorpus but have not been publicly released so\nfar.\nCommonCrawl. CommonCrawl [163] is one of the largest\nopen-source web crawling databases, containing a petabyte-\nscale data volume, which has been widely used as training\ndata for existing LLMs. As the whole dataset is very large,\nexisting studies mainly extract subsets of web pages from\nit within a specific period. However, due to the widespread\nexistence of noisy and low-quality information in web data,\nit is necessary to perform data preprocessing before usage.\nBased on CommonCrawl, there are four filtered datasets\nthat are commonly used in existing work: C4 [82], CC-\nStories [155], CC-News [27], and RealNews [156]. The Colos-\nsal Clean Crawled Corpus (C4) includes five variants21,\nnamely en (806G), en.noclean (6T), realnewslike (36G), web-\ntextlike (17G), and multilingual (38T). The enversion has\nbeen utilized for pre-training T5 [82], LaMDA [68], Go-\npher [64], and UL2 [89]. The multilingual C4, also called\nmC4, has been used in mT5 [83]. CC-Stories (31G) is com-\nposed of a subset of CommonCrawl data, in which the\ncontents are made in a story-like way. Because the original\nsource of CC-Stories is not available now, we include a re-\nproduction version, CC-Stories-R [164], in Table 2. Moreover,\ntwo news corpora extracted from CommonCrawl, i.e.,RE-\nALNEWS (120G) and CC-News (76G), are also commonly\nused as the pre-training data.\nReddit Links. Reddit is a social media platform that enables\nusers to submit links and text posts, which can be voted on\nby others through \u201cupvotes\u201d or \u201cdownvotes\u201d. Highly up-\nvoted posts are often considered useful, and can be utilized\nto create high-quality datasets. WebText [26] is a well-known\ncorpus composed of highly upvoted links from Reddit, but it\nis not publicly available. As a surrogate, there is a readily ac-\ncessible open-source alternative called OpenWebText [157].\nAnother corpus extracted from Reddit is PushShift.io [158],\na real-time updated dataset that consists of historical data\nfrom Reddit since its creation day. Pushshift provides not\nonly monthly data dumps but also useful utility tools to\nsupport users in searching, summarizing, and conducting\n21. https://www.tensorflow.org/datasets/catalog/c414\npreliminary investigations on the entire dataset. This makes\nit easy for users to collect and process Reddit data.\nWikipedia. Wikipedia [159] is an online encyclopedia con-\ntaining a large volume of high-quality articles on diverse\ntopics. Most of these articles are composed in an expository\nstyle of writing (with supporting references), covering a\nwide range of languages and fields. Typically, the English-\nonly filtered versions of Wikipedia are widely used in most\nLLMs ( e.g., GPT-3 [55], LaMDA [68], and LLaMA [57]).\nWikipedia is available in multiple languages, so it can be\nused in multilingual settings.\nCode. To collect code data, existing work mainly crawls\nopen-source licensed codes from the Internet. Two major\nsources are public code repositories under open-source li-\ncenses ( e.g., GitHub) and code-related question-answering\nplatforms ( e.g., StackOverflow). Google has publicly re-\nleased the BigQuery dataset [160], which includes a substan-\ntial number of open-source licensed code snippets in various\nprogramming languages, serving as a representative code\ndataset. CodeGen has utilized BIGQUERY [86], a subset of\nthe BigQuery dataset, for training the multilingual version\nof CodeGen (CodeGen-Multi).\nOthers. The Pile [161] is a large-scale, diverse, and open-\nsource text dataset consisting of over 800GB of data from\nmultiple sources, including books, websites, codes, scientific\npapers, and social media platforms. It is constructed from\n22 diverse high-quality subsets. The Pile dataset is widely\nused in models with different parameter scales, such as\nGPT-J (6B) [165], CodeGen (16B) [86], and Megatron-Turing\nNLG (530B) [113]. ROOTS [162] is composed of various\nsmaller datasets (totally 1.61 TB of text) and covers 59\ndifferent languages (containing natural languages and pro-\ngramming languages), which have been used for training\nBLOOM [78].\nIn practice, it commonly requires a mixture of different\ndata sources for pre-training LLMs (see Figure 6), instead\nof a single corpus. Therefore, existing studies commonly\nmix several ready-made datasets ( e.g., C4, OpenWebText,\nand the Pile), and then perform further processing to obtain\nthe pre-training corpus. Furthermore, to train the LLMs that\nare adaptive to specific applications, it is also important\nto extract data from relevant sources ( e.g., Wikipedia and\nBigQuery) for enriching the corresponding information in\npre-training data. To have a quick reference of the data\nsources used in existing LLMs, we present the pre-training\ncorpora of three representative LLMs:\n\u2022GPT-3 (175B) [55] was trained on a mixed dataset of\n300B tokens, including CommonCrawl [163], WebText2 [55],\nBooks1 [55], Books2 [55], and Wikipedia [159].\n\u2022PaLM (540B) [56] uses a pre-training dataset of 780B\ntokens, which is sourced from social media conversations,\nfiltered webpages, books, Github, multilingual Wikipedia,\nand news.\n\u2022LLaMA [57] extracts training data from various sources,\nincluding CommonCrawl, C4 [82], Github, Wikipedia,\nbooks, ArXiv, and StackExchange. The training data size for\nLLaMA (6B) and LLaMA (13B) is 1.0T tokens, while 1.4T\ntokens are used for LLaMA (32B) and LLaMA (65B).TABLE 3: A detailed list of available collections for instruc-\ntion tuning.\nCategories Collections Time #Examples\nTaskNat. Inst. [166] Apr-2021 193K\nFLAN [67] Sep-2021 4.4M\nP3 [167] Oct-2021 12.1M\nSuper Nat. Inst. [88] Apr-2022 5M\nMVPCorpus [168] Jun-2022 41M\nxP3 [94] Nov-2022 81M\nOIG[169] Mar-2023 43M\nChatHH-RLHF [170] Apr-2022 160K\nHC3 [171] Jan-2023 87K\nShareGPT [148] Mar-2023 90K\nDolly [172] Apr-2023 15K\nOpenAssistant [173] Apr-2023 161K\nSyntheticSelf-Instruct [143] Dec-2022 82K\nAlpaca [137] Mar-2023 52K\nGuanaco [174] Mar-2023 535K\nBaize [175] Apr-2023 158K\nBELLE [176] Apr-2023 1.5M\nTABLE 4: A list of available collections for alignment.\nDataset Release Time #Examples\nSummarize from Feedback [129] Sep-2020 193K\nSHP [177] Oct-2021 385K\nWebGPT Comparisons [81] Dec-2021 19K\nStack Exchange Preferences [178] Dec-2021 10M\nHH-RLHF [170] Apr-2022 169K\nSandbox Alignment Data [179] May-2023 169K\nCValues [180] Jul-2023 145K\nPKU-SafeRLHF [181] Oct-2023 330K\n3.3 Commonly Used Datasets for Fine-tuning\nAfter pre-training, it requires further fine-tuning LLMs to\nenhance the model capacity, which often involve two major\nsteps, namely instruction tuning (supervised fine-tuning)\nand alignment tuning. In this section, we mainly focus on\ndiscussing the related available datasets for the two kinds of\ntuning approaches, and more algorithm details can be found\nin Section 5.\n3.3.1 Instruction Tuning Datasets\nAfter pre-training, instruction tuning ( a.k.a., supervised fine-\ntuning) is an important method to enhance or unlock spe-\ncific abilities of LLMs ( e.g., instruction following). In this\npart, we introduce several widely used datasets for in-\nstruction tuning, and categorize them into three main types\nbased on the construction method of formatted instruction\ninstances, namely NLP task datasets, daily chat datasets and\nsynthetic datasets. We show their details in Table 3.\nNLP Task Datasets. This kind of datasets are formatted\nbased on collected NLP task datasets ( e.g., text classifica-\ntion and summarization) with corresponding natural lan-\nguage task descriptions. In this category, P3 [182] and\nFLAN [67, 183] are two widely used datasets for instruction\ntuning.\n\u2022P3[182] is composed of 170 English NLP datasets and\n2,052 English prompt templates, where the input and output\nof each data example have been formatted with specific\nprompt templates for composing the training instance.15\n\u2022FLAN [67] consists of 62 widely used NLP benchmarks\nin its original version. Recently, FLAN-v2 [183] is also pro-\nposed, which expands FLAN by mixing additional instruc-\ntion datasets, including Muffin [67], NIV2 [88], T0-SF [28],\nand CoT [184\u2013186]. Muffin contains 62 tasks from the orig-\ninal FLAN and additional 26 tasks, including conversation\nand code synthesis tasks. T0-SF is extracted from T0 [28]\nwhile ensuring no overlap with Muffin. NIV2 refers to the\nNatural-Instructions v2 dataset [88], and CoT [184\u2013186] is\na combination of nine reasoning tasks with corresponding\nchain-of-thought prompts and outputs.\nDaily Chat Datasets. This kind of datasets are constructed\nbased on real user conversations where queries are posed\nby humans and responses are mainly generated by hu-\nman labelers or LLMs ( e.g., ChatGPT, GPT-4). The con-\nversation types include open-ended generation, question\nanswering, brainstorming, and chatting. In this category,\nShareGPT [148], OpenAssistant [173] and Dolly [172] are\nthree commonly used datasets for LLM fine-tuning.\n\u2022ShareGPT [148] is collected from a data collection\nplatform where users can upload their conversations with\nChatGPT or GPT-4 through the ShareGPT API. Currently,\nthis dataset consists of approximately 90,000 conversations,\nincluding real instructions or inquiries from human and\nresponses from ChatGPT.\n\u2022OpenAssistant [173] is a multilingual corpus containing\n66,497 real-world conversation trees between human and AI\nassistant. Each conversation tree consists of multiple nodes,\nand each node represents the information generated by a\nrole in the dialogue. It spans 35 languages and includes\n461,292 manually annotated quality ratings of responses.\n\u2022Dolly [172] is an English dataset comprising 15,000\nhuman-generated data instances (prompt-response pairs)\nfrom Databricks. This dataset covers seven domains out-\nlined in the InstructGPT [66], including brainstorming, clas-\nsification, closed-book quality assurance, generation, infor-\nmation extraction, open-book quality assurance, and sum-\nmarization.\nSynthetic Datasets. This kind of datasets are typically\nconstructed by instructing LLMs, based on pre-defined\nguidance rules or methods. In this category, Self-Instruct-\n52K [143], Alpaca [142] and Baize [175] are three commonly\nused synthetic datasets for LLMs.\n\u2022Self-Instruct-52K [143] is an instruction dataset gener-\nated through the self-instruct [143] method, consisting of\n82,000 instances with 52,000 instructions. Concretely, the\nauthors construct 175 seed instances, and then iteratively\nprompt the LLM [55] to synthesize additional instructions\nbased on randomly selected 8 instructions as reference.\nSubsequently, the LLM is further instructed to generate in-\nstance inputs and their corresponding outputs based on the\nsynthetic instructions, and finally obtain the Self-Instruct-\n52K dataset.\n\u2022Alpaca [142] is also a synthetic dataset based on the self-\ninstruct [143] method. It utilizes the text-davinci-003\nmodel on the 175 seed datasets from Self-Instruct-52K to\nobtain 52,000 new instructions and corresponding inputs\nand outputs. Moreover, 60% of the examples are pure in-\nstructions without the input part in the final dataset.\u2022Baize [175] is an English multi-turn conversation corpus\nconstructed using ChatGPT, comprising 111.5K instances. To\ncreate Baize, a method called \u201cself-chat\u201d [175] is purposed,\nwhere ChatGPT takes on the roles of both the user and the\nAI assistant in turns, generating information in a conversa-\ntional format.\n3.3.2 Alignment Datasets\nApart from instruction tuning, it is important to construct\nhigh-quality datasets for aligning LLMs with human values\nand preferences ( e.g., helpfulness, honesty, and harmless-\nness). In this section, we introduce several widely used\ndatasets for alignment tuning, including HH-RLHF [170],\nSHP [177], PKU-SafeRLHF [181], Stack Exchange Prefer-\nences [178] and Sandbox Alignment Data [179]. We show\ntheir details in Table 4.\n\u2022HH-RLHF [170] consists of around 169K instances, and\ncan be divided into two parts that focus on the helpfulness\nand harmlessness of LLMs, respectively. Each instance is\nan open-ended conversation between a crowdworker and\na chat model, about seeking assistance, advice, or task\ncompletion. The chat model provides two responses to each\nuser query, and the more helpful or harmful responses will\nbe chosen as the annotations.\n\u2022SHP [177] focuses on the helpfulness of responses.\nIt comprises 385K collective human preferences over re-\nsponses to questions/instructions across 18 diverse subject\nareas, spanning topics from cooking to legal advice. Each\ninstance is a Reddit post containing a question or instruction\nand a pair of top-level comments, one of which is deemed\nas more preferable by Reddit users and the other one is\ndeemed as less helpful. Different from HH-RLHF [170], the\ndata in SHP consists of naturally occurring and human-\nwritten responses.\n\u2022PKU-SafeRLHF [181] encompasses more than 330K\ninstances of expert comparison data, concentrating on the\nhelpfulness and harmlessness. Each instance in the dataset\nincludes a question and two responses, accompanied by\nsafety labels for each response and two preference anno-\ntations between the two responses according to helpfulness\nand harmlessness. The harmlessness of a response indicates\nits classification as risk-neutral across all 14 harm categories,\nwhile the helpfulness of a response is evaluated based on its\neffectiveness in addressing the question.\n\u2022Stack Exchange Preferences [178] focuses on the help-\nfulness of answers. It comprises about 10M questions and\nanswers from Stack Overflow. Each instance consists of a\nquestion and more than two corresponding answers. Each\nanswer is annotated with a score calculated based on its\nvotes and a label denoting whether it is selected.\n\u2022Sandbox Alignment Data [179] is an alignment dataset\ncontaining feedback from LLMs rather than human. It\ncomes from a virtual interaction environment called SAND-\nBOX, where the model simulates social interactions with\nother models and revise responses according to the feedback\nfrom other models. The dataset contains 169K instances, and\neach instance consists of a societal query, several responses,\nand corresponding ratings from other models.16\n3.4 Library Resource\nIn this part, we briefly introduce a series of available li-\nbraries for developing LLMs.\n\u2022Transformers [187] is an open-source Python library\nfor building models using the Transformer architecture,\nwhich is developed and maintained by Hugging Face. It\nhas a simple and user-friendly API, making it easy to use\nand customize various pre-trained models. It is a powerful\nlibrary with a large and active community of users and\ndevelopers who regularly update and improve the models\nand algorithms.\n\u2022DeepSpeed [74] is a deep learning optimization library\n(compatible with PyTorch) developed by Microsoft, which\nhas been used to train a number of LLMs, such as MT-\nNLG [113] and BLOOM [78]. It provides the support of\nvarious optimization techniques for distributed training,\nsuch as memory optimization (ZeRO technique, gradient\ncheckpointing), and pipeline parallelism.\n\u2022Megatron-LM [75\u201377] is a deep learning library devel-\noped by NVIDIA for training large-scale language models.\nIt also provides rich optimization techniques for distributed\ntraining, including model and data parallelism, mixed-\nprecision training, and FlashAttention. These optimization\ntechniques can largely improve the training efficiency and\nspeed, enabling efficient distributed training across GPUs.\n\u2022JAX [188] is a Python library for high-performance\nmachine learning algorithms developed by Google, allow-\ning users to easily perform computations on arrays with\nhardware acceleration ( e.g.,GPU or TPU). It enables efficient\ncomputation on various devices and also supports several\nfeatured functions, such as automatic differentiation and\njust-in-time compilation.\n\u2022Colossal-AI [189] is a deep learning library developed\nby HPC-AI Tech for training large-scale AI models. It is\nimplemented based on PyTorch and supports a rich collec-\ntion of parallel training strategies. Furthermore, it can also\noptimize heterogeneous memory management with meth-\nods proposed by PatrickStar [190]. Recently, a ChatGPT-like\nmodel called ColossalChat [140] has been publicly released\nwith two versions (7B and 13B), which are developed using\nColossal-AI based on LLaMA [57].\n\u2022BMTrain [191] is an efficient library developed by\nOpenBMB for training models with large-scale parameters\nin a distributed manner, which emphasizes code simplicity,\nlow resource, and high availability. BMTrain has already\nincorporated several common LLMs ( e.g., Flan-T5 [69] and\nGLM [93]) into its ModelCenter, where developers can use\nthese models directly.\n\u2022FastMoE [192] is a specialized training library for MoE\n(i.e.,mixture-of-experts) models. It is developed based on\nPyTorch, prioritizing both efficiency and user-friendliness\nin its design. FastMoE simplifies the process of transferring\nTransformer models to MoE models and supports both data\nparallelism and model parallelism during training.\n\u2022vLLM [193] is a fast, memory efficient, and easy-\nto-use library for LLM inference and serving. To enable\nfast inference, it is specially optimized with high serving\nthroughput, effective attention memory management using\nPagedAttention [193], continuous batching, and optimized\nCUDA kernels. Furthermore, vLLM also supports variousdecoding algorithms, tensor parallelism and streaming out-\nputs. To ease the integration with other systems, vLLM is\nfriendly to the use of HuggingFace models, and also provide\nOpenAI-compatible API servers.\n\u2022DeepSpeed-MII [194] is also a memory efficient\nPython library developed by DeepSpeed [74]. It aims to\ndemocratize LLMs inference by prioritizing high through-\nput, low latency, and cost-effectiveness. DeepSpeed-MII\nachieves accelerated text generation inference by leveraging\nfour essential technologies: blocked KV caching, continuous\nbatching, dynamic SplitFuse, and high-performance CUDA\nKernels. It currently supports over 13,000 models across\nthree popular model architectures, such as LLaMA [57],\nMistral [195], and OPT [90].\n\u2022DeepSpeed-Chat [196] is a fast, cost-effective, and\neasy-to-use system framework that enables the integration\nof the complete RLHF process during model training. It\nis featured by three major functionalities: (1) it simplifies\nthe training and inference process for ChatGPT-like models,\nenabling using a simple script to implement multiple train-\ning or inference steps; (2) it replicates the training mode\nof InstructGPT [66] and provides a complete pipeline for\nthree training steps ( i.e.,SFT, reward model fine-tuning, and\nRLHF); (3) it integrates the training engine and inference en-\ngine of Deepspeed into a unified hybrid engine (Deepspeed\nHE) for RLHF training, which enables seamless switch be-\ntween training and inference modes, and leveraging various\noptimizations from DeepSpeed Inference.\nIn addition to the above library resources, existing deep\nlearning frameworks ( e.g., PyTorch [197], TensorFlow [198],\nMXNet [199], PaddlePaddle [200], MindSpore [136] and\nOneFlow [201]) have also provided the support for parallel\nalgorithms, which are commonly used for training large-\nscale models.\n4 P RE-TRAINING\nPre-training establishes the basis of the abilities of LLMs. By\npre-training on large-scale corpora, LLMs can acquire essen-\ntial language understanding and generation skills [55, 56]. In\nthis process, the scale and quality of the pre-training corpus\nare critical for LLMs to attain powerful capabilities. Fur-\nthermore, to effectively pre-train LLMs, model architectures,\nacceleration methods, and optimization techniques need to\nbe well designed. In what follows, we first discuss the data\ncollection and processing in Section 4.1, then introduce the\ncommonly used model architectures in Section 4.2, and fi-\nnally present the training techniques to stably and efficiently\noptimize LLMs in Section 4.3.\n4.1 Data Collection and Preparation\nCompared with small-scale language models, LLMs have\na stronger demand for high-quality data for model pre-\ntraining, and their model capacities largely rely on the pre-\ntraining corpus and how it has been preprocessed. In this\npart, we discuss the collection and processing of pre-training\ndata, including data sources, preprocessing methods, and\nimportant analysis of how pre-training data affects the\nperformance of LLMs.17\nPaLM (540B)\n5%\n14%\n50%\n31%GPT-3 (175B)\n16%\n84%LLaMA (65B)\n5%\n2%\n87%Chinchilla (70B)\n4%\n40%\n56%\nGalactica (120B)\n7%\n86%\n8%T5 (11B)\n100%\nCodeGen (16B)\n39%\n25%\n10%\n6%\n20%GPT-NeoX (20B)\n8%\n38%\n15%\n10%\n30%Gopher (280B)\n3%\n37%\n60%\nLaMDA (137B)\n13%\n50%\n38%MT-NLG (530B)\n2%\n4%\n26%\n6%\n62%\nGLaM (1200B)\n22%\n30%\n48%AlphaCode (41B)\n100%Falcon (40B)\n100%3%5%\n\ud83d\udcda BookCorpus (5G, 2015), \n\ud83d\udcda Gutenberg (-, 2021), \n\ud83d\udcda CC-Stories-R (31G, 2019), \n\ud83d\udcf0 CC-NEWES (78G, 2019), \n\ud83d\udcf0 REALNEWs (120G, 2019)\n\ud83d\udcac the Pile - StackExchange (41G, 2020)\n\ud83d\udcbb C4 (800G, 2019), \n\ud83d\udcbb OpenWebText (38G, 2023), \n\ud83d\udcbb Wikipedia (21G, 2023)\n\ud83d\udd2c the Pile - ArXiv (72G, 2020), \n\ud83d\udd2c the Pile - PubMed Abstracts (25G, 2020)\n\u2328 BigQuery (-, 2023), the Pile - GitHub (61G, 2020)\nFig. 6: Ratios of various data sources in the pre-training data for existing LLMs.\n4.1.1 Data Source\nTo develop a capable LLM, it is key to collect a large amount\nof natural language corpus from various data sources. Ex-\nisting LLMs mainly leverage a mixture of diverse public\ntextual datasets as the pre-training corpus. Figure 6 shows\nthe distribution of the sources of pre-training data for a\nnumber of representative LLMs.\nThe source of pre-training corpus can be broadly cate-\ngorized into two types: general data and specialized data.\nGeneral data, such as webpages, books, and conversational\ntext, is utilized by most LLMs [55, 56, 90] due to its large,\ndiverse, and accessible nature, which can enhance the lan-\nguage modeling and generalization abilities of LLMs. In\nlight of the impressive generalization capabilities exhibited\nby LLMs, there are also studies that extend their pre-training\ncorpus to more specialized datasets, such as multilingual\ndata, scientific data, and code, endowing LLMs with specific\ntask-solving capabilities [35, 56, 86]. In what follows, we\ndescribe these two types of pre-training data sources and\ntheir effects on LLMs. For a detailed introduction to the\ncommonly used corpus, one can refer to Section 3.2.\nGeneral Text Data. As we can see in Figure 6, the vast\nmajority of LLMs adopt general-purpose pre-training data,\nsuch as webpages, books, and conversational text, which\nprovides rich text sources on a variety of topics. Next, we\nbriefly summarize three important kinds of general data.\n\u2022Webpages. Owing to the proliferation of the Internet,\nvarious types of data have been created, which enables\nLLMs to gain diverse linguistic knowledge and enhance\ntheir generalization capabilities [26, 82]. For convenient\nuse of these data resources, a large amount of data is\ncrawled from the web in previous work, such as Com-\nmonCrawl [163]. However, the crawled web data tends to\ncontain both high-quality text, such as Wikipedia and low-\nquality text, like spam mail, thus it is important to filter and\nprocess webpages for improving the data quality.\n\u2022Conversation text. Conversation data can enhance the\nconversational competence of LLMs [90] and potentially im-prove their performance on a range of question-answering\ntasks [56]. Researchers can utilize subsets of public conver-\nsation corpus ( e.g., PushShift.io Reddit corpus) [158, 202] or\ncollect conversation data from online social media. Since on-\nline conversational data often involves discussions among\nmultiple participants, an effective processing way is to\ntransform a conversation into a tree structure, where the\nutterance is linked to the one it responds to. In this way, the\nmulti-party conversation tree can be divided into multiple\nsub-conversations, which can be collected in the pre-training\ncorpus. Furthermore, a potential risk is that the excessive\nintegration of dialogue data into LLMs may result in a side\neffect [90]: declarative instructions and direct interrogatives\nare erroneously perceived as the beginning of conversations,\nthus leading to a decline in the efficacy of the instructions.\n\u2022Books. Compared to other corpus, books provide an\nimportant source of formal long texts, which are potentially\nbeneficial for LLMs to learn linguistic knowledge, model\nlong-term dependency, and generate narrative and coherent\ntexts. To obtain open-source book data, existing studies\nusually adopt the Books3 and Bookcorpus2 datasets, which\nare available in the Pile dataset [161].\nSpecialized Text Data. Specialized datasets are useful to\nimprove the specific capabilities of LLMs on downstream\ntasks. Next, we introduce three kinds of specialized data.\n\u2022Multilingual text. In addition to the text in the target\nlanguage, integrating a multilingual corpus can enhance\nthe multilingual abilities of language understanding and\ngeneration. For example, BLOOM [78] and PaLM [56] have\ncurated multilingual data covering 46 and 122 languages,\nrespectively, within their pre-training corpora. FLM [102]\nmixes Chinese and English corpora in nearly equal propor-\ntions. These models demonstrate impressive performance in\nmultilingual tasks, such as translation, multilingual summa-\nrization, and multilingual question answering, and achieve\ncomparable or superior performance to the state-of-the-\nart models that are fine-tuned on the corpus in the target\nlanguage(s).18\n\u2022Scientific text. The exploration of science by humans has\nbeen witnessed by the increasing growth of scientific publi-\ncations. In order to enhance the understanding of scientific\nknowledge for LLMs [35, 203], it is useful to incorporate a\nscientific corpus for model pre-training [35, 203]. By pre-\ntraining on a vast amount of scientific text, LLMs can\nachieve impressive performance in scientific and reasoning\ntasks [204]. To construct the scientific corpus, existing efforts\nmainly collect arXiv papers, scientific textbooks, math web-\npages, and other related scientific resources. Due to the com-\nplex nature of data in scientific fields, such as mathematical\nsymbols and protein sequences, specific tokenization and\npreprocessing techniques are usually required to transform\nthese different formats of data into a unified form that can\nbe processed by language models.\n\u2022Code. Program synthesis has been widely studied in\nthe research community [105, 205\u2013208], especially the use of\nPLMs trained on code [165, 209]. However, it remains chal-\nlenging for these PLMs ( e.g., GPT-J [165]) to generate high-\nquality and accurate programs. Recent studies [105, 208]\nhave found that training LLMs on a vast code corpus\ncan lead to a substantial improvement in the quality of\nthe synthesized programs. The generated programs can\nsuccessfully pass expert-designed unit-test cases [105] or\nsolve competitive programming questions [114]. In gen-\neral, two types of code corpora are commonly used for\npre-training LLMs. The first source is from programming\nquestion answering communities like Stack Exchange [210].\nThe second source is from public software repositories\nsuch as GitHub [86, 105, 208], where code data (includ-\ning comments and docstrings) are collected for utilization.\nCompared to natural language text, code is in the format\nof a programming language, corresponding to long-range\ndependencies and accurate execution logic [211]. A recent\nstudy [47] also speculates that training on code might be a\nsource of complex reasoning abilities ( e.g., chain-of-thought\nability [33]). Furthermore, it has been shown that formatting\nreasoning tasks into code can help LLMs generate more\naccurate results [211].\n4.1.2 Data Preprocessing\nAfter collecting a large amount of text data, it is essential\nto preprocess the data for constructing the pre-training\ncorpus, especially removing noisy, redundant, irrelevant,\nand potentially toxic data [56, 64, 212], which may largely\naffect the capacity and performance of LLMs. To facilitate\nthe data processing, a recent study [213] proposes a useful\ndata processing system for LLMs, named Data-Juicer, which\nprovides over 50 processing operators and tools. In this\npart, we review the detailed data preprocessing strategies\nto improve the quality of the collected data [64, 78, 112]. A\ntypical pipeline of preprocessing the pre-training data for\nLLMs has been illustrated in Figure 7.\nQuality Filtering. To remove low-quality data from the\ncollected corpus, existing work generally adopts two ap-\nproaches: (1) classifier-based, and (2) heuristic-based. The\nformer approach trains a selection classifier based on high-\nquality texts and leverages it to identify and filter out low-\nquality data. Typically, these methods [55, 56, 112] train\na binary classifier with well-curated data ( e.g., Wikipediapages) as positive instances and sample candidate data\nas negative instances, and predict the score that measures\nthe quality of each data example. However, several stud-\nies [64, 112] find that a classifier-based approach may result\nin the unintentional removal of high-quality texts in dialec-\ntal, colloquial, and sociolectal languages, which potentially\nleads to bias in the pre-training corpus and diminishes the\ncorpus diversity. As the second approach, several studies,\nsuch as BLOOM [78] and Gopher [64], employ heuristic-\nbased approaches to eliminate low-quality texts through a\nset of well-designed rules, which can be summarized as\nfollows:\n\u2022Language based filtering. If a LLM would be mainly used\nin the tasks of certain languages, the text in other lan-\nguages can be filtered.\n\u2022Metric based filtering. Evaluation metrics about the gener-\nated texts, e.g., perplexity, can be employed to detect and\nremove unnatural sentences.\n\u2022Statistic based filtering. Statistical features of a corpus,\ne.g., the punctuation distribution, symbol-to-word ratio,\nand sentence length, can be utilized to measure the text\nquality and filter the low-quality data.\n\u2022Keyword based filtering. Based on specific keyword set, the\nnoisy or unuseful elements in the text, such as HTML\ntags, hyperlinks, boilerplates, and offensive words, can\nbe identified and removed.\nDe-duplication. Existing work [214] has found that dupli-\ncate data in a corpus would reduce the diversity of language\nmodels, which may cause the training process to become un-\nstable and thus affect the model performance. Therefore, it is\nnecessary to de-duplicate the pre-training corpus. Specially,\nde-duplication can be performed at different granularities,\nincluding sentence-level, document-level, and dataset-level\nde-duplication. First, low-quality sentences that contain re-\npeated words and phrases should be removed, as they may\nintroduce repetitive patterns in language modeling [215].\nAt the document level, existing studies mostly rely on the\noverlap ratio of surface features ( e.g., words and n-grams\noverlap) between documents to detect and remove duplicate\ndocuments containing similar contents [57, 64, 78, 216].\nFurthermore, to avoid the dataset contamination problem,\nit is also crucial to prevent the overlap between the training\nand evaluation sets [56], by removing the possible duplicate\ntexts from the training set. It has been shown that the three\nlevels of de-duplication are useful to improve the training\nof LLMs [56, 217], which should be jointly used in practice.\nPrivacy Reduction. The majority of pre-training text data is\nobtained from web sources, including user-generated con-\ntent involving sensitive or personal information, which may\nincrease the risk of privacy breaches [218]. Thus, it is nec-\nessary to remove the personally identifiable information (PII)\nfrom the pre-training corpus. One direct and effective ap-\nproach is to employ rule-based methods, such as keyword\nspotting, to detect and remove PII such as names, addresses,\nand phone numbers [162]. Furthermore, researchers also\nfind that the vulnerability of LLMs under privacy attacks\ncan be attributed to the presence of duplicate PII data in the\npre-training corpus [219]. Therefore, de-duplication can also19\nLanguage Filtering\nMetric Filtering\nStatistic Filtering\nKeyword FilteringRaw Corpus\n Quality Filtering De-duplication\nSentence-level\nDocument-level\nSet-level\nPrivacy Reduction\n Tokenization\nReady to  \npre-train!\n32, 145, 66, 79, 12, 56, ... Alice is writing a paper about\nLLMs. #$^&  Alice is writing\na paper about LLMs.Alice is writing a paper about\nLLMs. Alice is writing a paper\nabout LLMs.Replace(' Alice') is\nwriting a paper about LLMs.Encode(' [Somebody]  is\nwriting a paper about LLMs. ')Detect Personality\nIdentifiable\nInformation (PII)  \nRemove PIIReuse Existing\nTokenizer\nSentencePiece\nByte-level BPE\nFig. 7: An illustration of a typical data preprocessing pipeline for pre-training large language models.\nreduce privacy risks to some extent.\nTokenization. Tokenization is also a crucial step for data\npreprocessing. It aims to segment raw text into sequences\nof individual tokens, which are subsequently used as the\ninputs of LLMs. In traditional NLP research ( e.g., sequence\nlabeling with conditional random fields [220]), word-based\ntokenization is the predominant approach, which is more\naligned with human\u2019s language cognition. However, word-\nbased tokenization can yield different segmentation results\nfor the same input in some languages ( e.g., Chinese word\nsegmentation), generate a huge word vocabulary containing\nmany low-frequency words, and also suffer from the \u201c out-\nof-vocabulary \u201d issue. Thus, several neural network models\nemploy character as the minimum unit to derive the word\nrepresentation ( e.g., a CNN word encoder in ELMo [21]).\nRecently, subword tokenizers have been widely used in Trans-\nformer based language models, typically including Byte-\nPair Encoding tokenization, WordPiece tokenization and\nUnigram tokenization. HuggingFace has maintained an\nexcellent online NLP course on tokenizer22with running\nexamples, and we refer to the beginners to this course. Next,\nwe briefly describe the three representative tokenization\nmethods.\n\u2022Byte-Pair Encoding (BPE) tokenization . BPE was origi-\nnally proposed as a general data compression algorithm in\n1994 [221], and then adapted to NLP for tokenization [222].\nIt starts with a set of basic symbols ( e.g., the alphabets\nand boundary characters), and iteratively combine frequent\npairs of two consecutive tokens in the corpus as new to-\nkens (called merge ). For each merge, the selection criterion\nis based on the co-occurrence frequency of two contigu-\nous tokens: the top frequent pair would be selected. The\nmerge process continues until it reaches the predefined\nsize. Further, Byte-level BPE has been used to improve the\ntokenization quality for multilingual corpus ( e.g., the text\ncontaining non-ASCII characters) by considering bytes as the\nbasic symbols for merge. Representative language models\nwith this tokenization approach include GPT-2, BART, and\nLLaMA.\n\u2022WordPiece tokenization . WordPiece was a Google inter-\nnal subword tokenization algorithm. It was originally pro-\nposed by Google in developing voice search systems [223].\nThen, it was used in the neural machine translation system\nin 2016 [224], and was adopted as the word tokenizer for\nBERT in 2018 [23]. WordPiece has a very similar idea with\nBPE by iteratively merging consecutive tokens, whereas\n22. https://huggingface.co/learn/nlp-course/chapter6taking a slightly different selection criterion for the merge.\nTo conduct the merge, it first trains a language model and\nemploys it to score all possible pairs. Then, at each merge, it\nselects the pair that leads to the most increase in the likeli-\nhood of training data. Since Google has\u2019t released the official\nimplementation of the WordPiece algorithm, HuggingFace\ngives a more intuitive selection measure in its online NLP\ncourse: a pair is scored by dividing the co-occurrence count\nby the product of the occurrence counts of two tokens in the\npair based on training corpus.\n\u2022Unigram tokenization . Unlike BPE and WordPiece, Un-\nigram tokenization [225] starts with a sufficiently large\nset of possible substrings or subtokens for a corpus, and\niteratively removes the tokens in the current vocabulary\nuntil the expected vocabulary size is reached. As the se-\nlection criterion, it calculates the yielded increase in the\nlikelihood of training corpus by assuming that some to-\nken was removed from current vocabulary. This step is\nconducted based on a trained unigram language model.\nTo estimate the unigram language model, it adopts an\nexpectation\u2013maximization (EM) algorithm: at each iteration,\nwe first find the currently optimal tokenization of words\nbased on the old language model, and then re-estimate the\nprobabilities of unigrams to update the language model.\nDuring this procedure, dynamic programming algorithms\n(i.e.,the Viterbi algorithm) are used to efficiently find the\noptimal decomposition way of a word given the language\nmodel. Representative models that adopt this tokenization\napproach include T5 and mBART.\nAlthough it is expedient to leverage an existing tokenizer\n(e.g., OPT [90] and GPT-3 [55] utilize the tokenizer of GPT-\n2 [26]), using a tokenizer specially designed for the pre-\ntraining corpus can be highly beneficial [78], especially for\nthe corpus that consists of diverse domains, languages, and\nformats. Therefore, recent LLMs often train the customized\ntokenizers specially for the pre-training corpus with the\nSentencePiece library [226], which includes Byte-level BPE\nand Unigram tokenization. A note is that normalization\ntechniques in BPE, such as NFKC [227], may degrade the\ntokenization performance [34, 64, 78]. When extending\nexisting LLMs ( i.e., continual pre-training or instruction\ntuning), we should be also aware of the potential side effect\nwith customized tokenizers. For example, LLaMA trains\nthe BPE tokenizer based on a pre-training corpus mainly\nconsisting of English texts, and the derived vocabulary\nmight be less capable in processing non-English data, e.g.,\ntaking longer inference latency to generate Chinese texts.20\nData CurriculumStage 1\n\u00b7\u00b7\u00b7Stage 2 Stage Data MixtureData\nSource  \nStage 1\n2\n3\n4\nFig. 8: An illustration of data scheduling for pre-training\nLLMs.\nDiscussion on Effect of Data Quality. For pre-training, the\nquality of pre-training data is vital to the model capacities\nof LLMs. Existing work has shown that pre-training on the\nlow-quality corpus, such as noisy, toxic, and duplicate data,\nwould largely hurt the performance of models [64, 214,\n216, 219]. Recent studies, such as T5 [82], GLaM [112], and\nGopher [64], have investigated the influence of data quality\non the LLMs\u2019 capacities. By comparing the performance of\nmodels trained on the filtered and unfiltered corpus, they\nhave reached the similar conclusion that pre-training LLMs\non cleaned data can improve the model performance. More\nspecifically, the duplication of data may result in \u201c double\ndescent \u201d (referring to the phenomenon of performance ini-\ntially deteriorating and subsequently improving) [214, 228],\nor even overwhelm the training process [214]. In addition,\nit has been shown that duplicate data degrades the ability\nof LLMs to copy from the context, which might further\naffect the generalization capacity of LLMs using in-context\nlearning [214]. Therefore, as suggested in [56, 64, 78, 212],\nit is essential to utilize preprocessing methods like quality\nfiltering, toxic filtering and deduplication to carefully clean\nthe pre-training corpus (as illustrated in Section 4.1.2), to\nimprove stability of the training process and avoid affecting\nthe model performance.\n4.1.3 Data Scheduling\nAfter data preprocessing, it is essential to design suit-\nable strategies to schedule these multi-source data for pre-\ntraining a capable LLM. Generally, two key aspects should\nbe paid close attention for data scheduling: the proportion\nof each data source ( data mixture ), and the order in which\neach data source is scheduled for training ( data curriculum ).\nNext, we discuss the two aspects in detail. An illustration of\ndata scheduling has been presented in Figure 8.\nData Mixture. Since each kind of data source is closely\nrelated to the development of certain capacities for LLMs\n(referring to the discussions in Section 4.1), it is important\nto set a suitable distribution to mix these data. The data\nmixture is generally set in a global level ( i.e.,the distribution\nof the entire pre-training data), and can be also locally set\nto varied proportions at different training stages. During\npre-training, data samples from different sources would be\nselected according to the mixture proportions: more data\nwill be sampled from a data source with a larger weight.\nTypically, existing LLMs such as LLaMA [57] may employ\nupsampling or downsampling on the full data of eachsource to create specific data mixtures as pre-training data.\nAs Figure 6 illustrates, existing LLMs use different data mix-\ntures to construct the pre-training data. As a representative\nmodel, the pre-training data of LLaMA [57] mainly consists\nof webpages (over 80%), alongside 6.5% of code-heavy data\nfrom GitHub and StackExchange, 4.5% from books, and\n2.5% of scientific data sourced from arXiv, which has become\nan important reference for training general-purpose LLMs.\nFurthermore, special data mixtures can be used to facilitate\ndifferent purposes. For example, Falcon [141] is trained on\npure webpages, and CodeGen [86] largely increases the\namount of code data. In practice, data mixture is often de-\ntermined empirically, and we summarize several common\nstrategies for finding an effective data mixture as follows:\n\u2022Increasing the diversity of data sources. Recent studies\nhave empirically shown that training on excessive data\nabout a certain domain would degrade the generalization\ncapability of LLMs on other domains [35, 64]. In contrast,\nincreasing the data source heterogeneity ( e.g., including\ndiverse data sources) is critical for improving the down-\nstream performance of LLMs [212, 229, 230]. To further\nexamine the effect of different data sources, some studies\nhave conducted ablation experiments by removing each\ndata source one by one, and pre-train LLMs with specially\ncurated datasets [212]. It has been shown that dropping data\nsources with high heterogeneity ( e.g., webpages) impacts\nLLM\u2019s abilities more severely than dropping sources with\nlow heterogeneity ( e.g., academic corpus).\n\u2022Optimizing data mixtures. In addition to manually set-\nting the data mixtures, several studies have proposed to\noptimize the data mixtures for improving the model pre-\ntraining [59, 231]. Given the target downstream tasks, one\ncan select pre-training data with either higher proximity\nin the feature space [231] or those that provide positive\ninfluences on downstream task performance [232]. Further,\nto reduce the reliance of target tasks, DoReMi [59] first trains\na small reference model using given initial domain weights,\nand then trains another small proxy model, upweighting the\ndomains on which the greatest discrepancies in likelihood\nbetween the two models are observed. Finally, the learned\ndomain weights of the proxy model are applied to train\na much larger LLM. In a more simple way, one can train\nseveral small language models with different data mixtures,\nand select the data mixture that leads to the most desir-\nable performance. However, an assumption made in this\napproach is, when trained in a similar way, small models\nwould resemble with large models in model abilities or\nbehaviors, which may not always hold in practice.\n\u2022Specializing the targeted abilities. The model capacities\nof LLMs heavily rely on data selection and mixture, and\none can boost the proportions of specific data sources to\nenhance certain model abilities [64, 212]. For example, the\nmathematical reasoning and coding abilities can be specially\nenhanced by training with more mathematical texts and\ncode data, respectively. Furthermore, experimental results\non the LAMBADA dataset [233] show that increasing the\nproportion of books data can improve the model capacity in\ncapturing long-term dependencies from text, and increasing\nthe proportion of the C4 dataset [82] leads to performance\nimprovement on the C4 validation dataset [64]. Generally,\nit is important to identify more implicit relations between21\ndata sources and model abilities. To enhance specific skills\nsuch as mathematics and coding in LLMs, or to develop\nspecialized LLMs, a practical way is to employ a multi-stage\ntraining approach, e.g., general and skill-specific data can\nbe scheduled at two consecutive stages. This approach of\ntraining LLMs on varying sources or proportions of data\nacross multiple stages is also known as \u201cdata curriculum\u201d,\nwhich will be introduced below.\nData Curriculum. After preparing the data mixture, it\nis important to schedule the order that specific data is\npresented to LLMs for pre-training. It has been shown that,\nin some cases, to learn a certain skill, learning in a skill-\nset sequence ( e.g., basic skills \u2192target skill) outperforms\ndirect learning from a corpus focused solely on the target\nskill [234, 235]. Following the idea of curriculum learn-\ning [236], data curriculum has been proposed and widely\nused in model pre-training [234, 235, 237, 238]. It aims to\norganize different parts of pre-training data for LLMs in\na specific order, e.g., starting with easy/general examples\nand progressively introducing more challenging/special-\nized ones. More generally, it can broadly refer to the adap-\ntive adjustment of data proportions for different sources\nduring pre-training. Existing work about data curriculum\nmainly focuses on continual pre-training, such as special-\nized coding LLMs ( e.g., CodeLLaMA [235]) or long context\nLLMs ( e.g., LongLLaMA [238]). However, it still lacks of\nmore detailed report about data curriculum for general-\npurpose LLMs ( e.g., LLaMA) in the literature. To determine\ndata curriculum, a practical approach is to monitor the de-\nvelopment of key abilities of LLMs based on specially con-\nstructed evaluation benchmarks, and then adaptively adjust\nthe data mixture during pre-training. Next, we take three\ncommon abilities as examples to introduce how the concept\nof data curriculum23applies in continual pre-training.\n\u2022Coding . To improve the coding ability of LLMs, CodeL-\nLaMA [235] is developed based on LLaMA 2 [99] (2T general\ntokens \u2192500B code-heavy tokens), aiming to improve the\ncode generation ability and retain natural language under-\nstanding skills. CodeLLaMA also provides a version that\nis further specialized to a certain programming language,\nnamely CodeLLaMA-Python (2T general tokens \u2192500B\ncode-heavy tokens \u2192100B Python-heavy tokens).\n\u2022Mathematics. Llemma [239] is proposed to enhance\nthe mathematical capacities of general-purpose LLMs. It\nis developed based on CodeLLaMA. Although CodeL-\nLaMA [235] mainly focuses on the coding ability, exper-\niments have shown that it performs better than its base\nmodel LLaMA 2 on mathematics benchmarks [239]. Based\non CodeLLaMA, Llemma is continually trained on mixtures\nof scientific papers, web data containing mathematical text\nand code (2T general tokens \u2192500B code-heavy tokens\n\u219250\u223c200B math-heavy tokens). Note that the pre-training\ndata of Llemma also contains 5% general domain data as a\nform of regularization.\n\u2022Long context . Long context modeling is an important\nability for LLMs, and many studies have explored extend-\n23. We utilize the symbol \u201c \u2192\u201d to represent the data order in data\ncurriculum. For example, \u201c2T webpage tokens \u2192500B code tokens\u201d\nmeans that the LLM is firstly trained with 2T webpage tokens and\nsubsequently with 500B code data tokens.ing the context windows of LLMs via continually train-\ning [235, 238]. With modifications on position embeddings\n(i.e., position interpolation) of RoPE-based LLMs [57, 99,\n240], CodeLLaMA further extends the context window of\nLLaMA 2 (2.5T tokens with 4K context window \u219220B\ntokens with 16K context window). LongLLaMA [238] also\nachieves longer context window with the help of external\nmemory and a unique training objective (1T tokens with 2K\ncontext window \u219210B tokens with 8K context window).\n4.1.4 Summary of Data Preparation\nIn this part, we summarize the general procedure and key\npoints to prepare pre-training data for LLMs, which are\ndetailed in the following three aspects.\n\u2022Data collection. It is suggested to include diverse data\nsources in the pre-training data. Although Falcon [141]\nshows that webpages alone can be employed to train power-\nful LLMs, a more typical approach is to also incorporate di-\nverse high-quality text like code, books, scientific papers, etc.\nIf a LLM is specialized with a certain skill, the proportion of\ncorresponding data source should be increased accordingly.\nFor example, Gopher [64] and Chinchilla [34] are trained\nwith approximately 40% of data from books. PaLM [44] and\nLaMDA [68] use approximately 50% conversational data.\n\u2022Data cleaning. After data collection, it is crucial to clean\nthe raw corpus to enhance its quality as possible. First,\ndeduplication is commonly used in existing work [99, 141,\n229]. Second, low-quality text, toxic content, and data with\nprivacy concerns should be removed at different granulari-\nties ( e.g., document, passage or sentence). In practice, both\nheuristic and classifier-based methods can be employed\nfor quality and toxicity filtering ( e.g., CCNet [241], fast-\nText [242], and Data-Juicer [243]). Third, with the cleaned\ndata, one can further unify or specify the format for pre-\ntraining data, and perform the tokenization by training\nthe tokenizer on the filtered and deduplicated corpus with\nlibraries like SentencePiece [226].\n\u2022Data scheduling. With the preprocessed data, the next\nstep is to determine the data mixture and the specific order\nof data for pre-training LLMs. To determine both settings, a\npractical way is to first train several small language models\nwith multiple candidate plans and then select a good plan\namong them [59]. Overall, it is more difficult to find a\nsuitable data curriculum. In practice, one can monitor the\nperformance of intermediate model checkpoints on specific\nevaluation benchmarks, and dynamically tune the data mix-\nture and distribution during pre-training. In this process, it\nis also useful to explore the potential relations between data\nsources and model abilities to instruct the design of data\ncurriculum.\n4.2 Architecture\nIn this section, we review the architecture design of LLMs,\ni.e.,mainstream architecture, pre-training objective, and de-\ntailed configuration. Table 5 presents the model cards of\nseveral representative LLMs with public details.\n4.2.1 Typical Architectures\nDue to the excellent parallelizability and capacity, the Trans-\nformer architecture [22] has become the de facto backbone to22\nTABLE 5: Model cards of several selected LLMs with public configuration details. Here, PE denotes position embedding,\n#L denotes the number of layers, #H denotes the number of attention heads, dmodel denotes the size of hidden states, and\nMCL denotes the maximum context length during training.\nModel Category Size Normalization PE Activation Bias #L #H dmodel MCL\nGPT3 [55] Causal decoder 175B Pre LayerNorm Learned GeLU \u2713 96 96 12288 2048\nPanGU- \u03b1[84] Causal decoder 207B Pre LayerNorm Learned GeLU \u2713 64 128 16384 1024\nOPT [90] Causal decoder 175B Pre LayerNorm Learned ReLU \u2713 96 96 12288 2048\nPaLM [56] Causal decoder 540B Pre LayerNorm RoPE SwiGLU \u00d7 118 48 18432 2048\nBLOOM [78] Causal decoder 176B Pre LayerNorm ALiBi GeLU \u2713 70 112 14336 2048\nMT-NLG [113] Causal decoder 530B - - - - 105 128 20480 2048\nGopher [64] Causal decoder 280B Pre RMSNorm Relative - - 80 128 16384 2048\nChinchilla [34] Causal decoder 70B Pre RMSNorm Relative - - 80 64 8192 -\nGalactica [35] Causal decoder 120B Pre LayerNorm Learned GeLU \u00d7 96 80 10240 2048\nLaMDA [68] Causal decoder 137B - Relative GeGLU - 64 128 8192 -\nJurassic-1 [107] Causal decoder 178B Pre LayerNorm Learned GeLU \u2713 76 96 13824 2048\nLLaMA [57] Causal decoder 65B Pre RMSNorm RoPE SwiGLU \u00d7 80 64 8192 2048\nLLaMA 2 [99] Causal decoder 70B Pre RMSNorm RePE SwiGLU \u00d7 80 64 8192 4096\nFalcon [141] Causal decoder 40B Pre LayerNorm RoPE GeLU \u00d7 60 64 8192 2048\nGLM-130B [93] Prefix decoder 130B Post DeepNorm RoPE GeGLU \u2713 70 96 12288 2048\nT5 [82] Encoder-decoder 11B Pre RMSNorm Relative ReLU \u00d7 24 128 1024 512\nDecoder\nEncoder DecoderCausal Decoder Prefix Decoder Encoder -Decoder\nLanguage Models A Survey of Large Language Models A Survey of Large Language Models A Survey of LargeLanguage Models A Survey of Large\nLanguage Models A Survey of Large\nLanguage Models A Survey of LargeEncoder DecoderDecoder\nDecoder Decoder\nFig. 9: A comparison of the attention patterns in three mainstream architectures. Here, the blue, green, yellow and grey\nrounded rectangles indicate the attention between prefix tokens, attention between prefix and target tokens, attention\nbetween target tokens, and masked attention respectively.\ndevelop various LLMs, making it possible to scale language\nmodels to hundreds or thousands of billions of parameters.\nIn general, the mainstream architectures of existing LLMs\ncan be roughly categorized into three major types, namely\nencoder-decoder, causal decoder, and prefix decoder, as\nshown in Figure 9.\nEncoder-decoder Architecture. The vanilla Transformer\nmodel is built on the encoder-decoder architecture [22],\nwhich consists of two stacks of Transformer blocks as\nthe encoder and decoder, respectively. The encoder adopts\nstacked multi-head self-attention layers to encode the input\nsequence for generating its latent representations, while\nthe decoder performs cross-attention on these representa-\ntions and autoregressively generates the target sequence.\nEncoder-decoder PLMs ( e.g., T5 [82] and BART [24]) have\nshown effectiveness on a variety of NLP tasks. So far,\nthere are only a small number of LLMs that are built based\non the encoder-decoder architecture, e.g., Flan-T5 [69]. We\nleave a detailed discussion about the architecture selectionin Section 4.2.6.\nCausal Decoder Architecture. The causal decoder archi-\ntecture incorporates the unidirectional attention mask, to\nguarantee that each input token can only attend to the\npast tokens and itself. The input and output tokens are\nprocessed in the same fashion through the decoder. As\nrepresentative language models of this architecture, the\nGPT-series models [26, 55, 122] are developed based on\nthe causal-decoder architecture. In particular, GPT-3 [55]\nhas successfully demonstrated the effectiveness of this ar-\nchitecture, also showing an amazing in-context learning\ncapability of LLMs. Interestingly, GPT-1 [122] and GPT-\n2 [26] do not exhibit such superior abilities as those in\nGPT-3, and it seems that scaling plays an important role\nin increasing the model capacity of this model architecture.\nSo far, the causal decoders have been widely adopted as\nthe architecture of LLMs by various existing LLMs, such\nas OPT [90], BLOOM [78], and Gopher [64]. Note that both\nthe causal decoder and prefix decoder discussed next belong23\nto decoder-only architectures. When mentioning \u201cdecoder-\nonly architecture\u201d, it mainly refers to the causal decoder\narchitecture in existing literature, unless specified.\nPrefix Decoder Architecture. The prefix decoder architec-\nture ( a.k.a., non-causal decoder [244]) revises the masking\nmechanism of causal decoders, to enable performing bidi-\nrectional attention over the prefix tokens [245] and unidi-\nrectional attention only on generated tokens. In this way,\nlike the encoder-decoder architecture, the prefix decoders\ncan bidirectionally encode the prefix sequence and autore-\ngressively predict the output tokens one by one, where the\nsame parameters are shared during encoding and decoding.\nInstead of pre-training from scratch, a practical suggestion\nis to continually train causal decoders and then convert\nthem into prefix decoders for accelerating convergence [29],\ne.g., U-PaLM [118] is derived from PaLM [56]. Existing rep-\nresentative LLMs based on prefix decoders include GLM-\n130B [93] and U-PaLM [118].\nMixture-of-Experts. For the above three types of archi-\ntectures, we can further extend them via the mixture-of-\nexperts (MoE) scaling, in which a subset of neural network\nweights for each input are sparsely activated, e.g., Switch\nTransformer [25] and GLaM [112]. The major merit is that\nMoE is a flexible way to scale up the model parameter while\nmaintaining a constant computational cost [25]. It has been\nshown that substantial performance improvement can be\nobserved by increasing either the number of experts or the\ntotal parameter size [246]. Despite the merits, training large\nMoE models may suffer from instability issues due to the\ncomplex, hard-switching nature of the routing operation.\nTo enhance the training stability of MoE-based language\nmodels, techniques such as selectively using high-precision\ntensors in the routing module or initializing the model with\na smaller range have been introduced [25]. More recently,\nthere is widespread speculation that GPT-4 has been devel-\noped based on the MoE architecture, but without official\nverification.\nEmergent Architectures. The conventional Transformer ar-\nchitectures typically suffer from quadratic computational\ncomplexity. Because of this, efficiency has become an im-\nportant issue when training and making inference with\nlong inputs. To improve efficiency, some studies aim to\ndevise new architectures for language modeling, including\nparameterized state space models ( e.g., S4 [247], GSS [248],\nand H3 [249]), long convolutions like Hyena [250], and\nTransformer-like architectures that incorporate recursive up-\ndate mechanisms ( e.g., RWKV [251] and RetNet [252]). The\nkey merits of these new architectures are twofold. First,\nthese models can generate outputs recursively like RNNs,\nmeaning that they only need to refer to the single previous\nstate during decoding. It makes the decoding process more\nefficient as it eliminates the need to revisit all previous\nstates as in conventional Transformers. Second, these mod-\nels have the capacity to encode an entire sentence in parallel\nlike Transformers. This contrasts with conventional RNNs\nwhich has to encode sentences on a token-by-token basis.\nThus, they can benefit from the parallelism of GPUs with\ntechniques such as Parallel Scan [253, 254], FFT [250, 251],\nand Chunkwise Recurrent [252]. These techniques enablemodels with these new architectures to be trained in a highly\nparallel and efficient manner.\n4.2.2 Detailed Configuration\nSince the launch of Transformer [22], various improvements\nhave been proposed to enhance its training stability, per-\nformance, and computational efficiency. In this part, we\nwill discuss the corresponding configurations for four major\nparts of the Transformer, including normalization, position\nembeddings, activation functions, and attention and bias.\nTo make this survey more self-contained, we present the\ndetailed formulations for these configurations in Table 6.\nNormalization Methods. Training instability is a challeng-\ning issue for pre-training LLMs. To alleviate this issue,\nnormalization is a widely adopted strategy to stabilize the\ntraining of neural networks. In the vanilla Transformer [22],\nLayerNorm [256] is employed. Recently, several advanced\nnormalization techniques have been proposed as alterna-\ntives to LayerNorm, e.g., RMSNorm, and DeepNorm.\n\u2022LayerNorm. In the early research, BatchNorm [265] is\na commonly used normalization method. However, it is\ndifficult to deal with sequence data of variable lengths and\nsmall-batch data. Thus, LayerNorm [256] is introduced to\nconduct layerwise normalization. Specifically, the mean and\nvariance over all activations per layer are calculated to re-\ncenter and re-scale the activations.\n\u2022RMSNorm. To improve the training speed of Lay-\nerNorm (LN), RMSNorm [257] is proposed by re-scaling\nthe activations with only the root mean square (RMS) of\nthe summed activations, instead of the mean and variance.\nRelated research has demonstrated its superiority in training\nspeed and performance on Transformer [266]. Representa-\ntive models that adopt RMSNorm include Gopher [64] and\nChinchilla [34].\n\u2022DeepNorm. DeepNorm is proposed by Microsoft [258]\nto stabilize the training of deep Transformers. With Deep-\nNorm as residual connections, Transformers can be scaled\nup to 1,000 layers [258], which has shown the advantages\nof stability and good performance. It has been adopted by\nGLM-130B [93].\nNormalization Position. In addition to the normalization\nmethod, normalization position also plays a crucial role in\nthe LLMs. There are generally three choices for the normal-\nization position, i.e.,post-LN, pre-LN, and sandwich-LN.\n\u2022Post-LN. Post-LN is used in the vanilla Trans-\nformer [22], which is placed between residual blocks. How-\never, existing work has found that the training of Trans-\nformers with post-LN tends to be instable due to the large\ngradients near the output layer [267]. Thus, post-LN is rarely\nemployed in existing LLMs except combined with other\nstrategies ( e.g., combining post-LN with pre-LN in GLM-\n130B [93]).\n\u2022Pre-LN. Different from post-LN, pre-LN [268] is applied\nbefore each sub-layer, and an additional LN is placed before\nthe final prediction. Compared with post-LN, the Trans-\nformers with pre-LN are more stable in training. However,\nit performs worse than the variants with post-LN [269].\nDespite the decreasing performance, most LLMs still adopt\npre-LN due to the training stability. However, one excep-24\nTABLE 6: Detailed formulations for the network configurations. Here, Sublayer denotes a FFN or a self-attention module\nin a Transformer layer, ddenotes the size of hidden states, pidenotes position embedding at position i,Aijdenotes the\nattention score between a query and a key, ri\u2212jdenotes a learnable scalar based on the offset between the query and the\nkey, and R\u0398,tdenotes a rotary matrix with rotation degree t\u00b7\u0398.\nConfiguration Method Equation\nNormalization positionPost Norm [22] Norm( x+Sublayer( x))\nPre Norm [26] x+ Sublayer(Norm( x))\nSandwich Norm [255] x+ Norm(Sublayer(Norm( x)))\nNormalization methodLayerNorm [256]x\u2212\u00b5\n\u03c3\u00b7\u03b3+\u03b2, \u00b5 =1\ndPd\ni=1xi, \u03c3 =q\n1\ndPd\ni=1(xi\u2212\u00b5))2\nRMSNorm [257]x\nRMS( x)\u00b7\u03b3,RMS( x) =q\n1\ndPd\ni=1x2\ni\nDeepNorm [258] LayerNorm( \u03b1\u00b7x+ Sublayer( x))\nActivation functionReLU [259] ReLU( x) = max( x,0)\nGeLU [260] GeLU( x) = 0 .5x\u2297[1 + erf( x/\u221a\n2)],erf(x) =2\u221a\u03c0Rx\n0e\u2212t2dt\nSwish [261] Swish( x) =x\u2297sigmoid( x)\nSwiGLU [262] SwiGLU( x1,x2) = Swish( x1)\u2297x2\nGeGLU [262] GeGLU( x1,x2) = GeLU( x1)\u2297x2\nPosition embeddingAbsolute [22] xi=xi+pi\nRelative [82] Aij=WqxixT\njWT\nk+ri\u2212j\nRoPE [263] Aij=WqxiR\u0398,i\u2212jxT\njWT\nk= (WqxiR\u0398,i)(WkxjR\u0398,j)T\nALiBi [264] Aij=WqxixT\njWT\nk\u2212m(i\u2212j)\ntion is that pre-LN has been found unstable in GLM when\ntraining models more than 100B parameters [93].\n\u2022Sandwich-LN. Based on pre-LN, Sandwich-LN [255]\nadds extra LN before the residual connections to avoid\nthe value explosion issues in Transformer layer outputs.\nHowever, it has been found that Sandwich-LN sometimes\nfails to stabilize the training of LLMs and may lead to the\ncollapse of training [93].\nActivation Functions. To obtain good performance, activa-\ntion functions also need to be properly set in feed-forward\nnetworks. In existing LLMs, GeLU activations [270] are\nwidely used. Specially, in the latest LLMs ( e.g., PaLM and\nLaMDA), variants of GLU activation [262, 271] have also\nbeen utilized, especially the SwiGLU and GeGLU variants,\nwhich often achieve better performance in practice [266].\nHowever, compared with GeLU, they require extra parame-\nters (about 50%) in the feed-forward networks [272].\nPosition Embeddings. Since the self-attention modules in\nTransformer are permutation equivariant, position embed-\ndings (PE) are employed to inject absolute or relative posi-\ntion information for modeling sequences.\n\u2022Absolute position embedding. In the vanilla Trans-\nformer [22], absolute position embeddings are employed.\nAt the bottoms of the encoder and the decoder, the absolute\npositional embeddings are added to the input embeddings.\nThere are two variants of absolute position embeddings\nproposed in the vanilla Transformer [22], i.e.,sinusoidal and\nlearned position embeddings, where the latter is commonly\nused in existing pre-trained language models.\n\u2022Relative position embedding. Unlike absolute position\nembeddings, relative positional embeddings are generated\naccording to the offsets between keys and queries [273].\nA popular variant of relative PE was introduced in\nTransformer-XL [274, 275]. The calculation of attention\nscores between keys and queries has been modified to\nintroduce learnable embeddings corresponding to relative\npositions. T5 [82] further simplified relative positional em-beddings, which was subsequently adopted by Gopher [64].\nSpecifically, it adds learnable scalars to the attention scores,\nwhere the scalars are calculated based on the distances\nbetween the positions of the query and the key. Compared\nwith the absolute PE, Transformers with relative position\nembedding can generalize to sequences longer than those\nsequences for training, i.e.,extrapolation [264].\n\u2022Rotary Position Embedding. Rotary position embedding\n(RoPE) [263] sets specific rotatory matrices based on the\nabsolute position of each key or query. The scores between\nkeys and queries can be computed with relative position\ninformation (Table 6). RoPE combines each consecutive pair\nof elements in query and key vectors as a dimension , so there\nared/2dimensions for an original d-length embedding.\nFor each dimension i\u2208 {1, . . . , d/ 2}, the pair of involved\nelements will rotate based on the rotation angle t\u00b7\u03b8i, where\ntdenotes the position index and \u03b8iis the basis in the\ndimension. Following sinusoidal position embeddings [22],\nRoPE defines the basis \u03b8ias an exponentiation of the baseb\n(set to 10000 by default):\n\u0398 ={\u03b8i=b\u22122(i\u22121)/d|i\u2208 {1,2, . . . , d/ 2}}. (4)\nFurthermore, a recent study [276] defines the distance re-\nquired to rotate one cycle ( 2\u03c0) for each dimension as wave-\nlength:\n\u03bbi= 2\u03c0b2(i\u22121)/d= 2\u03c0/\u03b8i. (5)\nDue to the excellent performance and the long-term decay\nproperty, RoPE is widely adopted in the latest LLMs, e.g.,\nPaLM [56] and LLaMA [57]. Based on RoPE, xPos [277] fur-\nther improves the translation invariance and length extrap-\nolation of Transformer. At each dimension of the rotation\nangle vector, xPos adds a special exponential decay that is\nsmaller when the basis is larger. It can alleviate the unstable\nphenomenon during training as the distance increases.\n\u2022ALiBi. ALiBi [264] is proposed to improve the extrap-\nolation of Transformer. Similar to relative position embed-\nding, it biases attention scores with a penalty based on the25\ndistances between keys and queries. Different from the rela-\ntive positional embedding methods like T5 [82], the penalty\nscores in ALiBi are pre-defined without any trainable pa-\nrameters. Empirical results in [264] have shown that ALiBi\nhas a better extrapolation performance on sequences that are\nlonger than those for training than several popular position\nembedding methods such as sinusoidal PE [22], RoPE [263],\nand T5 bias [82]. In addition, it has been shown that ALiBi\ncan also improve training stability in BLOOM [78].\nAttention. Attention mechanism is a critical component of\nTransformer. It allows the tokens across the sequence to\ninteract with each other and compute the representations\nof the input and output sequence.\n\u2022Full attention . In the vanilla Transformer [22], the atten-\ntion mechanism is conducted in a pairwise way, considering\nthe relations between all token pairs in a sequence. It adopts\nscaled dot-product attention, in which the hidden states\nare mapped into queries, keys, and values. Additionally,\nTransformer uses multi-head attention instead of single\nattention, projecting the queries, keys, and values with\ndifferent projections in different heads. The concatenation\nof the output of each head is taken as the final output.\n\u2022Sparse attention . A crucial challenge of full attention\nis the quadratic computational complexity, which becomes\na burden when dealing with long sequences. Therefore,\nvarious efficient Transformer variants are proposed to re-\nduce the computational complexity of the attention mecha-\nnism [278, 279]. For instance, locally banded sparse attention\n(i.e.,Factorized Attention [280] has been adopted in GPT-\n3 [55]. Instead of the whole sequence, each query can only\nattend to a subset of tokens based on the positions.\n\u2022Multi-query/grouped-query attention . Multi-query atten-\ntion refers to the attention variant where different heads\nshare the same linear transformation matrices on the keys\nand values [281]. It achieves higher inference speed with\nonly a minor sacrifice in model quality. Representative\nmodels with multi-query attention include PaLM [56] and\nStarCoder [98]. To make a trade-off between multi-query\nattention and multi-head attention, grouped-query attention\n(GQA) [282] has been explored. In GQA, heads are assigned\ninto different groups, and those heads that belong to the\nsame group will share the same transformation matrices.\nSpecially, GQA has been adopted and empirically tested in\nthe recently released LLaMA 2 model [99].\n\u2022FlashAttention . Different from most existing approx-\nimate attention methods that trade-off model quality to\nimprove the computing efficiency, FlashAttention [283] pro-\nposes to optimize the speed and memory consumption of\nattention modules on GPUs from an IO-aware perspective.\nThere exist different levels of memory on modern GPUs,\ne.g., SRAM with a fast IO and HBM with a relatively\nslow IO. FlashAttention organizes the input into blocks and\nintroduces necessary recomputation, both to make better\nuse of the fast memory SRAM. Implemented as a fused\nkernel in CUDA, FlashAttention has been integrated into\nPyTorch [197], DeepSpeed [74], and Megatron-LM [75]. The\nupdated version FlashAttention-2 [284] further optimizes\nthe work partitioning of GPU thread blocks and warps, lead-\ning to around 2 \u00d7speedup when compared to the original\nFlashAttention.\u2022PagedAttention . It has been observed when LLM are\ndeployed on servers, GPU memory is largely occupied by\ncached attention key and value tensors (called KV cache ).\nThe major reason is that the input lengths are often varied,\nleading to fragmentation and over-reservation issues. In-\nspired by the classic paging technique in operating systems,\nPagedAttention has been proposed to improve the memory\nefficiency and throughput of deployed LLMs [285]. In detail,\nPagedAttention partitions each sequence into subsequences,\nand the corresponding KV caches of these subsequences are\nallocated into non-contiguous physical blocks. The paging\ntechnique increases the GPU utilization and enables efficient\nmemory sharing in parallel sampling.\nTo put all these discussions together, we summarize the\nsuggestions from existing literature for detailed configura-\ntion. For stronger generalization and training stability, it is\nsuggested to choose the pre RMSNorm for layer normaliza-\ntion, and SwiGLU or GeGLU as the activation function. In\naddition, LN may not be used immediately after embedding\nlayers, which is likely to incur performance degradation. As\nfor position embeddings, RoPE or ALiBi is a better choice\nsince it performs better on long sequences.\n4.2.3 Pre-training Tasks\nPre-training plays a key role that encodes general knowl-\nedge from large-scale corpus into the massive model param-\neters. For training LLMs, there are two commonly used pre-\ntraining tasks, namely language modeling and denoising\nautoencoding.\nLanguage Modeling. The language modeling task (LM) is\nthe most commonly used objective to pre-train decoder-only\nLLMs, e.g., GPT3 [55] and PaLM [56]. Given a sequence of\ntokens x={x1, . . . , x n}, the LM task aims to autoregres-\nsively predict the target tokens xibased on the preceding\ntokens x<iin a sequence. A general training objective is to\nmaximize the following likelihood:\nLLM(x) =nX\ni=1logP(xi|x<i). (6)\nSince most language tasks can be cast as the prediction\nproblem based on the input, these decoder-only LLMs might\nbe potentially advantageous to implicitly learn how to ac-\ncomplish these tasks in a unified LM way. Some studies\nhave also revealed that decoder-only LLMs can be naturally\ntransferred to certain tasks by autoregressively predicting\nthe next tokens [26, 55], without fine-tuning. An important\nvariant of LM is the prefix language modeling task, which is\ndesigned for pre-training models with the prefix decoder\narchitecture. The tokens within a randomly selected prefix\nwould not be used in computing the loss of prefix language\nmodeling. With the same amount of tokens seen during pre-\ntraining, prefix language modeling performs slightly worse\nthan language modeling, since fewer tokens in the sequence\nare involved for model pre-training [29].\nDenoising Autoencoding. In addition to conventional\nLM, the denoising autoencoding task (DAE) has also been\nwidely used to pre-train language models [24, 82]. The\ninputs x\\\u02dcxfor DAE task are corrupted text with randomly26\nI am sleepy. I start a pot of\ncoffee 0.661 strong 0.008 soup 0.005\nwater 0.119 black 0.008 . . . . . .\ntea 0.057 hot 0.007 happy 4.3e-6\nrice 0.017 oat 0.006 Boh 4.3e-6\nchai 0.012 beans 0.006 . . . . . .\nFig. 10: The probability distribution over the vocabulary in\ndescending order for the next token of the context \u201c I am\nsleepy. I start a pot of \u201d. For ease of discussion, this example is\ngiven in word units instead of subword units.\nreplaced spans. Then, the language models are trained to re-\ncover the replaced tokens \u02dcx. Formally, the training objective\nof DAE is denoted as follows:\nLDAE(x) = log P(\u02dcx|x\\\u02dcx). (7)\nHowever, the DAE task seems to be more complicated\nin implementation than LM task. As a result, it has not\nbeen widely used to pre-train large language models. Exist-\ning LLMs that take DAE as pre-training objectives include\nT5 [82] and GLM-130B [93]. These models are mainly trained\nto recover the replaced spans in an autoregressive way.\nMixture-of-Denoisers. Mixture-of-Denoisers (MoD) [89],\nalso known as UL2 loss, was introduced as a unified ob-\njective for pre-training language models. MoD regards both\nLM and DAE objectives as different types of denoising tasks,\nnamely S-denoiser (LM), R-denoiser (DAE, short span and\nlow corruption), and X-denoiser (DAE, long span or high\ncorruption). Among the three denoising tasks, S-denoiser\nis similar to the conventional LM objective (Equation (6)),\nwhile R-denoiser and X-denoiser are similar to DAE ob-\njectives (Equation (7)) but differ from each other in the\nlengths of spans and ratio of corrupted text. For input sen-\ntences started with different special tokens ( i.e.,{[R],[S],\n[X]}), the model will be optimized using the corresponding\ndenoisers. MoD has been applied in the latest PaLM 2\nmodel [120].\n4.2.4 Long Context Modeling\nIn real applications, there is an increasing demand for long\ncontext modeling capacities of LLMs, such as PDF pro-\ncessing and story writing [286]. Many closed-source LLMs\nprovide professional support for long text processing. For\ninstance, OpenAI releases GPT-4 Turbo with a 128K context\nwindow, and Anthropic releases Claude 2.1 with a 200K\ncontext window. To enhance the long context modeling\nabilities, there are generally two feasible directions, namely\nscaling position embeddings and adapting context window.\nNext, we introduce the two parts in detail.\nScaling Position Embeddings. Transformer-based LLMs\ncan learn effective position embeddings within the maxi-\nmum training length. Thus, when adapting LLMs to lan-\nguage tasks beyond the maximum training length, it is\nnecessary to scale to larger position indices. Some specific\nposition embeddings have been shown to possess a certain\ndegree of ability to generalize to text beyond the training\nlength, which is formally termed extrapolation capability ,including T5 bias [82], ALiBi [264], xPos [277] and even\nNoPE [287]. However, as one of the mainstream position\nembedding methods, RoPE exhibits limited extrapolation\nability in empirical studies [240]. In the following, we dis-\ncuss several methods that can scale RoPE to longer texts.\n\u2022Direct model fine-tuning. To adapt LLMs to a long con-\ntext window, a straightforward approach is to directly fine-\ntune the models on long texts with the desired length. The\ncontext extension can be scheduled with increased lengths\nin a multi-stage approach ( e.g.,2K\u21928K\u219232K). To conduct\neffective extension, it needs specially prepared long texts\nfor training. Specially, some recent study has shown that\nthe quality is more important than the lengths of training\ntext in long context models [288]. However, a recent study\nhas highlighted that the fine-tuning approach tends to be\ninherently slow when adapting LLMs for long texts [240].\n\u2022Position interpolation. This method downscales the po-\nsition indices within the original context window, to avoid\nout-of-distribution rotation angles during pre-training [240,\n289]. To be more specific, this approach multiplies all posi-\ntion indices by a coefficient L/L\u2032(L < L\u2032), where Land\nL\u2032represent the original and target context window length,\nrespectively. Experimental results [240] have shown that\nthis method can extend the context window effectively and\nefficiently, compared to the above approach of direct model\nfine-tuning. However, it is worth noting that this technique\nmay have an adverse impact on the model\u2019s performance\nwhen handling shorter texts[240, 290].\n\u2022Position truncation. To mitigate the challenges posed\nby out-of-distribution rotation angles, another practical ap-\nproach is to truncate longer relative positions to satisfy the\nrequirement of the maximum training length. Specifically,\nReRoPE and LeakyReRoPE [291] introduce a pre-defined\nwindow length, which is smaller than the maximum train-\ning length. Position indices within this pre-defined window\nare retained, while those indices beyond the window are\neither truncated to the pre-defined window length or in-\nterpolated to align with the maximum training length. This\nstrategy can reserve local position relationships and enhance\nthe extrapolation capacity. However, this approach needs\nto compute the attention matrices twice, accommodating\nadditional computational budget.\n\u2022Base modification. LLMs are usually trained with a pre-\nset maximum training length, e.g., 4096 in Llama 2 [99].\nHowever, wavelengths in certain dimensions of RoPE may\nexceed the training length for longer text [276], so that\nlanguage models have not undergone sufficient training\n(i.e.,a complete rotation cycle) on these dimensions. Thus,\nwhen we adapt LLMs to longer texts, the rotation angles\nfor certain dimensions would be never seen in the training\nphase [292]. Given a fixed rotation angle t\u00b7\u03b8i, a smaller basis\n\u03b8iallows for a greater distance t,i.e.,enabling the modeling\nof longer texts [235, 276, 288]. According to the formula\n\u03b8i=b\u22122(i\u22121)/din Equation 4, decreasing the basis can be\nachieved by increasing the value of the base. In addition,\ndecreasing the base can also help re-scale the wavelengths\nof all dimensions below the training length, while it often\nneeds continual pre-training to adapt the LLMs to long\ncontext windows [292]. A recent study [292] has empirically\ncompared these two base modification methods, and shown\nthat decreasing the base demonstrates a better extrapolation27\ncapacity beyond the training length, while increasing the\nbase performs better within the training length.\n\u2022Basis truncation. Similar to the base modification, the\ntruncation of the basis also concentrates on dealing with\nthe singular dimensions with wavelengths exceeding the\ntraining length [293]. According to the definition \u03bbi= 2\u03c0/\u03b8i\nin Equation 5, the dimension with a large wavelength \u03bbi\nhas a small basis \u03b8iaccordingly. Based on this observation,\nthis approach first defines a basis range [a, c]. Given the\nbasis range, the value of basis is modified according to the\nfollowing ways: (1) when \u03b8i\u2265c, the value is retained,\n(2) when \u03b8i\u2264a, the value is set to zero, and (3) when\na < \u03b8 i< c , the value is truncated to a fixed small\nvalue. Via basis truncation, the out-of-distribution rotation\nangles can be avoided at larger position indices. However,\nthis approach does not perform very well at long context\ntasks [293].\nAdapting Context Window. Since Transformer-based LLMs\nhave limited context windows, they can not directly inte-\ngrate or utilize the entire information of the long sequences\nexceeding the context window. To alleviate the limitation,\nseveral methods adapting LLMs to long context have been\nproposed, as discussed below.\n\u2022Parallel context window. Inspired by fusion-in-\ndecoder [294], parallel context window methods [295, 296]\nadopt a divide-and-conquer strategy to process input text.\nSpecially, it divides the input text into multiple segments,\neach independently encoded with shared position embed-\ndings. In the generation stage, the attention masks are mod-\nified to make that subsequent tokens can access to previous\ntokens in each segment. Nevertheless, this method cannot\ndistinguish the order of different segments, constraining the\nmodel capacity on certain tasks.\n\u2022\u039b-shaped context window. Some prior work has revealed\nthat LLMs tend to allocate greater attention weights to\nthe starting and nearest tokens among all previous to-\nkens [297, 298], so called the \u201c lost in the middle \u201d phe-\nnomenon [299]. Based on this observation, LM-Infinite [300]\nand StreamingLLM [298] propose to employ a \u201c \u039b-shaped\u201d\nattention mask, which selectively preserves the initial tokens\nand the nearest tokens that each query can attend to and\nthen discards any tokens beyond this scope. Experiments\ndemonstrate that this method can facilitate extra-long text\ngeneration with a fixed memory [298]. However, it may\nstruggle to model the long-range dependency in prompts,\nsince it cannot effectively utilize the information from the\ndiscarded tokens [298].\n\u2022External memory. It has been shown that a relatively\nsmall subset of tokens can effectively capture the majority\nof attention patterns in a Transformer [301], i.e., the top-\nkattention keys can well approximate the original full\nattention. Therefore, a number of studies propose to store\nthe past keys in external memory and utilize a k-NN\nsearch method to retrieve the kmost relevant tokens for\ngeneration [238, 301, 302]. For a decoder model, it typically\nemploys one certain layer to access these top- kexternal\ntokens, while still adopts the normal context window in the\nrest layers [238, 302].\nIn addition to the studies based on vanilla Transformer,\nthere are a surge of Transformer variants with efficient at-tentions and other efficient architectures, aiming to alleviate\nhigh computational cost for modeling long texts. These\nstudies have been extensively discussed in Section 4.2.1\nand Section 4.2.2. Furthermore, context compression and\nprompting techniques ( e.g., iterative reasoning [303]) have\nalso been proven to be a viable strategy for handling long\ntext tasks [303\u2013306], without the need of model adaption.\n4.2.5 Decoding Strategy\nAfter the LLMs have been pre-trained, it is essential to em-\nploy a specific decoding strategy to generate the appropriate\noutput from the LLMs.\nBackground. We start the discussion with the prevalent\ndecoder-only architecture, and introduce the auto-regressive\ndecoding mechanism. Since such LLMs are pre-trained\nbased on the language modeling task (Equation 6), a basic\ndecoding method is greedy search that predicts the most\nlikely token at each step based on the previously generated\ntokens, formally modeled as:\nxi= arg max\nxP(x|x<i), (8)\nwhere xiis the token with the highest probability at i-\nth step of generation conditioned on the context x<i. For\ninstance in Figure 10, when predicting the next token of\nthe sentence \u201cI am sleepy. I start a pot of\u201d , greedy search\nselects the token \u201ccoffee\u201d which has the highest probability\nat the current step. Greedy search can achieve satisfactory\nresults in text generation tasks ( e.g., machine translation\nand text summarization), in which the output is highly\ndependent on the input [307]. However, in terms of open-\nended generation tasks ( e.g., story generation and dialog),\ngreedy search sometimes tends to generate awkward and\nrepetitive sentences [308].\nAs another alternative decoding strategy, sampling-\nbased methods are proposed to randomly select the next\ntoken based on the probability distribution to enhance the\nrandomness and diversity during generation:\nxi\u223cP(x|x<i). (9)\nFor the example in Figure 10, sampling-based methods will\nsample the word \u201ccoffee\u201d with higher probability while\nalso retaining the possibilities of selecting the rest words,\n\u201cwater\u201d, \u201ctea\u201d, \u201crice\u201d, etc.\nNot limited to the decoder-only architecture, these two\ndecoding methods can be generally applied to encoder-\ndecoder models and prefix decoder models in a similar way.\nImprovement for Greedy Search. Selecting the token with\nthe highest probability at each step may result in overlook-\ning a sentence with a higher overall probability but a lower\nlocal estimation. Next, we introduce several improvement\nstrategies to alleviate this issue.\n\u2022Beam search. Beam search [309] retains the sentences\nwith the n(beam size) highest probabilities at each step\nduring the decoding process, and finally selects the gener-\nated response with the top probability. Typically, the beam\nsize is configured within the range of 3 to 6. However,\nopting for a larger beam size might result in a decline in\nperformance [310].28\n\u2022Length penalty. Since beam search favours shorter sen-\ntences, imposing length penalty ( a.k.a., length normaliza-\ntion) is a commonly used technique [311] to overcome this\nissue, which normalizes the sentence probability according\nto the sentence length (divided by an exponential power \u03b1\nof the length).\nBesides, some researchers [312] propose to penalize the\ngeneration of previously generated tokens or n-grams to\nalleviate the issue of repetitive generation. In addition,\ndiverse beam search [313] can be leveraged to produce a\nset of diverse outputs based on the same input.\nImprovement for Random Sampling. Sampling-based\nmethods sample the token over the whole vocabulary, which\nmay select wrong or irrelevant tokens ( e.g., \u201chappy\u201d and\n\u201cBoh\u201d in Figure 10) based on the context. To improve the\ngeneration quality, several strategies have been proposed\nfor mitigating or preventing the selection of words with\nexceedingly low probabilities.\n\u2022Temperature sampling. To modulate the randomness of\nsampling, a practical method is to adjust the temperature\ncoefficient of the softmax function for computing the proba-\nbility of the j-th token over the vocabulary:\nP(xj|x<i) =exp (lj/t)P\nj\u2032exp (lj\u2032/t), (10)\nwhere lj\u2032is the logits of each word and tis the temperature\ncoefficient. Reducing the temperature tincreases the chance\nof selecting words with high probabilities while decreases\nthe chances of selecting words with low probabilities. When\ntis set to 1, it becomes the default random sampling; when\ntis approaching 0, it is equivalent to greedy search. In\naddition, when tgoes to infinity, it degenerates to uniform\nsampling.\n\u2022Top-ksampling. Different from temperature sampling,\ntop-ksampling directly truncates the tokens with lower\nprobability and only samples from the tokens with the top\nkhighest probabilities [314]. For example in Figure 10, top-\n5sampling will sample from the words \u201ccoffee\u201d, \u201cwater\u201d,\n\u201ctea\u201d, \u201crice\u201d, and \u201cchai\u201d from their re-scaled probabilities.\n\u2022Top-psampling. Since top- ksampling does not consider\nthe overall possibility distribution, a constant value of kmay\nbe not be suitable for different contexts. Therefore, top- p\nsampling ( a.k.a., nucleus sampling) is proposed by sampling\nfrom the smallest set having a cumulative probability above\n(or equal to) p[308]. In practice, the smallest set can be con-\nstructed by gradually adding tokens from the vocabulary\nsorted in descending order of generative probability, until\ntheir cumulative value exceeds p.\nRecently, researchers have also explored other sampling\nstrategies for LLMs. For instance, \u03b7-sampling [315] further\nimproves top- psampling by introducing a dynamic thresh-\nold based on the probability distribution. Furthermore, con-\ntrastive search [316] and typical sampling [317] can be utilized\nto improve the generation coherence during decoding. Since\nit has been found that large models tend to assign higher\nprobability to important tokens compared to small models,\ncontrastive decoding [318] utilizes a larger LM ( e.g., OPT-\n13B) and a smaller LM ( e.g., OPT-125M) to measure their\nlog-likelihood differences. Subsequently, tokens are sampled\nbased on the delta value of the probability distribution,thereby amplifying the impact of important tokens. Based\non this contrastive idea, DoLa [319] further extends this\napproach to contrasting the logits across different layers of\na single LLM, as higher layers tend to assign more weight\nto important tokens.\nMemory Wall\nWhen generating a new token, the most time-\nconsuming steps revolve around data transfer and\nweight computation. A main issue is the significant\namount of time overwhelmed by data transfer, of-\nten referred to as the memory wall issue.\nTo address this issue, researchers formally quantify\ndata transfer from GPU memory to GPU caches\nusing the number of bytes in I/O, and they assess\nweight computation by measuring the number of\nFLOPs [320]. Specifically, let b,s,n,d, and hdenote\nthe batch size, sequence length, number of attention\nheads, hidden size of each head, and overall hidden\nsize ( h=n\u00b7d), respectively. During the layer-\nwise multi-head self-attention calculation in causal\ndecoder, the I/O bytes and FLOPs at each decoding\nstep can be expressed as 8bsn+ 4bsnd + 4bnd and\n8bsnd , respectively [320].\nArithmetic intensity is further defined as the ratio of\nFLOPs to I/O bytes:\nintensity =FLOPs\nI/O bytes=2\n1 +2\nd+1\ns(11)\nLet\u2019s consider LLaMA 13B ( d= 128 ) with a se-\nquence length of 1024 ( s= 1024 ) as an example.\nThe calculated arithmetic intensity is 1.97. How-\never, the A100 80G GPU can perform 312 TFLOPs\nand transfer 2TB of data in one second, i.e.,its ideal\narithmetic intensity is 156. This indicates that the\nbottleneck in attention calculation lies in the process\nof data transfer ( i.e.,excessive I/O loading).\nDecoding Efficiency Issues. In this part, we briefly ana-\nlyze the decoding efficiency issues of LLMs. Overall, the\ndecoding process of LLMs can be divided into two stages\nfor overhead analysis: (1) the prefill stage, which computes\nthe hidden states of the input sequence, and (2) the incre-\nmental decoding stage, which generates a token and updates\nhidden states in an auto-regressive manner [321]. As shown\nin the above memory wall box, the arithmetic intensity of\nthe incremental decoding stage is only 1.97, which is far\nfrom the expected value of 156 (calculated according to\nthe standard configuration of A100 80GB GPU). In contrast,\nthe arithmetic intensity of the prefill stage achieves 113.78\nfor LLaMA-13B. Consequently, existing work mainly inves-\ntigates how to enhance the efficiency of the incremental\ndecoding algorithm, which can be categorized into two\nmain approaches:\n\u2022Reducing data transfer mainly focuses on optimizing\nGPU memory access, thereby increasing the arithmetic in-\ntensity. As introduced in Section 4.2.2, KV cache can avoid\nredundant computation of previous tokens and PagedAt-29\ntention allocates KV caches into continuous blocks to reduce\nmemory fragmentation. Furthermore, Flash-Decoding [322]\nspeeds up attention computation by loading the keys and\nvalues in parallel, especially effective for long text gen-\neration. As another alternative approach, multi-query and\ngrouped-query attention can reduce the GPU memory band-\nwidth overhead by sharing KV parameters (loading fewer\nweights).\n\u2022Decoding strategy optimization aims to improve the se-\nquential nature of the auto-regressive generation manner in\ndifferent ways. As a representative study, speculative decod-\ning[323, 324] first leverages a compact but efficient model\n(e.g., an-gram model or a small PLM) to generate short\nsegments and then utilizes the LLM to verify and correct\nthese drafts. It can lead to a notable 2 \u00d7to 3\u00d7speedup\nwithout compromising the generation quality. Researchers\nfurther suggest several variants to improve the efficiency of\nthis approach, such as a learning-based method to combine\nseveral small models [325] and a stage-wise acceleration\nwhich employs a more smaller LM to accelerate the small\nLM first [326]. In addition, token-level early-exit techniques\nhave been proposed enabling the generation of a token at\nlower Transformer layers, rather than passing through all\nthe layers [327]. It can attain greater speedup, but at the cost\nof sacrificing generation quality.\nPractical Settings. In practice, existing libraries ( e.g., Trans-\nformers [187]) and public APIs of LLMs ( e.g., OpenAI) have\nsupported various decoding strategies to serve different\nscenarios of text generation. Next, we present the decoding\nsettings of several representative LLMs:\n\u2022T5[82] utilizes greedy search as the default setting and\napplies beam search (beam size of 4) with a length penalty\nof 0.6 for translation and summarization tasks.\n\u2022GPT-3 [55] employs beam search with a beam size of 4\nand a length penalty of 0.6 for all generation tasks.\n\u2022Alpaca [142] utilizes sampling-based strategies with\ntop-k(k= 50 ), top- p(p= 0.9), and temperature of 0.7 for\nopen-ended generation.\n\u2022LLaMA [57] applies diverse decoding strategies tai-\nlored to specific tasks. For instance, it employs the greedy\nsearch for question answering tasks while utilizes a sam-\npling strategy with the temperature settings of 0.1 (pass@1)\nand 0.8 (pass@100) for code generation.\n\u2022OpenAI API supports several basic decoding strate-\ngies, including greedy search (by setting temperature to\n0), beam search (with the setting best_of ), temperature\nsampling (with the setting temperature ), nucleus sam-\npling (with the setting top_p ). It also introduce param-\neters presence_penalty andfrequency_penalty to\ncontrol the repetition degree of generation. According to\nthe OpenAI\u2019s document, their APIs would produce different\noutputs even if the input and the hyper-parameters are the\nsame. Setting temperature to 0 can yield more deterministic\noutputs, albeit with a slight chance of variability.\n4.2.6 Summary and Discussion\nThe choice of architecture and pre-training tasks may incur\ndifferent inductive biases for LLMs, which would lead to\ndifferent model capacities. In this part, we discuss one open\nissue about the architecture choice for LLMs.Why does Predicting the Next Word Works?\nThe essence of decoder-only architecture is to\naccurately predict the next word for reconstructing\nthe pre-training data. Till now, there has been no\nformal study that theoretically demonstrates its\nadvantage over other architectures. An interesting\nexplanation was from Ilya Sutskever during the\ninterview held by Jensen Huanga. The original\ntranscript from the interview was copied belowb:\nSay you read a detective novel. It\u2019s\nlike complicated plot, a storyline,\ndifferent characters, lots of events,\nmysteries like clues, it\u2019s unclear.\nThen, let\u2019s say that at the last\npage of the book, the detective has\ngathered all the clues, gathered\nall the people and saying, \"okay,\nI\u2019m going to reveal the identity of\nwhoever committed the crime and that\nperson\u2019s name is\". Predict that word.\n...\nNow, there are many different words.\nBut predicting those words better and\nbetter, the understanding of the text\nkeeps on increasing. GPT-4 predicts\nthe next word better.\na. https://www.nvidia.com/en-us/on-\ndemand/session/gtcspring23-S52092/\nb. https://lifearchitect.ai/ilya/\nArchitecture Choice . In earlier literature of pre-trained lan-\nguage models, there are lots of discussions on the effects\nof different architectures [29, 89]. However, most LLMs are\ndeveloped based on the causal decoder architecture, and\nthere still lacks a theoretical analysis on its advantage over\nthe other alternatives. Next, we briefly summarize existing\ndiscussions on this issue.\n\u2022By pre-training with the LM objective, it seems that\ncausal decoder architecture can achieve a superior zero-\nshot and few-shot generalization capacity. Existing research\nhas shown that without multi-task fine-tuning, the causal\ndecoder has better zero-shot performance than other archi-\ntectures [29]. The success of GPT-3 [55] has demonstrates\nthat the large causal decoder model can be a good few-\nshot learner. In addition, instruction tuning and alignment\ntuning discussed in Section 5 have been proven to fur-\nther enhance the capability of large causal decoder mod-\nels [66, 67, 69].\n\u2022Scaling law has been widely observed in causal de-\ncoders. By scaling the model size, the dataset size, and\nthe total computation, the performance of causal decoders\ncan be substantially improved [30, 55]. Thus, it has become\nan important strategy to increase the model capacity of\nthe causal decoder via scaling. However, more detailed\ninvestigation on encoder-decoder models is still lacking, and\nmore efforts are needed to investigate the performance of\nencoder-decoder models at a large scale.\nMore research efforts about the discussions on architec-30\ntures and pre-training objectives are in need to analyze how\nthe choices of the architecture and pre-training tasks affect\nthe capacity of LLMs, especially for encoder-decoder archi-\ntectures. Despite the effectiveness of decoder-only architec-\nture, it is also suggested to make more diverse exploration\non architecture design. Besides the major architecture, the\ndetailed configuration of LLM is also worth attention, which\nhas been discussed in Section 4.2.2.\n4.3 Model Training\nIn this part, we review the important settings, techniques,\nor tricks for training LLMs.\n4.3.1 Optimization Setting\nFor parameter optimization of LLMs, we present the com-\nmonly used settings for batch training, learning rate, opti-\nmizer, and training stability.\nBatch Training. For language model pre-training, existing\nwork generally sets the batch size to a large number ( e.g.,\n2,048 examples or 4M tokens) to improve the training\nstability and throughput. For LLMs such as GPT-3 and\nPaLM, they have introduced a new strategy that dynam-\nically increases the batch size during training, ultimately\nreaching a million scale. Specifically, the batch size of GPT-3\nis gradually increasing from 32K to 3.2M tokens. Empirical\nresults have demonstrated that the dynamic schedule of\nbatch size can effectively stabilize the training process of\nLLMs [56].\nLearning Rate. Existing LLMs usually adopt a similar learn-\ning rate schedule with the warm-up and decay strategies\nduring pre-training. Specifically, in the initial 0.1% to 0.5%\nof the training steps, a linear warm-up schedule is employed\nfor gradually increasing the learning rate to the maximum\nvalue that ranges from approximately 5\u00d710\u22125to1\u00d710\u22124\n(e.g.,6\u00d710\u22125for GPT-3). Then, a cosine decay strategy\nis adopted in the subsequent steps, gradually reducing the\nlearning rate to approximately 10% of its maximum value,\nuntil the convergence of the training loss.\nOptimizer. The Adam optimizer [328] and AdamW opti-\nmizer [329] are widely utilized for training LLMs ( e.g., GPT-\n3), which are based on adaptive estimates of lower-order\nmoments for first-order gradient-based optimization. Com-\nmonly, its hyper-parameters are set as follows: \u03b21= 0.9,\n\u03b22= 0.95and\u03f5= 10\u22128. Meanwhile, the Adafactor op-\ntimizer [330] has also been utilized in training LLMs ( e.g.,\nPaLM and T5), which is a variant of the Adam optimizer\nspecially designed for conserving GPU memory during\ntraining. The hyper-parameters of the Adafactor optimizer\nare set as: \u03b21= 0.9and\u03b22= 1.0\u2212k\u22120.8, where kdenotes\nthe number of training steps.\nStabilizing the Training. During the pre-training of LLMs,\nit often suffers from the training instability issue, which\nmay cause the model collapse. To address this issue, weight\ndecay and gradient clipping have been widely utilized,\nwhere existing studies [55, 78, 90, 93, 113] commonly set\nthe threshold of gradient clipping to 1.0 and weight decay\nrate to 0.1. However, with the scaling of LLMs, the training\nloss spike is also more likely to occur, leading to unstabletraining. To mitigate this problem, PaLM [56] and OPT [90]\nuse a simple strategy that restarts the training process from\nan earlier checkpoint before the occurrence of the spike and\nskips over the data that may have caused the problem.\nFurther, GLM [93] finds that the abnormal gradients of the\nembedding layer usually lead to spikes, and proposes to\nshrink the embedding layer gradients to alleviate it.\n4.3.2 Scalable Training Techniques\nAs the model and data sizes increase, it has become chal-\nlenging to efficiently train LLMs under a limited compu-\ntational resource. Especially, two primary technical issues\nare required to be resolved, i.e.,increasing training through-\nput and loading larger models into GPU memory. In this\npart, we review several widely used approaches in existing\nwork to address the above two challenges, namely 3D\nparallelism [75, 331, 332], ZeRO [333], and mixed precision\ntraining [334], and also give general suggestions about how\nto utilize them for training.\n3D Parallelism. 3D parallelism is actually a combination of\nthree commonly used parallel training techniques, namely\ndata parallelism, pipeline parallelism [331, 332], and tensor\nparallelism [75]24. We next introduce the three parallel train-\ning techniques.\n\u2022Data parallelism. Data parallelism is one of the most\nfundamental approaches to improving the training through-\nput. It replicates the model parameters and optimizer states\nacross multiple GPUs and then distributes the whole train-\ning corpus into these GPUs. In this way, each GPU only\nneeds to process the assigned data for it, and performs\nthe forward and backward propagation to obtain the gra-\ndients. The computed gradients on different GPUs will be\nfurther aggregated to obtain the gradients of the entire batch\nfor updating the models in all GPUs. In this way, as the\ncalculations of gradients are independently performed on\ndifferent GPUs, the data parallelism mechanism is highly\nscalable, enabling the way that increases the number of\nGPUs to improve training throughput. Furthermore, this\ntechnique is simple in implementation, and most of existing\npopular deep learning libraries have already implemented\ndata parallelism, such as TensorFlow and PyTorch.\n\u2022Pipeline parallelism. Pipeline parallelism aims to dis-\ntribute the different layers of a LLM into multiple GPUs.\nEspecially, in the case of a Transformer model, pipeline\nparallelism loads consecutive layers onto the same GPU, to\nreduce the cost of transmitting the computed hidden states\nor gradients between GPUs. However, a naive implemen-\ntation of pipeline parallelism may result in a lower GPU\nutilization rate as each GPU has to wait for the previous\none to complete the computation, leading to the unneces-\nsary cost of bubbles overhead [331]. To reduce these bubbles\nin pipeline parallelism, GPipe [331] and PipeDream [332]\npropose the techniques of padding multiple batches of data\nand asynchronous gradient update to improve the pipeline\nefficiency.\n\u2022Tensor parallelism. Tensor parallelism is also a com-\nmonly used technique that aims to decompose the LLM for\n24. Model parallelism is a more broader term that includes tensor\nparallelism and pipeline parallelism in some work [75].31\nTABLE 7: Detailed optimization settings of several existing LLMs.\nModelBatch Size\n(#tokens)Learning\nRateWarmup Decay Method OptimizerPrecision\nTypeWeight\nDecayGrad\nClipDropout\nGPT3 (175B) 32K \u21923.2M 6\u00d710\u22125yes cosine decay to 10% Adam FP16 0.1 1.0 -\nPanGu- \u03b1(200B) - 2\u00d710\u22125- - Adam - 0.1 - -\nOPT (175B) 2M 1.2\u00d710\u22124yes manual decay AdamW FP16 0.1 - 0.1\nPaLM (540B) 1M \u21924M 1\u00d710\u22122no inverse square root Adafactor BF16 lr21.0 0.1\nBLOOM (176B) 4M 6\u00d710\u22125yes cosine decay to 10% Adam BF16 0.1 1.0 0.0\nMT-NLG (530B) 64 K \u21923.75M 5\u00d710\u22125yes cosine decay to 10% Adam BF16 0.1 1.0 -\nGopher (280B) 3M \u21926M 4\u00d710\u22125yes cosine decay to 10% Adam BF16 - 1.0 -\nChinchilla (70B) 1.5M \u21923M 1\u00d710\u22124yes cosine decay to 10% AdamW BF16 - - -\nGalactica (120B) 2M 7\u00d710\u22126yes linear decay to 10% AdamW - 0.1 1.0 0.1\nLaMDA (137B) 256K - - - - BF16 - - -\nJurassic-1 (178B) 32 K \u21923.2M 6\u00d710\u22125yes - - - - - -\nLLaMA (65B) 4M 1.5\u00d710\u22124yes cosine decay to 10% AdamW - 0.1 1.0 -\nLLaMA 2 (70B) 4M 1.5\u00d710\u22124yes cosine decay to 10% AdamW - 0.1 1.0 -\nFalcon (40B) 2M 1.85\u00d710\u22124yes cosine decay to 10% AdamW BF16 0.1 - -\nGLM (130B) 0.4M \u21928.25M 8\u00d710\u22125yes cosine decay to 10% AdamW FP16 0.1 1.0 0.1\nT5 (11B) 64K 1\u00d710\u22122no inverse square root AdaFactor - - - 0.1\nERNIE 3.0 Titan (260B) - 1\u00d710\u22124- - Adam FP16 0.1 1.0 -\nPanGu- \u03a3(1.085T) 0.5M 2\u00d710\u22125yes - Adam FP16 - - -\nmulti-GPU loading. Unlike pipeline parallelism, tensor par-\nallelism focuses on decomposing the tensors (the parameter\nmatrices) of LLMs. For a matrix multiplication operation\nY=XA in the LLM, the parameter matrix Acan be\nsplit into two submatrices, A1andA2, by column, which\ncan be expressed as Y= [XA1, XA 2]. By placing matrices\nA1andA2on different GPUs, the matrix multiplication\noperation would be invoked at two GPUs in parallel, and\nthe final result can be obtained by combining the outputs\nfrom the two GPUs through across-GPU communication.\nCurrently, tensor parallelism has been supported in several\nopen-source libraries, e.g., Megatron-LM [75], and can be\nextended to higher-dimensional tensors. Also, Colossal-AI\nhas implemented tensor parallelism for higher-dimensional\ntensors [335\u2013337] and proposed sequence parallelism [338]\nespecially for sequence data, which can further decompose\nthe attention operation of the Transformer model.\nZeRO. ZeRO [333] technique, proposed by the Deep-\nSpeed [74] library, focuses on the issue of memory re-\ndundancy in data parallelism. As mentioned before, data\nparallelism requires each GPU to store the same copy of\na LLM, including model parameters, model gradients, and\noptimizer parameters. Whereas, not all of the above data is\nnecessary to be retained on each GPU, which would cause\na memory redundancy problem. To resolve it, the ZeRO\ntechnique aims to retain only a fraction of data on each\nGPU, while the rest data can be retrieved from other GPUs\nwhen required. Specifically, ZeRO provides three solutions,\ndepending on how the three parts of the data are stored,\nnamely optimizer state partitioning, gradient partitioning,\nand parameter partitioning. Empirical results indicate that\nthe first two solutions do not increase the communication\noverhead, and the third solution increases about 50% com-\nmunication overhead but saves memory proportional to\nthe number of GPUs. PyTorch has implemented a similar\ntechnique as ZeRO, called FSDP [339].\nMixed Precision Training. In previous PLMs ( e.g.,\nBERT [23]), 32-bit floating-point numbers, also known as\nFP32, have been predominantly used for pre-training. In\nrecent years, to pre-train extremely large language models,some studies [334] have started to utilize 16-bit floating-\npoint numbers (FP16), which reduces memory usage and\ncommunication overhead. Additionally, as popular NVIDIA\nGPUs ( e.g., A100) have twice the amount of FP16 computa-\ntion units as FP32, the computational efficiency of FP16 can\nbe further improved. However, existing work has found that\nFP16 may lead to the loss of computational accuracy [64, 78],\nwhich affects the final model performance. To alleviate it, an\nalternative called Brain Floating Point (BF16) has been used\nfor training, which allocates more exponent bits and fewer\nsignificant bits than FP16. For pre-training, BF16 generally\nperforms better than FP16 on representation accuracy [78].\nOverall Training Suggestion. In practice, the above train-\ning techniques, especially 3D parallelism, are often jointly\nused to improve the training throughput and large model\nloading. For instance, researchers have incorporated 8-way\ndata parallelism, 4-way tensor parallelism, and 12-way\npipeline parallelism, enabling the training of BLOOM [78]\non 384 A100 GPUs. Currently, open-source libraries like\nDeepSpeed [74], Colossal-AI [189], and Alpa [340] can well\nsupport the three parallel training methods. To reduce the\nmemory redundancy, ZeRO, FSDP , and activation recom-\nputation techniques [77, 341] can be also employed for\ntraining LLMs, which have already been integrated into\nDeepSpeed, PyTorch, and Megatron-LM. In addition, the\nmixed precision training technique such as BF16 can be\nalso leveraged to improve the training efficiency and reduce\nGPU memory usage, while it requires necessary support on\nhardware ( e.g., A100 GPU). Because training large models is\na time-intensive process, it would be useful to forecast the\nmodel performance and detect abnormal issues at an early\nstage. For this purpose, GPT-4 [46] has recently introduced\na new mechanism called predictable scaling built on a deep\nlearning stack, enabling the performance prediction of large\nmodels with a much smaller model, which might be quite\nuseful for developing LLMs. In practice, one can further\nleverage the supporting training techniques of mainstream\ndeep learning frameworks. For instance, PyTorch supports\nthe data parallel training algorithm FSDP [339] ( i.e.,fully\nsharded data parallel), which allows for partial offloading32\nof training computations to CPUs if desired.\n5 A DAPTATION OF LLM S\nAfter pre-training, LLMs can acquire the general abilities\nfor solving various tasks. However, an increasing number\nof studies have shown that LLM\u2019s abilities can be further\nadapted according to specific goals. In this section, we\nintroduce two major approaches to adapting pre-trained\nLLMs, namely instruction tuning and alignment tuning. The\nformer approach mainly aims to enhance (or unlock) the\nabilities of LLMs, while the latter approach aims to align the\nbehaviors of LLMs with human values or preferences. Fur-\nther, we will also discuss efficient tuning and quantization\nfor model adaptation in resource-limited settings. In what\nfollows, we will introduce the four parts in detail.\n5.1 Instruction Tuning\nIn essence, instruction tuning is the approach to fine-tuning\npre-trained LLMs on a collection of formatted instances in\nthe form of natural language [67], which is highly related\nto supervised fine-tuning [66] and multi-task prompted\ntraining [28]. In order to perform instruction tuning, we first\nneed to collect or construct instruction-formatted instances.\nThen, we employ these formatted instances to fine-tune\nLLMs in a supervised learning way ( e.g., training with the\nsequence-to-sequence loss). After instruction tuning, LLMs\ncan demonstrate superior abilities to generalize to unseen\ntasks [28, 67, 69], even in a multilingual setting [94].\nA recent survey [342] presents a systematic overview\nof the research on instruction tuning. In comparison to\nthat, we mainly focus on the effect of instruction tuning\non LLMs and provide detailed guidelines or strategies for\ninstance collection and tuning. In addition, we also discuss\nthe use of instruction tuning for satisfying the real needs of\nusers, which has been widely applied in existing LLMs, e.g.,\nInstructGPT [66] and GPT-4 [46].\n5.1.1 Formatted Instance Construction\nGenerally, an instruction-formatted instance consists of a\ntask description (called an instruction ), an optional input,\nthe corresponding output, and a small number of demon-\nstrations (optional). As important public resources, existing\nstudies have released a large number of labeled data format-\nted in natural language (see the list of available resources in\nTable 3) as introduced in Section 3.3.1. Next, we introduce\nthree major methods for constructing formatted instances\n(see an illustration in Figure 11) and then discuss several\nkey factors for instance construction.\nFormatting NLP Task Datasets. Before instruction tuning\nwas proposed, several early studies [168, 343, 344] collected\nthe instances from a diverse range of traditional NLP tasks\n(e.g., text summarization, text classification, and translation)\nto create supervised multi-task training datasets. As a major\nsource of instruction tuning instances, it is convenient to for-\nmat these multi-task training datasets with natural language\ntask descriptions. Specifically, recent work [28, 66, 67, 88]\naugments the labeled datasets with human-written task de-\nscriptions, which instructs LLMs to understand the tasks by\nexplaining the task goal. For example, in Figure 11(a), a taskdescription \u201c Please answer this question \u201d is added for each\nexample in the question-answering task. After instruction\ntuning, LLMs can generalize well to other unseen tasks by\nfollowing their task descriptions [28, 67, 69]. In particular,\nit has been shown that instructions are the crucial factor\nin task generalization ability for LLMs [67]: by fine-tuning\nthe model on labeled datasets with the task descriptions re-\nmoved, it results in a dramatic drop in model performance.\nTo better generate labeled instances for instruction tuning,\na crowd-sourcing platform, PromptSource [167] has been\nproposed to effectively create, share, and verify the task\ndescriptions for different datasets. To enrich the training\ninstances, several studies [28, 168, 345] also try to invert the\ninput-output pairs of existing instances with specially de-\nsigned task descriptions for instruction tuning. For instance,\ngiven a question-answer pair, we can create a new instance\nby predicting the answer-conditioned question ( e.g., \u201cPlease\ngenerate a question based on the answer:\u201d ).\nFormatting Daily Chat Data. Despite that a large number\nof training instances have been formatted with instructions,\nthey mainly come from public NLP datasets, either lack-\ning instruction diversity or mismatching with real human\nneeds [66]. To overcome this issue, InstructGPT [66] pro-\nposes to take the queries that real users have submitted to\nthe OpenAI API as the task descriptions. Additionally, to\nenrich the task diversity, human labelers are also asked to\ncompose the instructions for real-life tasks, including open-\nended generation, open question answering, brainstorm-\ning, and chatting. Then, they let another group of labelers\ndirectly answer these instructions as the output. Finally,\nthey pair one instruction ( i.e.,the collected user query) and\nthe expected output ( i.e.,the human-written answer) as a\ntraining instance. Note that InstructGPT also employs these\nreal-world tasks formatted in natural language for align-\nment tuning (discussed in Section 5.2). Further, GPT-4 [46]\nhas designed potentially high-risk instructions and guided\nthe model to reject these instructions through supervised\nfine-tuning for safety concerns. Considering the absence\nof high-quality public chat data, several studies have also\ncollected users\u2019 chat requests as input data, and then utilized\nChatGPT or GPT-4 to generate responses as output data. A\nnotable example of such a dataset is the conversational data\nfrom ShareGPT [148]. Additionally, Dolly [172] and Ope-\nnAssistant [173] have further released their conversation\ndata, which has been carefully labeled by human annotators\nto attain a high level of quality.\nFormatting Synthetic Data. To reduce the burden of human\nannotation or manual collection, several semi-automated\napproaches [143] have been proposed for constructing in-\nstances by feeding existing instances into LLMs to synthe-\nsize diverse task descriptions and instances. As illustrated\nin Figure 11(c), the Self-Instruct method only needs 175\ninstances as the initial task pool. Then, they randomly select\na few instances from the pool as demonstrations and prompt\na LLM to generate new instructions and corresponding\ninput-output pairs. After the quality and diversity filter-\ning, newly generated instances would be added into the\ntask pool. Hence, the synthetic method is an effective and\neconomical way to generate large-scale instruction data for33\n(a) Formatting Task Datasets (b) Formatting Daily Chat DataHuman -written\nAPI collection\n&\nNLP  Datasets\nHuman -written\nPlease answer this question:Task description\nQ: What is the capital of France?\nA: Paris.\nQ: What is the capital of Brazil?\nA: BrasiliaDemonstrations\nQ: What is the capital of China?\nA: Beijing.Output InputDesired output written by human\nHere are some ways to lose weight:\n1. Eat a healthy diet: Focus on \u2026\n2. Increase physical activity: Engage \u2026OutputCan you recommend some ways \nto lose weight?Task description\nLLM\nGive me a quote from a \nfamous person on this topic.Task descriptionInstruction \nGeneration\nLLMInput -Output \nGeneration\nInput: The importance of being honest. \nOutput: Honesty is the first chapter in \nthe book of wisdom.Output InputInstance Pool\nFilter\n(c) Formatting Synthetic DataSeed \nInstances\nFig. 11: An illustration of instance formatting and three different methods for constructing the instruction-formatted\ninstances.\nLLMs. However, the instances generated by the Self-Instruct\nmethod might be simplistic or lack the diversity. To improve\nthe quality of synthetic int ructions, WizardLM [346] intro-\nduces Evol-Instruct by proposing in-depth and in-breadth\nevolving to enrich the complexity and diversity of the\ninstances. Furthermore, Self-Align [347] establishes multiple\nhuman-aligned principles to filter the synthesized instances.\nIt then employs these instances to train a LLM in order\nto yield more aligned instances. To enhance the quality\nof the instance output, researchers directly adopt human-\nwritten texts as the output and synthesize corresponding\ninstructions using ICL examples [348].\nKey Factors for Instance Construction. The quality of\ninstruction instances has an important impact on the perfor-\nmance of the model. Here, we discuss some essential factors\nfor instance construction.\n\u2022Scaling the instructions. It has been widely shown that\nscaling the number of tasks can largely enhance the gen-\neralization ability of LLMs [28, 67, 88]. With the increasing\nof the task number, the model performance initially shows\na continuous growth pattern, while the gain becomes neg-\nligible when it reaches a certain level [69, 88]. A plausible\nspeculation is that a certain number of representative tasks\ncan provide relatively sufficient knowledge and adding\nmore tasks may not bring additional gains [69]. Also, it is\nbeneficial to enhance the diversity of the task descriptions in\nseveral aspects, such as length, structure, and creativity [28].\nAs for the number of instances per task, it has been found\nthat a small number of instances can usually saturate the\ngeneralization performance of the model to perform a spe-\ncific task [67, 69]. Specially, several recent work [349, 350]\nhas explored the effect of fine-tuning with a small amount of\nhigh-quality instruction data ( e.g., one or a few thousand in-\nstances), showing very promising results on the evaluation\ntasks. In contrast, another line of studies continue to explore\nthe scaling effect of instruction data [351, 352]. For example,\nOrca [351] scales up the synthesized instances to 5 million\nwith step-by-step explanations, and it achieves superiorperformance across a wide range of tasks compared to the\nmethods tuned with instruction data.\n\u2022Formatting design. As an important factor, the design\nof natural language format also highly impacts the gener-\nalization performance of LLMs [88]. Typically, we can add\ntask descriptions and optional demonstrations to the input-\noutput pairs of existing datasets, where the task description\nis the most key part for LLMs to understand the task [88].\nFurther, it can lead to substantial improvements by using an\nappropriate number of exemplars as demonstrations [69],\nwhich also alleviates the model sensitivity to instruction\nengineering [67, 69]. However, incorporating other compo-\nnents ( e.g., things to avoid, reasons, and suggestions) into\ninstructions may have a negligible or even adverse effect\non the performance of LLMs [88, 166]. Recently, to elicit\nthe step-by-step reasoning ability of LLMs, some work [69]\nproposes to include chain-of-thought (CoT) examples for\nsome reasoning datasets, such as arithmetic reasoning. It\nhas been shown that fine-tuning LLMs with both CoT and\nnon-CoT examples can lead to a good performance across\nvarious reasoning tasks, including those that require multi-\nhop reasoning ability ( e.g., commonsense question answer-\ning and arithmetic reasoning) as well as those without the\nneed for such a reasoning way ( e.g., sentiment analysis and\nextractive question answering) [69, 95].\nTo summarize, diversity and quality of instructions seem\nto be more important than the number of instances [349]\nsince the well-performing InstructGPT [66] and LLaMA-2-\nChat [99] utilize fewer but more diverse instructions (or\ninstances) than the Flan-series LLMs [67, 69]. However,\na large amount of training data may compensate for the\nabsence of high-quality data [351]. Further, it is more useful\nto invite labelers to compose human-need tasks than using\ndataset-specific tasks. However, it still lacks general guide-\nlines to annotate human-need instances, making the task\ncomposition somehow heuristic. To reduce human efforts,\nwe can either reuse existing formatted datasets (Table 3)\nor automatically construct the instructions using existing\nLLMs [143]. We conduct a preliminary experiment to show34\nthe effectiveness of different construction methods in Sec-\ntion 5.1.4.\n5.1.2 Instruction Tuning Strategies\nUnlike pre-training, instruction tuning is often more effi-\ncient since only a moderate number of instances are used\nfor training. Since instruction tuning can be considered as\na supervised training process, its optimization is different\nfrom pre-training in several aspects [69], such as the training\nobjective ( i.e.,sequence-to-sequence loss) and optimization\nconfiguration ( e.g., smaller batch size and learning rate),\nwhich require special attention in practice. In addition to\nthese optimization configurations, there are also four im-\nportant aspects to consider for instruction tuning:\nBalancing the Data Distribution. Since instruction tun-\ning involves a mixture of different tasks, it is important\nto balance the proportion of different tasks during fine-\ntuning. A widely used method is the examples-proportional\nmixing strategy [82], i.e., combining all the datasets and\nsampling each instance equally from the mixed datasets.\nFurthermore, increasing the sampling ratio of high-quality\ncollections ( e.g., FLAN [67] and P3 [167]) can generally\nlead to performance improvement according to recent find-\nings [69, 95]. Further, it is common to set a maximum\ncapto control the maximum number of examples that a\ndataset can contain during instruction tuning [82], which\nis set to prevent larger datasets from overwhelming the\nentire distribution [82, 95]. In practice, the maximum cap\nis typically set to several thousands or tens of thousands\naccording to different datasets [67, 69]. Recently, it has been\nempirically found that existing instruction datasets (Table 3)\nmainly focus on enhancing LLMs\u2019 capabilities in certain\naspects, and a single dataset alone cannot lead to a compre-\nhensive enhancement in model capacity [353]. Therefore, it\nis often suggested to use a mixture of existing instruction\ndatasets to achieve a balanced improvement in different\ncapacities, including NLP task data ( e.g., FLAN v2 [292]),\nchat data ( e.g., ShareGPT [148]), and synthetic data ( e.g.,\nGPT4-Alpaca [354]).\nCombining Instruction T uning and Pre-Training. To make\nthe tuning process more effective and stable, OPT-IML [95]\nincorporates pre-training data during instruction tuning,\nwhich can be regarded as regularization for model tuning.\nFurther, instead of using a separate two-stage process ( pre-\ntraining then instruction tuning ), some studies attempt to\ntrain a model from scratch with a mixture of pre-training\ndata ( i.e.,plain texts) and instruction tuning data ( i.e.,for-\nmatted datasets) using multi-task learning [82]. Specifically,\nGLM-130B [93] and Galactica [35] integrate instruction-\nformatted datasets as a small proportion of the pre-training\ncorpora to pre-train LLMs, which potentially achieves the\nadvantages of pre-training and instruction tuning at the\nsame time.\nMulti-stage Instruction T uning. For instruction tuning,\nthere are two kinds of important instruction data, namely\ntask-formatted instructions and daily chat instructions. Gen-\nerally, the former has a significantly larger volume than the\nlatter. It is important to balance the training with the two\nkinds of instruction data. In addition to carefully mixingdifferent instruction data, we can also adopt a multi-stage\ninstruction tuning strategy [352], where LLMs are first fine-\ntuned with large-scale task-formatted instructions and sub-\nsequently fine-tuned on daily chat ones. To avoid the capac-\nity forgetting issue, it is also useful to add an amount of task-\nformatted instructions at the second stage. Actually, such\na multi-stage tuning strategy can be also applied to other\nsettings for instruction tuning. For example, we can sched-\nule different fine-tuning stages with progressively increased\nlevels on difficulty and complexity, and gradually improve\nthe capacities of LLMs to follow complex instructions.\nOther Practical Tricks. In practice, there are also several\nuseful strategies and tricks that are helpful to improve the\nfine-tuning performance of LLMs. We list several represen-\ntative ones as follows:\n\u2022Efficient training for multi-turn chat data. Given a multi-\nturn chat example (the conversation between a user and\nchatbot), a straightforward fine-tuning way is to split it into\nmultiple context-response pairs for training: a LLM is fine-\ntuned to generate the response based on the correspond-\ning context for all splits ( i.e., at each utterance from the\nuser). In such a fine-tuning way, it is apparent that there\nexist overlapping utterances in the split examples from a\nconversation. To save the training cost, Vicuna [138] has\nadopted an efficient way that feeds the whole conversation\ninto the LLM, but relies on a loss mask that only computes\nthe loss on the responses of the chatbot for training. It can\nsignificantly reduce the compute costs derived from the\noverlapped utterances.\n\u2022Establishing self-identification for LLM. To deploy LLMs\nfor real-world applications, it is necessary to establish its\nidentity and make LLMs aware of these identity informa-\ntion, such as name, developer and affiliation. A practical\nway is to create identity-related instructions for fine-tuning\nthe LLM. It is also feasible to prefix the input with the self-\nidentification prompt, e.g., \u201cThe following is a conversation\nbetween a human and an AI assistant called CHATBOT NAME ,\ndeveloped by DEVELOPER .\u201d, where C HATBOT NAME and D E-\nVELOPER refer to the name and developer of the chatbot,\nrespectively.\nIn addition to the above practical strategies and tricks,\nexisting work has also used other tricks, e.g., concatenating\nmultiple examples into a single sequence to approach the\nmax length [355].\n5.1.3 The Effect of Instruction Tuning\nIn this part, we discuss the effect of instruction tuning on\nLLMs in three major aspects.\nPerformance Improvement. Despite being tuned on a mod-\nerate number of instances, instruction tuning has become\nan important way to improve or unlock the abilities of\nLLMs [69]. Recent studies have experimented with language\nmodels in multiple scales (ranging from 77M to 540B),\nshowing that the models of different scales can all benefit\nfrom instruction tuning [69, 345], yielding improved perfor-\nmance as the parameter scale increases [94]. Further, smaller\nmodels with instruction tuning can even perform better\nthan larger models without fine-tuning [28, 69]. Besides\nthe model scale, instruction tuning demonstrates consistent\nimprovements in various model architectures, pre-training35\nTABLE 8: Basic statistics of the required number of GPUs, tuning time, batch size (denoted as BS) per device (full tuning\nand LoRA tuning), and inference rate (the number of generated tokes per second). Our experiments are conducted based\non two Linux servers having 8 A800-80G SXM4 GPUs with 6 NVSwitch and 8 3090-24G GPUs, respectively. The major\ndifference between A800 and A100 lies in the NVLink interconnect speed. Thus, our estimations about training and\ninference efficiency would be slightly improved for A100, while the rest memory consumption would remain the same.\nFor full tuning experiments, we use data parallel training, ZeRO Stage 3, BF16, and gradient checkpointing. Additionally,\nthe LoRA tuning can be executed on one 80G GPU utilizing INT8 quantization with the rank setting set to 16. All the\nexperiments are conducted with Alpaca-52K dataset by training LLaMA models three epochs. The max sequence length\nfor both training settings is set to 512. The inference experiments are performed with the batch size set to 1.\nModelsA800 Full Training A800 LoRA Training A800 Inference (16-bit) 3090 Inference (16-bit) 3090 Inference (8-bit)\n#GPU BS Time #GPU BS Time #GPU #Token/s #GPU #Token/s #GPU #Token/s\nLLaMA (7B) 2 8 3.0h 1 80 3.5h 1 36.6 1 24.3 1 7.5\nLLaMA (13B) 4 8 3.1h 1 48 5.1h 1 26.8 2 9.9 1 4.5\nLLaMA (30B) 8 4 6.1h 1 24 14.3h 1 17.7 4 3.8 2 2.6\nLLaMA (65B) 16 2 11.2h 1 4 60.6h 2 8.8 8 2.0 4 1.5\nobjectives, and model adaptation methods [69]. In practice,\ninstruction tuning offers a general approach to enhancing\nthe abilities of existing language models [69] (including\nsmall-sized PLMs). Also, it is much less costly than pre-\ntraining, since the amount of instruction data required by\nLLMs is significantly smaller than pre-training data.\nTask Generalization. Instruction tuning encourages the\nmodel to understand natural language instructions for task\ncompletion. It endows LLMs with the ability (often con-\nsidered as an emergent ability) to follow human instruc-\ntions [31] to perform specific tasks without demonstrations,\neven on unseen tasks [69]. A large number of studies\nhave confirmed the effectiveness of instruction tuning to\nachieve superior performance on both seen and unseen\ntasks [95, 345]. Also, instruction tuning has been shown to\nbe useful in alleviating several weaknesses of LLMs ( e.g.,\nrepetitive generation or complementing the input without\naccomplishing a certain task) [66, 69], leading to a superior\ncapacity to solve real-world tasks for LLMs. Furthermore,\nLLMs trained with instruction tuning can generalize to re-\nlated tasks across languages. For example, BLOOMZ-P3 [94]\nis fine-tuned based on BLOOM [78] using English-only task\ncollection P3 [167]. Interestingly, BLOOMZ-P3 can achieve\na more than 50% improvement in multilingual sentence\ncompletion tasks compared to BLOOM, which shows that\ninstruction tuning can help LLMs acquire general task skills\nfrom English-only datasets and transfer such skills into\nother languages [94]. In addition, it has been found that\nusing English-only instructions can produce satisfactory\nresults on multilingual tasks [94], which helps reduce the\neffort of instruction engineering for a specific language.\nDomain Specialization. Existing LLMs have showcased su-\nperior capabilities in traditional NLP tasks ( e.g., generation\nand reasoning) and daily questions. However, they may\nstill lack domain knowledge to accomplish specific tasks,\nsuch as medicine, law, and finance (See Section 8 for a\ndetailed discussion of LLMs in different applications). In-\nstruction tuning is an effective approach to adapting existing\ngeneral LLMs to be domain-specific experts. For instance,\nresearchers propose to fine-tune Flan-PaLM [69] using medi-\ncal datasets to create Med-PaLM [356], a medical knowledge\nassistant that achieves performance levels comparable tothose of expert clinicians. Furthermore, a recent study [357]\nfine-tunes FLAN-T5 to support e-commerce recommender\nsystems with natural language instructions, showing strong\nperformance in a variety of recommendation tasks. There\nare also several open-sourced medical models instruction-\ntuned based on LLaMA [57], such as BenTsao [358]. Also,\nresearchers explore instruction tuning on law [359], fi-\nnance [360], and arithmetic computation [361].\n5.1.4 Empirical Analysis for Instruction Tuning\nFine-tuning LLMs with different instruction sets tend to lead\nto model variants with varied performance on downstream\ntasks. In this section, we will explore the effect of different\ntypes of instructions in fine-tuning LLMs ( i.e.,LLaMA (7B)\nand LLaMA (13B)25), as well as examine the usefulness of\nseveral instruction improvement strategies.\nInstruction Datasets. According to the discussion in Sec-\ntion 5.1.1, we mainly consider three common kinds of in-\nstructions as follows:\n\u2022Task-specific instructions. For the first type of instruc-\ntions, we adopt the most commonly-used multi-task instruc-\ntion dataset, FLAN-T5 [69], which contains 1,836 tasks and\nover 15M instructions by combining four data mixtures from\nprior work.\n\u2022Daily chat instructions. This type of instructions are con-\nversations posed by users about daily life, which are more\nclosely related to real-life scenarios. We adopt the ShareGPT\ninstruciton set, consisting of 63K real-user instructions. It\nhas been used as the core instructions for Vicuna.\n\u2022Synthetic instructions. In addition to reusing existing\ninstructions, we can also automatically synthesize massive\ninstructions using LLMs. We adopt the popular synthetic\ninstruction dataset Self-Instruct-52K [143], consisting of 52K\ninstructions paired with about 82K instance inputs and\noutputs. These generated instructions have a similar data\ndistribution as the human-written seed tasks ( e.g., grammar\nchecking, brainstorming).\nAs the original FLAN-T5 dataset is very large ( i.e.,over\n15M), we randomly sample 80,000 instructions from it for\nconducting a fair comparison with other instruction datasets\n25. Due to the limit of computational resources, we cannot conduct\nlarge-scale experiments on larger LLaMA variants right now, which\nwould be scheduled in a future version.36\nTABLE 9: Results of instruction-tuning experiments (all in a single-turn conversation) based on the LLaMA (7B) and LLaMA\n(13B) model under the chat and QA setting. We employ four instruction improvement strategies on the Self-Instruct-52K\ndataset, i.e.,enhancing the complexity ( w/ complexity ), increasing the diversity ( w/ diversity ), balancing the difficulty ( w/\ndifficulty ), and scaling the instruction number ( w/ scaling ).\u2217Since we select the LLaMA (7B)/(13B) model fine-tuned on\nSelf-Instruct-52K as the baseline, we omit the win rate of the fine-tuned model with Self-Instruct-52K against itself.\nModelsDataset\nMixturesInstruction\nNumbersLexical\nDiversityChat QA\nAlpacaFarm MMLU BBH3k\nLLaMA (7B) \u2460FLAN-T5 80,000 48.48 23.77 38.58 32.79\n\u2461ShareGPT 63,184 77.31 81.30 38.11 27.71\n\u2462Self-Instruct-52K 82,439 25.92 /\u221737.52 29.81\n\u2461+\u2462 145,623 48.22 71.36 41.26 28.36\n\u2460+\u2461+\u2462 225,623 48.28 70.00 43.69 29.69\n\u2462Self-Instruct-52K 82,439 25.92 /\u221737.52 29.81\nw/ complexity 70,000 70.43 76.96 39.73 33.25\nw/ diversity 70,000 75.59 81.55 38.01 30.03\nw/ difficulty 70,000 73.48 79.15 32.55 31.25\nw/ scaling 220,000 57.78 51.13 33.81 26.63\nLLaMA (13B) \u2460FLAN-T5 80,000 48.48 22.12 34.12 34.05\n\u2461ShareGPT 63,184 77.31 77.13 47.49 33.82\n\u2462Self-Instruct-52K 82,439 25.92 /\u221736.73 25.43\n\u2461+\u2462 145,623 48.22 72.85 41.16 29.49\n\u2460+\u2461+\u2462 225,623 48.28 69.49 43.50 31.16\n\u2462Self-Instruct-52K 82,439 25.92 /\u221736.73 25.43\nw/ complexity 70,000 70.43 77.94 46.89 35.75\nw/ diversity 70,000 75.59 78.92 44.97 36.40\nw/ difficulty 70,000 73.48 80.45 43.15 34.59\nw/ scaling 220,000 57.78 58.12 38.07 27.28\n(i.e.,ShareGPT and Self-Instruct-52K) at a similar scale. In\nour experiments, we test on each individual instruction\nset to explore their own effects and also examine their\ncombinatorial effects on model performance.\nImprovement Strategies. Although real-world instructions\nfrom human users are more suitable for fine-tuning LLMs,\nit is difficult to collect them at a large scale. As alternatives\nto human-generated instructions, most existing research\nmainly adopts synthetic instructions generated by LLMs.\nHowever, there are some potential problems with synthetic\ninstructions, such as poor topic diversity and uneven in-\nstruction difficulty (either too simple or too difficult). Thus,\nit is necessary to improve the quality of the synthetic in-\nstructions. Next, we summarize four major improvement\nstrategies widely used in existing work as follows:\n\u2022Enhancing the instruction complexity. As discussed in\nexisting work [346], enhancing the complexity of instruc-\ntions can improve the model capacity of LLMs in following\ncomplex instructions, e.g., including more task demands or\nrequiring more reasoning steps. To validate this strategy,\nwe follow WizardLM [346] by gradually increasing the\ncomplexity levels, e.g., adding constraints, increasing rea-\nsoning steps, and complicating the input. We leverage the\npublicly released WizardLM-70K instructions [346] as the\ncomplexity-enhanced instruction dataset, which has been\ngenerated via the above enhancement approach based on\nthe Self-Instruct-52K dataset [346].\n\u2022Increasing the topic diversity. In addition to the complex-\nity, improving the topic diversity of the instruction dataset\ncan help elicit different abilities of LLMs on diverse tasks in\nreal world [347]. However, it is difficult to directly control\nthe self-instruct process for generating diverse instructions.Following YuLan-Chat [352], we employ ChatGPT to rewrite\nthe instructions from Self-Instruct-52K dataset for adapting\nthem into 293 topics via specific prompts. Finally, we obtain\n70K instructions as the diversity-increased dataset.\n\u2022Scaling the instruction number. In addition to the above\naspects, the number of instructions is also an important\nfactor that may affect the model performance. Specially,\nusing more instructions can extend the task knowledge and\nimprove the ability of instruction following for LLMs [69].\nTo examine this strategy, we sample new instructions from\nthe synthesized instruction set released from the MOSS\nproject [362], as they are also synthesized using the same\nself-instruct method [143]. We mix them with the Self-\nInstruct-52K dataset to compose a larger one containing\n220K instructions.\n\u2022Balancing the instruction difficulty. As the synthetic\ninstructions tend to contain too easy or too hard ones, it\nis likely to result in training instability or even overfitting\nfor LLMs. To explore the potential effects, we leverage\nthe perplexity score of LLMs to estimate the difficulty of\ninstructions and remove too easy or too hard instructions. To\ngenerate the same scale of instructions for fair comparison,\nwe adopt a LLaMA (7B) model to compute the perplexity for\nthe 220K instructions from the large instruction dataset, and\nthen keep 70K instructions of moderate perplexity scores as\nthe difficulty-balanced dataset.\nExperimental Setup. To conduct the experiments on the\neffect of instruction data, we leverage these new instruction\ndatasets for tuning LLaMA, a popular LLM backbone that\nhas been widely used for instruction-tuning. We use the\ncode from YuLan-Chat [352] for our experiments, and train\nLLaMA 7B and 13B on a server of 8 A800-80G GPUs. All37\nthe hyper-parameters settings remain the same as Stanford\nAlpaca. To better evaluate the instruction following ability\nof fine-tuned models, we consider two settings, namely\nChat setting and QA setting . The chat setting mainly utilizes\nuser instructions and queries from daily chat, whereas the\nQA setting mainly employs question answering examples\nfrom existing NLP datasets. The evaluation on the chat\nsetting is conducted based on the AlpacaFarm evaluation\nset [363]. Instead of using a full pairwise comparison, we\nselect the LLaMA 7B and 13B models fine-tuned on Self-\nInstruct-52K as the reference baselines, and then compare\nthem with other fine-tuned LLaMA 7B and 13B models\nusing different instructions, respectively. Since our focus is\nto examine the usefulness of different strategies to generate\nthe instructions, the model fine-tuned on Self-Instruct-52K\ncan serve as a good reference. Following AlpacaFarm [363],\nfor each comparison, we employ ChatGPT to automatically\nannotate which response from two compared models each\ntime is the best for the user query, and report the win\nrate (%) as the evaluation metric. For the QA setting, we\nselect two benchmarks, MMLU [364] and BBH [365], and\nevaluate the accuracy based on their default settings by\nusing heuristic rules to parse the answers from these LLMs.\nFor both instruction tuning and evaluation, we adopt\nthe following prompt: \u201c The following is a conversation be-\ntween a human and an AI assistant. The AI assistant gives\nhelpful, detailed, and polite answers to the user\u2019s questions. \\n\n[|Human |]:{input}\\n[|AI|]:\u201d. To reproduce our results, we\nrelease the code and data at the link: https://github.com/\nRUCAIBox/LLMSurvey/tree/main/Experiments.\nResults and Analysis. The results using different instruction\ndatasets based on 7B and 13B LLaMA are in Table 9. Next,\nwe summarize and analyze our findings in detail.\n\u2022Task-formatted instructions are more proper for the QA\nsetting, but may not be useful for the chat setting. By comparing\nthe performance of instruction tuning using FLAN-T5 with\nthat of ShareGPT and Self-Instruct-52K, we can observe\nthat FLAN-T5 mostly achieves a better performance on QA\nbenchmarks while underperforms ShareGPT on the chat set-\nting. The reason is that FLAN-T5 is composed of a mixture\nof instructions and examples from existing NLP tasks, e.g.,\ntranslation and reading comprehension. As a result, LLaMA\nfine-tuned with FLAN-T5 performs better on QA tasks, but\npoorly on user queries. In contrast, ShareGPT consists of\nreal-world human-ChatGPT conversations, which is able to\nbetter elicit LLaMA to follow user instructions in daily life,\nwhile may not be suitable for accomplishing the QA tasks.\n\u2022A mixture of different kinds of instructions are helpful to\nimprove the comprehensive abilities of LLMs. After mixing the\nthree kinds of instructions for fine-tuning, we can see that\nthe derived LLaMA variant (with FLAN-T5, ShareGPT and\nSelf-Instruct-52K) performs well in both task settings. In\nMMLU, the performance of LLaMA (7B) can surpass the\nones using individual instruction set by a large margin, i.e.,\n43.69 vs. 38.58 (FLAN-T5). It shows that mixing multiple\nsources of instruction datasets is helpful to improve the\nperformance of instruction-tuned LLMs, which scales the\ninstruction number as well as increases the diversity.\n\u2022Enhancing the complexity and diversity of instructions\nleads to an improved model performance. By increasing thecomplexity and diversity of the Self-Instruct-52K dataset\nrespectively, the chat and QA performance of LLaMA can\nbe consistently improved, e.g., from 37.52 to 39.73 in MMLU\nfor LLaMA (7B). It demonstrates that both strategies are\nuseful to improve the instruction following ability of LLMs.\nFurther, we can see that improving the complexity yields a\nlarger performance improvement on QA tasks. The reason\nis that the QA tasks mostly consist of difficult questions for\nevaluating LLMs, which can be better solved by LLMs that\nhave learned complex instructions at the fine-tuning stage.\n\u2022Simply increasing the number of instructions may not be\nthat useful, and balancing the difficulty is not always helpful.\nAs the results shown in Table 9, balancing the difficulty\nand increasing the number of fine-tuning instructions are\nnot very helpful in our experiments. Especially for scaling\nthe instruction number, it even hurts the performance, e.g.,\na decrease from 29.81 to 26.63 in BBH for LLaMA (7B).\nIt shows that simply scaling the number of synthesized\ninstructions without quality control may not be effective to\nimprove the performance. Furthermore, fine-tuning with the\ninstructions of moderate difficulty also performs well in the\nchat setting, while slightly decreasing the performance in\nthe QA setting. A possible reason is that we filter complex\nand hard instructions with large perplexity scores, hurting\nthe model performance in answering complex questions.\n\u2022A larger model scale leads to a better instruction following\nperformance. By comparing the performance of LLaMA (7B)\nand LLaMA (13B) models fine-tuned with the same set\nof instruction data, we can see that LLaMA (13B) mostly\nachieves a better performance. It indicates that scaling the\nmodel size is helpful for improving the instruction following\ncapability. Besides, we can see that the QA performance has\nbeen improved a lot, e.g., from 38.11 to 47.49 in MMLU. It is\nlikely because that the larger models generally have better\nknowledge utilization and reasoning capability [33, 55],\nwhich can accurately answer more complex questions.\nInstruction Tuning Suggestions\nTo conduct instruction tuning on LLMs, one can\nprepare the computational resources according to\nthe basic statistics about the required number of\nGPUs and tuning time in Table 8. After setting\nup the development environment, we recommend\nbeginners to follow the code of Alpaca reposi-\ntory [137] for instruction tuning. Subsequently, one\nshould select the base model and construct the\ninstruction datasets as we discuss in this section.\nWhen computational resources for training are con-\nstrained, users can utilize LoRA for parameter-\nefficient tuning (see Section 5.3). As for inference,\nusers can further use quantization methods to de-\nploy LLMs on fewer or smaller GPUs (see Sec-\ntion 5.4).\n5.2 Alignment Tuning\nThis part first presents the background of alignment with\nits definition and criteria, then focuses on the collection\nof human feedback data for aligning LLMs, and finally38\ndiscusses the key technique of reinforcement learning from\nhuman feedback (RLHF) for alignment tuning.\n5.2.1 Background and Criteria for Alignment\nBackground. LLMs have shown remarkable capabilities\nin a wide range of NLP tasks [55, 56, 67, 90]. However,\nthese models may sometimes exhibit unintended behav-\niors, e.g., fabricating false information, pursuing inaccurate\nobjectives, and producing harmful, misleading, and biased\nexpressions [66, 366]. For LLMs, the language modeling\nobjective pre-trains the model parameters by word predic-\ntion while lacking the consideration of human values or\npreferences. To avert these unexpected behaviors, human\nalignment has been proposed to make LLMs act in line with\nhuman expectations [66, 367]. However, unlike the original\npre-training and adaptation tuning ( e.g., instruction tuning),\nsuch an alignment requires considering very different crite-\nria (e.g., helpfulness, honesty, and harmlessness). It has been\nshown that alignment might harm the general abilities of\nLLMs to some extent, which is called alignment tax in related\nliterature [368].\nAlignment Criteria. Recently, there is increasing attention\non developing multifarious criteria to regulate the behav-\niors of LLMs. Here, we take three representative alignment\ncriteria ( i.e.,helpful, honest, and harmless) as examples for\ndiscussion, which have been widely adopted in existing\nliterature [66, 368]. In addition, there are other alignment\ncriteria for LLMs from different perspectives including be-\nhavior, intent, incentive, and inner aspects [366], which\nare essentially similar (or at least with similar alignment\ntechniques) to the above three criteria. It is also feasible to\nmodify the three criteria according to specific needs, e.g.,\nsubstituting honesty with correctness [116]. Next, we give\nbrief explanations about the three representative alignment\ncriteria:\n\u2022Helpfulness. To be helpful, the LLM should demon-\nstrate a clear attempt to assist users in solving their tasks\nor answering questions in a concise and efficient manner\nas possible. At a higher level, when further clarification\nis needed, the LLM should demonstrate the capability of\neliciting additional relevant information through pertinent\ninquiries and exhibit suitable levels of sensitivity, percep-\ntiveness, and prudence [368]. Realizing the alignment of\nhelpful behavior is challenging for LLMs since it is difficult\nto precisely define and measure the intention of users [366].\n\u2022Honesty. At a basic level, a LLM aligned to be honest\nshould present accurate content to users instead of fabri-\ncating information. Additionally, it is crucial for the LLM\nto convey appropriate degrees of uncertainty in its output,\nin order to avoid any form of deception or misrepresen-\ntation of information. This requires the model to know\nabout its capabilities and levels of knowledge ( e.g., \u201cknow\nunknowns\u201d). According to the discussion in [368], honesty\nis a more objective criterion compared to helpfulness and\nharmlessness, hence honesty alignment could potentially be\ndeveloped with less reliance on human efforts.\n\u2022Harmlessness. To be harmless, it requires that the lan-\nguage produced by the model should not be offensive or\ndiscriminatory. To the best of its abilities, the model should\nbe capable of detecting covert endeavors aimed at solicitingrequests for malicious purposes. Ideally, when the model\nwas induced to conduct a dangerous action ( e.g., commit-\nting a crime), the LLM should politely refuse. Nonetheless,\nwhat behaviors are deemed harmful and to what extent vary\namongst individuals or societies [368] highly depend on\nwho is using the LLM, the type of the posed question, and\nthe context ( e.g., time) at which the LLM is being used.\nAs we can see, these criteria are quite subjective, and are\ndeveloped based on human cognition. Thus, it is difficult\nto directly formulate them as optimization objectives for\nLLMs. In existing work, there are many ways to fulfill these\ncriteria when aligning LLMs. A promising technique is red\nteaming [369], which involves using manual or automated\nmeans to probe LLMs in an adversarial way to generate\nharmful outputs and then updates LLMs to prevent such\noutputs.\n5.2.2 Collecting Human Feedback\nDuring the pre-training stage, LLMs are trained using the\nlanguage modeling objective on a large-scale corpus. How-\never, it cannot take into account the subjective and qualita-\ntive evaluations of LLM outputs by humans (called human\nfeedback in this survey). High-quality human feedback is\nextremely important for aligning LLMs with human pref-\nerences and values. In this part, we discuss how to select a\nteam of human labelers for feedback data collection.\nHuman Labeler Selection. In existing work, the dominant\nmethod for generating human feedback data is human\nannotation [66, 116, 367]. This highlights the critical role\nof selecting appropriate human labelers. To provide high-\nquality feedback, human labelers are supposed to have a\nqualified level of education and excellent proficiency in En-\nglish. For example, Sparrow [116] requires human labelers\nto be UK-based native English speakers who have obtained\nat least an undergraduate-level educational qualification.\nEven then, several studies [367] have found that there still\nexists a mismatch between the intentions of researchers\nand human labelers, which may lead to low-quality human\nfeedback and cause LLMs to produce unexpected output.\nTo address this issue, InstructGPT [66] further conducts a\nscreening process to filter labelers by assessing the agree-\nment between human labelers and researchers. Specifically,\nresearchers first label a small amount of data and then\nmeasure the agreement between themselves and human\nlabelers. The labelers with the highest agreement will be\nselected to proceed with the subsequent annotation work.\nIn some other work [370], \u201csuper raters\u201d are used to ensure\nthe high quality of human feedback. Researchers evaluate\nthe performance of human labelers and select a group of\nwell-performing human labelers ( e.g., high agreement) as\nsuper raters. The super raters will be given priority to\ncollaborate with the researchers in the subsequent study.\nWhen human labelers annotate the output of LLMs, it is\nhelpful to specify detailed instructions and provide instant\nguidance for human labelers, which can further regulate the\nannotation of labelers.\nHuman Feedback Collection. In existing work, there are\nmainly three kinds of approaches to collecting feedback and\npreference data from human labelers.39\n\u2022Ranking-based approach. In early work [367], human\nlabelers often evaluate model-generated outputs in a coarse-\ngrained manner ( i.e.,only selecting the best) without taking\ninto account more fine-grained alignment criteria. Nonethe-\nless, different labelers may hold diverse opinions on the\nselection of the best candidate output, and this method\ndisregards the unselected samples, which may lead to inac-\ncurate or incomplete human feedback. To address this issue,\nsubsequent studies [116] introduce the Elo rating system\nto derive the preference ranking by comparing candidate\noutputs. The ranking of outputs serves as the training signal\nthat guides the model to prefer certain outputs over others,\nthus inducing outputs that are more reliable and safer.\n\u2022Question-based approach. Further, human labelers can\nprovide more detailed feedback by answering certain ques-\ntions designed by researchers [81], covering the alignment\ncriteria as well as additional constraints for LLMs. Specially,\nin WebGPT [81], to assist the model in filtering and utiliz-\ning relevant information from retrieved documents, human\nlabelers are required to answer questions with multiple\noptions about whether the retrieved documents are useful\nfor answering the given input.\n\u2022Rule-based approach. Many studies also develop rule-\nbased methods to provide more detailed human feedback.\nAs a typical case, Sparrow [116] not only selects the response\nthat labelers consider the best but also uses a series of\nrules to test whether model-generated responses meet the\nalignment criteria of being helpful, correct, and harmless.\nIn this way, two kinds of human feedback data can be ob-\ntained: (1) the response preference feedback is obtained by\ncomparing the quality of model-generated output in pairs,\nand (2) the rule violation feedback is obtained by collecting\nthe assessment from human labelers ( i.e.,a score indicating\nto what extent the generated output has violated the rules).\nFurthermore, GPT-4 [46] utilizes a set of zero-shot classifiers\n(based on GPT-4 itself) as rule-based reward models, which\ncan automatically determine whether the model-generated\noutputs violate a set of human-written rules.\nIn the following, we focus on a well-known technique,\nreinforcement learning from human feedback (RLHF),\nwhich has been widely used in the recent powerful LLMs\nsuch as ChatGPT. As discussed below, the alignment criteria\nintroduced in Section 5.2.1 can be fulfilled by learning from\nhuman feedback on the responses of LLMs to users\u2019 queries.\n5.2.3 Reinforcement Learning from Human Feedback\nTo align LLMs with human values, reinforcement learning\nfrom human feedback (RLHF) [79, 367] has been proposed\nto fine-tune LLMs with the collected human feedback data,\nwhich is useful to improve the alignment criteria ( e.g.,\nhelpfulness, honesty, and harmlessness). RLHF employs\nreinforcement learning (RL) algorithms ( e.g., Proximal Pol-\nicy Optimization (PPO) [128]) to adapt LLMs to human\nfeedback by learning a reward model. Such an approach\nincorporates humans in the training loop for developing\nwell-aligned LLMs, as exemplified by InstructGPT [66].\nRLHF System. The RLHF system mainly comprises three\nkey components: a pre-trained LM to be aligned, a reward\nmodel learning from human feedback, and a RL algorithm\ntraining the LM. Specifically, the pre-trained LM is typically\nHuman  \nAnnotator\nDemonstration DataSupervised Fine-tuning\nReward Model Training\nRL Fine-tuning\nPrompts  \nLM Outputs  Training with RL algorithm (PPO)Ranking Training with feedback dataTraining with demonstration data\nPre-trained LM\ud83e\uddcaPre-trained LM\ud83d\udd25\nAligned LM\ud83d\udd25\nReward\ud83d\ude0a /\ud83d\ude1e Reward  \nModel\ud83d\udd25\n Reward  \nModel\ud83e\uddcaDemonstrations   Prompts  \nLM Outputs  Prompts  \nHuman FeedbackFig. 12: The workflow of the RLHF algorithm.\na generative model that is initialized with existing pre-\ntrained LM parameters. For example, OpenAI uses 175B\nGPT-3 for its first popular RLHF model, InstructGPT [66],\nand DeepMind uses the 280 billion parameter model Go-\npher [64] for its GopherCite model [370]. Further, the reward\nmodel (RM) provides (learned) guidance signals that reflect\nhuman preferences for the text generated by the LM, usually\nin the form of a scalar value. The reward model can take on\ntwo forms: a fine-tuned LM or a LM trained de novo using\nhuman preference data. Existing work typically employs\nreward models having a parameter scale different from that\nof the aligned LM [66, 370]. For example, OpenAI uses 6B\nGPT-3 and DeepMind uses 7B Gopher as the reward model,\nrespectively. Finally, to optimize the pre-trained LM using\nthe signal from the reward model, a specific RL algorithm\nis designed for large-scale model tuning. Specifically, Prox-\nimal Policy Optimization (PPO) [128] is a widely used RL\nalgorithm for alignment in existing work [66, 116, 370].\nKey Steps for RLHF. Figure 12 illustrates the overall three-\nstep process of RLHF [66] as introduced below.\n\u2022Supervised fine-tuning. To make the LM initially perform\ndesired behaviors, it usually needs to collect a supervised\ndataset containing input prompts (instruction) and desired\noutputs for fine-tuning the LM. These prompts and outputs\ncan be written by human labelers for some specific tasks\nwhile ensuring the diversity of tasks. For example, Instruct-\nGPT [66] asks human labelers to compose prompts ( e.g.,\n\u201cList five ideas for how to regain enthusiasm for my career \u201d) and\ndesired outputs for several generative tasks such as open\nQA, brainstorming, chatting, and rewriting. Note that the\nfirst step is optional in specific settings or scenarios.\n\u2022Reward model training. The second step is to train the\nRM using human feedback data. Specifically, we employ\nthe LM to generate a certain number of output texts using\nsampled prompts (from either the supervised dataset or\nthe human-generated prompt) as input. We then invite40\nLayer #1\nPrompt InputLayer # N\n\u2026\n(c) Prompt TuningInputAdapter Adapter MHA FFNAdapter Adapter MHA FFN\n\u2026\n(a) Adapter TuningLayer #1Prefix\nInputLayer # N\nPrefix\u2026\n(b) Prefix TuningLayer #1\nInputLayer # N\n\u2026\n(d) Low -Rank AdapationWdown\nWdown\nLoRA\nFig. 13: An illustration of four different parameter-efficient fine-tuning methods. MHA and FFN denote the multi-head\nattention and feed-forward networks in the Transformer layer, respectively.\nhuman labelers to annotate the preference for these pairs.\nThe annotation process can be conducted in multiple forms,\nand a common approach is to annotate by ranking the\ngenerated candidate texts, which can reduce the inconsis-\ntency among annotators. Then, the RM is trained to predict\nthe human-preferred output. In InstructGPT, labelers rank\nmodel-generated outputs from best to worst, and the RM\n(i.e.,6B GPT-3) is trained to predict the ranking. Note that, in\nrecent work [371], the annotation of preference on response\npairs has been conducted by an AI agent (usually an aligned\nLLM) instead of humans, which is called \u201c reinforcement\nlearning from AI feedback (RLAIF) \u201d. LLMs trained with typical\nRLHF algorithms tend to generate harmless responses with\nless helpfulness, which is called evasion problem [371]. To\nguarantee both the harmlessness and helpfulness, RLAIF\ngenerates the AI feedback based on pre-set alignment prin-\nciples in instructions [371, 372], which can also reduce the\nefforts of human annotation.\n\u2022RL fine-tuning. At this step, aligning ( i.e.,fine-tuning)\nthe LM is formalized as an RL problem. In this setting,\nthe pre-trained LM acts as the policy that takes as input\na prompt and returns an output text, the action space of\nit is the vocabulary, the state is the currently generated\ntoken sequence, and the reward is provided by the RM. To\navoid eviating significantly from the initial (before tuning)\nLM, a penalty term is commonly incorporated into the\nreward function. For example, InstructGPT optimizes the\nLM against the RM using the PPO algorithm. For each input\nprompt, InstructGPT calculates the KL divergence between\nthe generated results from the current LM and the initial\nLM as the penalty. It is noted that the second and final steps\ncan be iterated in multiple turns for better aligning LLMs.\nDue to the instability of the RL algorithm, recent work [373]\nreplaces the RL tuning with another supervised fine-tuning\nby reusing the best ranked samples with higher rewards.\nPractical Strategies for RLHF. Although RLHF is promising\nto effectively improve the alignment of LLMs with humans,\nit is practically challenging for researchers to successfully\nimplement it. In this part, we focus on discussing several\nuseful strategies and tricks for improving the effectiveness\nand efficiency of RLHF. Concretely, we focus on the effective\ntraining of reward models, efficient and effective RL train-\ning, respectively.\n\u2022Effective reward model training. Despite that InstructGPT\nused a small reward model (6B GPT model), increasing\nwork [99] has shown it is often more effective to use a\nlarge reward model ( e.g., equal or greater than the originalmodel size), since large reward models generally perform\nbetter in judging the quality of the LLM generated outputs.\nIn LLaMa 2 [99], pretrained chat model checkpoints are\nused to initialize the reward model, they argue that such an\napproach can effectively reduce the information mismatch\nbetween the model to be aligned and the reward model\nby sharing the same pre-training knowledge. Whereas, it is\ncommon to encounter the overfitting problem when train-\ning large-scale reward models. As a simple yet effective\nsolution, existing work [374, 375] has introduced the LM\nloss on the preferred response of the input prompt from\nthe human-annotated alignment dataset as a regularizer,\nwhich alleviates the overfitting of the reward model on the\nbinary classification task. In addition, as there are multiple\ncriteria for alignment ( e.g., helpfulness and honesty), it is\noften difficult to train a single reward model that can satisfy\nall the alignment criteria. Therefore, it is useful to train\nmultiple reward models that focus on different alignment\ncriteria [99], and compute the final reward based on the\nproduced ones from them via special combination strategies\n(e.g., mean pooling and weighted sum). Such a way enables\nmore flexible rules or standards on multiple criteria, e.g.,\nrelaxing the requirement on helpfulness while posing more\nstrict limits on harmfulness.\n\u2022Effective RL training. As the RL training process tends to\nbe unstable and hyper-parameter sensitive, it is suggested\nthat the language model should be well supervised fine-\ntuned before RL training, so as to reaching a good model\ncapacity. A commonly-used way is to fine-tune the LLM\non its best outputs of the prompts (referred to as rejec-\ntion sampling orbest-of- N) from the alignment dataset until\nconvergence before RL. Given a prompt, the LLM would\nfirst produce Noutputs via the sampling algorithm, and\nthen the best candidate from the model will be selected\nby the reward model for learning. After fine-tuning the\nLLM on the best samples until convergence, the RL process\nwill be performed to further improve the performance.\nLLaMA 2 [99] has successively trained five versions of RLHF\nmodels, where the LLM has been progressively improved\nwith the improvement of the reward models. In this way,\nthe collected prompts and annotations of human preference\ndata can better reflect the issues of the current model check-\npoint, thus making special tuning to address these issues. In\naddition, LLaMA 2 also adds samples from prior iterations\ninto the subsequent ones, to alleviate the possible capacity\nregression issue during iterative optimization.\n\u2022Efficient RL training. As the RL training requires to41\niterate the inference process of both the LLM and reward\nmodels, it would greatly increase the total memory and\ncomputation cost, especially for larger reward models and\nLLMs. As a practical trick, we can deploy the reward model\non a separate server, and invoke the corresponding API\nto work with the LLM on its own server. In addition, as\nRLHF requires the LLM to generate multiple candidate\noutputs, instead of calling the sample decoding procedure\nfor multiple times, it is more efficient to utilize the beam\nsearch decoding algorithm26. It only needs to perform one-\npass decoding for response generation, meanwhile such a\nstrategy can also enhance the diversity of the generated\ncandidate responses.\nProcess-Supervised RLHF. In existing literature of\nRLHF [376], the supervision signals for RL training can be\ngenerally classified into two distinct categories: outcome-\nsupervision signals and process-supervision signals. The\noutcome-supervised RLHF employs a quantitative score to\nassess the quality of the whole text generated by LLMs.\nIn contrast, process-supervised RLHF offers an evalua-\ntion of each individual component ( e.g., sentence, word,\nor reasoning step) within the generated content, which\ncan provide fine-grained supervision signals to guide the\ntraining, helping LLMs refine the undesired generation\ncontents [376, 377]. OpenAI has proposed a fine-grained\nannotation dataset named PRM800k [377] consisting of\n12K process-annotated mathematical problems ( i.e.,MATH\ndataset [378]) and 75K solutions generated by LLMs of\nthese problems, where each reasoning step of mathemat-\nical problems is labeled as positive ,negative orneutral in\nPRM800k. This fine-grained dataset has been utilized in\nexisting work [377, 379] to train the process-supervised re-\nward models (PRM), and the probability from the prediction\nof each label can be considered as the supervision signals\nduring RLHF procedure. To effectively leverage process-\nsupervision signals from PRMs, existing work [376] has\nutilized expert iteration [380, 381], an effective RL algo-\nrithm to improve the base policy via learning from expert\npolicy. Typically, expert iteration contains two main stages:\npolicy improvement and distillation [376]. In the policy\nimprovement stage, expert policy processes the systematic\nsearch procedure to produce the samples. PRMs provide\nprocess-supervision signals to guide expert policy in the\nsearch procedure and enhance the quality of samples. Subse-\nquently, during the distillation stage, the samples generated\nby expert policy in the first stage are utilized to improve\nthe base policy through supervised fine-tuning. In addition\nto expert iteration, PRMs can also be utilized to re-rank the\ncandidates of the final answers generated by LLMs [377] or\nto select better intermediate reasoning steps during step by\nstep reasoning [379, 382].\n5.2.4 Alignment without RLHF\nAlthough RLHF has achieved great success in aligning the\nbehaviors of LLMs with human values and preferences, it\nalso suffers from notable limitations. First, RLHF needs to\ntrain multiple LMs including the model being aligned, the\n26. https://huggingface.co/docs/transformers/v4.31.0/en/main\nclasses/text generation#transformers.GenerationMixin.group beam\nsearchreward model, and the reference model at the same time,\nwhich is tedious in algorithmic procedure and memory-\nconsuming in practice. Besides, the commonly-used PPO\nalgorithm in RLHF is rather complex and often sensitive\nto hyper-parameters. As an alternative, increasing studies\nexplore to directly optimize LLMs to adhere to human pref-\nerences, using supervised fine-tuning without reinforcement\nlearning [349].\nOverview. The basic idea of non-RL alignment approaches\nis to directly fine-tune LLMs with supervised learning on\nhigh-quality alignment dataset . It basically assumes that re-\nsponse feedback or golden rules to avert unsafe behaviors\nhave been injected or included in the specially curated align-\nment dataset, so that LLMs can directly learn aligned behav-\niors from these demonstration data via suitable fine-tuning\nstrategies. Thus, to implement this approach, two key issues\nare the construction of alignment dataset and the design of\nfine-tuning loss. For the first issue, the alignment dataset\ncan be automatically constructed by an aligned LLMs ac-\ncording to human-written safety principles [347] or refining\nexisting examples using edits operations [383]. In addition,\nwe can also reuse existing reward models to select high-\nrated responses from existing human feedback data [373].\nFor the second issue, non-RL alignment approaches mainly\nfine-tune LLMs in a supervised learning way (the same\nas the original instruction tuning loss) on a high-quality\nalignment dataset, meanwhile auxiliary learning objectives\ncan be used to enhance the alignment performance, e.g.,\nranking responses or contrasting instruction-response pairs.\nAlignment Data Collection. The construction of alignment\ndata is important to effectively align the behaviors of LLMs\nwith human preferences. To collect high-quality alignment\ndata, some work tries to reuse existing reward models to\nselect high-rated responses, and others explore to leverage\npowerful LLMs ( e.g., ChatGPT) or build a simulated envi-\nronment to generate synthetic alignment examples. Next,\nwe will discuss these three lines of research.\n\u2022Reward model based approaches. The reward model in\nRLHF has been trained to measure the alignment degree\non the responses of LLMs. It is straightforward to leverage\nexisting reward models to select high-quality responses as\nalignment data for subsequent fine-tuning. Based on this\nidea, RAFT [373] adopts reward models trained on human\npreference data to rank the responses of LLMs and collect\nthose with higher rewards for supervised fine-tuning. In\naddition, the reward model can be also used to score model\nresponses and assign them to different quality groups.\nQuark [384] sorts the responses of LLMs into different quan-\ntiles based on the reward scores. Each quantile is attached\nwith a special reward token to represent the reward level\nof the quantile. Conditioned on the highest-reward tokens,\nLLMs are subsequently prompted to generate high-quality\nresponses. Given an initial answer and the corresponding\nhuman feedback, ILF [385] first adopts LLMs to generate\nrefined answers, then utilizes the reward model to select\nthe answer that best matches the feedback for further\ntraining. As valuable resources for aligning LLMs, several\nreward models have been released, including DeBERTa-42\nbase/large/xxlarge from OpenAssistant27, Moss-7B from\nFudan28, and Flan-T5-xl from Stanford29.\n\u2022LLM based generative approaches. Reward models help\nto select aligned data from model responses. However,\ntraining reward models itself necessitates substantial high-\nquality human-labeled data, which is typically expensive\nand in short supply. In addition, although existing reward\nmodels can be reused, they might not be able to accurately\ncapture the nonalignment behaviors in another separately\ntrained LLM. Therefore, some work explores leveraging\npowerful LLMs to automatically generate human-aligned\ndata. As a representative work, constitutional AI [371] pro-\nposes that human supervision comes from a set of principles\n(i.e.,natural language instructions) governing AI behaviors.\nBased on these principles, LLMs will critique their own\nharmful responses and revise them repeatedly into finally\naligned responses. Similarly, Self-Align [347] first adopts\nself-instruct [143] to generate instructions focusing on cov-\nering diverse topics. Then, the model is also prompted\nwith multiple human-written principles that describe the\nrules of expected model behaviors (also with several in-\ncontext exemplars), to generate helpful, ethical, and reliable\nresponses as alignment data. To mitigate the limit that the\noriginal SFT method can only learn from positive responses,\nFIGA [386] develops an improved supervised alignment\napproach, where both negative (the original output of low\nquality) and positive (the refined output by LLMs) re-\nsponses are leveraged in a contrastive way, to enable LLMs\nto deeply understand what fine-grained revisions actually\nlead to good response.\n\u2022LLM based interactive approaches. Most existing ap-\nproaches train LLMs in isolation, where LLMs are not\npresent in actual environments to improve themselves\nthrough external feedback signals. As a comparison, hu-\nmans learn social norms and values from interactions with\nothers in social environments [387]. To mimic such a learn-\ning approach, Stable Alignment [179] builds a simulated\ninteraction environment consisting of a number of LLM\nagents, where AI agents keep interacting with and each\nother, receiving feedback on improvement. Once a central\nagent receives an instruction, it produces a response and\nshares it with nearby agents. These critic agents generate\nfeedback comprising ratings about the response and re-\nvision suggestions. Then the central agent would revise\nthe original response following these suggestions. Such\nan alignment approach can be also extended to real-world\nenvironment with humans.\nSupervised Alignment T uning. After obtaining alignment\ndata, it is also key to design suitable fine-tuning strategies\nfor direct alignment. A straightforward approach is to op-\ntimize LLMs using the conventional sequence-to-sequence\nobjective based on the alignment data. In addition to the\nconventional optimization objective, several studies further\nexplore auxiliary losses that enhance the learning from the\nalignment data.\n\u2022Primary training objective. Since the alignment data\ntypically consists of an input instruction and an output re-\n27. https://huggingface.co/OpenAssistant\n28. https://github.com/OpenLMLab/MOSS-RLHF\n29. https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xlsponse, the primary training loss is still the traditional cross-\nentropy loss for sequence-to-sequence learning. Based on\nthis loss, many studies propose a number of improvement\nvariants for enhancing the supervised alignment tuning.\nFor example, CoH [388] constructs the training data by\nprepending \u201c A helpful answer: \u201d and \u201c An unhelpful answer: \u201d\nto the annotated good and bad responses, respectively, and\nonly compute losses for those response tokens with special\nmasking. Quark [384] sorts model responses into different\nquantiles with varying alignment quality, it prepends a\nspecial reward token to each model response to represent\nthe reward level of the response. Further, to enable the\npreference modeling via the maximum likelihood objective,\nDPO [389] first reparameterizes the response rewards using\nthe policy model ( i.e.,the language model being optimized),\nand then the original reward modelling objective can be\nreformulated only based on the policy model. In this way,\nDPO removes the explicit reward modeling step, and opti-\nmizing the new learning objective only involving the policy\nmodel is equivalent to optimizing the rewards. Furthermore,\nFIGA [386] designs a fine-grained contrastive loss that aims\nto encourage desirable tokens, penalize undesirable ones,\nand disregard trivial tokens.\n\u2022Auxiliary optimization objectives. Besides the primary\ncross-entropy loss, several studies propose auxiliary train-\ning loss to enhance the learning from the alignment data.\nFirst, since the responses of each instruction can be scored\nby the reward model, the ranking loss can be used to train\nthe model to preserve the ranking order of these responses.\nFor example, RRHF [390] samples responses from multi-\nple sources, including model-generated responses, such as\nthose derived from the model itself, ChatGPT, and GPT-4,\nas well as human-written responses, spanning both high-\nquality and low-quality instances. To align with the scores\nfrom reward models, it further optimizes the ranking loss\nby encouraging the model to have a higher conditional log\nprobability for the response with a higher ranking. SLiC-\nHF [391] proposes to assess the similarity between model\noutputs and human preference via the distance in the latent\nspace, and introduces specific calibration and regularization\nloss to calibrate the candidate sequences based on human-\npreference data. Second, to enhance the relatedness be-\ntween the response and the instruction, some work adopts\ncontrastive learning to push up the probability of correct\ninstruction-response pairs while pushing down incorrect\ninstruction-response pairs. Specifically, for an output re-\nsponse, the proposed approach in [392] contrasts the target\ninstruction to the other irrelevant instructions. By doing so,\nit can enable the model to learn the right correlation between\ninstructions and responses.\n5.2.5 Remarks on SFT and RLHF\nAs discussed in Section 5.1, instruction tuning is the process\nof training pre-trained language models with formatted\ndemonstration data (instructions paired with desired out-\nputs). At early exploration, instruction data was mainly col-\nlected from NLP tasks [67], while it has been now extended\nto more diverse supervision data that pairs input and\noutput texts ( e.g., the utterances of open-ended dialogues).\nTraining with such paired texts is also called supervised fine-\ntuning (SFT) in the context of LLMs [66]. In this part, we43\nmainly use the abbreviation SFT for discussion but not\ninstruction tuning, due to the simplicity and popularity.\nSince SFT and RLHF are two major adaptation tuning\nmethods for LLMs, it is important to understand the con-\nnections and difference between them. Next, we make some\ndiscussions on this issue30.\nOverall Comparison with RL Formulation . Following the\ndiscussion in Section 5.2.3 (the part related to RL training),\nthe text generation problem can be formulated as a decision-\nmaking process based on RL. Taking a prompt as input,\nthe task of a LLM is to generate a text completion that\nappropriately responds to the prompt. This task would be\ncompleted step by step. At each step, an agent ( i.e.,LLM)\nwill perform an action ( i.e.,generating a token) according\nto the policy ( i.e.,the generative probability distribution of\nLLM) conditioned on the current state (currently generated\ntoken sequence and other available context information).\nIt is expected that a high-quality output text would be\nproduced by the LLM, which can earn a large reward score\nbased on the entire response. Overall, RLHF and SFT can be\nconsidered as two different training approaches to optimiz-\ning the above decision making process for LLMs. Specially,\nRLHF firstly learns the reward model, and then employs\nit to improve the LLM with RL training ( e.g., PPO). As a\ncomparison, SFT adopts a teacher-forcing approach, which\ndirectly optimizes the likelihood of a demonstration output.\nSuch a token-level training way essentially does behavior\ncloning (a special algorithm of imitation learning [393]): it\nutilizes the expert\u2019s action ( i.e.,the target token at each step)\nas the supervision label and directly learns to imitate the\ndemonstrations from experts without specifying a reward\nmodel as in typical RL algorithms. To learn the desired\npolicies, SFT adopts a \u201clocal\u201d optimization way ( i.e.,token-\nlevel loss) based on demonstration data, while RLHF takes a\n\u201cglobal\u201d optimization way ( i.e.,text-level loss) by involving\nhuman preference. More theoretical analysis about imitation\nlearning and reinforcement learning can be referred to the\nrelated RL literature [393, 394].\nPros and Cons of SFT . SFT has been shown to be an\neffective approach to boosting the performance of LLMs\non various benchmarks [67, 69, 137, 138], which can largely\nenhance the task generalization ability and flexibly endow\nspecific functions ( e.g., establishing the chatbot\u2019s identity).\nMore discussions about the usefulness of SFT can be found\nin Section 5.1.3. It has been widely recognized that SFT\nmainly unlocks the abilities but not inject new abilities into\nLLMs. Thus, it might become problematic when one tries\nto stimulate the non-endogenous abilities of LLMs via SFT.\nAs a concrete scenario, it would potentially advocate the\nhallucination behaviors when demonstration data is beyond\nthe knowledge or ability scope of LLMs, e.g., training a LLM\nto answer questions about its unknown facts. An interesting\nviewpoint from John Schulman\u2019s talk on RLHF [395] is that\ndistilling superior models to train less capable models ( e.g.,\nprompting GPT-4 to generate the response as fine-tuning\ndata) might increase the possibilities of generating the hal-\n30. This part would be somehow subjective, mainly based on the au-\nthors\u2019 opinions and experiences. Comments or corrections are welcome\nto enhance this part.lucinated texts, thus likely affecting the factual accuracy\nof LLMs. Furthermore, as a behavior cloning method, SFT\naims to imitate the behaviors (without explorations) of the\nexperts who construct the demonstration data. However,\nthere often exist variations among different annotators on\nthe writing styles, quality, and preferences of demonstration\ndata, which tends to affect the learning performance of SFT.\nThus, high-quality instruction data (but not the quantity) is\nthe primary factor for effective training of LLMs during the\nSFT stage [99].\nPros and Cons of RLHF . RLHF was early explored in the\nliterature of deep RL [79], then borrowed to improve the\ncapacity of language models ( e.g., summarization [129]),\nand subsequently adopted as the fundamental technique to\ndevelop InstructGPT [66]. Recently, increasing evidence [99,\n371] has demonstrated the effectiveness of RLHF in miti-\ngating the harmful responses and enhancing the model ca-\npacity. Specially, LLaMA 2 has demonstrated that RLHF can\nimprove both the helpfulness and harmlessness scores [99],\nand attributed this to a better human-LLM synergy for data\nannotation. They explain this reason in two major aspects\nas follows. First, since human annotators mainly provide\npreference annotations for RLHF, it can largely alleviate the\ndiscrepancies of annotators as that in SFT. Secondly, pref-\nerence annotation is much easier than writing the demon-\nstration data, and annotators can even judge the quality of\nmore superior generations than those they create, making it\npossible to explore a broader state space beyond what can\nbe demonstrated by human annotators. Another key point\nis that RLHF essentially encourages LLMs to learn correct\npolicies by contrasting the self-generated responses (dis-\ncriminating between good and bad responses). It no longer\nforces the model to imitate external demonstration data,\nand thus can mitigate the hallucination issues with SFT as\ndiscussed above31. Actually, RLHF has been demonstrated\nto be an important approach to reduce the hallucination\nbehaviors in GPT-4 [46]. However, RLHF inherits the draw-\nbacks of classic RL algorithms, e.g., sample inefficiency and\ntraining instability. When adapted to LLMs, RLHF further\nrelies on a strong SFT model as initial model checkpoint for\nefficiently achieving good performance. In addition, human\nannotators are involved in a complex iterative optimization\nprocess, in which a number of important details ( e.g., the\nprompt selection, the schedule of reward model training and\nPPO training, and the settings of hyper-parameters) have\nimportant impact on the whole model performance.\nOverall, SFT is particularly useful to increase the model\ncapacity of pre-trained model checkpoints right after pre-\ntraining, while RLHF is promising to further improve the\nmodel capacity of SFT models. However, RLHF has been\ndifficult to implement, and far from well explored (ac-\ncording to public literature), and more improvements ( e.g.,\nefficient and reliable annotation [371] and simplified opti-\nmization [389]) are still needed for further research.\n31. In RLHF, it seems to be also important that reward models\nshould be aware of the knowledge or ability of a LLM to be aligned.\nFor example, LLaMA 2 adopts pre-trained chat model checkpoints to\ninitialize reward models [99].44\n5.3 Parameter-Efficient Model Adaptation\nIn the above, we have discussed the approaches of instruc-\ntion tuning and alignment tuning to adapt LLMs according\nto specific goals. Since LLMs consist of a huge amount of\nmodel parameters, it would be costly to perform the full-\nparameter tuning. In this section, we will discuss how to\nconduct efficient tuning on LLMs. We first review several\nrepresentative parameter-efficient fine-tuning methods for\nTransformer language models, and then summarize existing\nwork on parameter-efficient fine-tuned LLMs.\n5.3.1 Parameter-Efficient Fine-Tuning Methods\nIn existing literature, parameter-efficient fine-tuning [145,\n396, 397] has been an important topic that aims to reduce\nthe number of trainable parameters while retaining a good\nperformance as possible. In what follows, we briefly re-\nview four parameter-efficient fine-tuning methods for Trans-\nformer language models, including adapter tuning, prefix\ntuning, prompt tuning and LoRA. The illustration of these\nfour methods are shown in Figure 13.\nAdapter T uning . Adapter tuning incorporates small neural\nnetwork modules (called adapter ) into the Transformer mod-\nels [398]. To implement the adapter module, a bottleneck\narchitecture has been proposed in [398, 399], which first\ncompresses the original feature vector into a smaller di-\nmension (followed by a nonlinear transformation) and then\nrecovers it to the original dimension. The adapter modules\nwould be integrated into each Transformer layer, typically\nusing a serial insertion after each of the two core parts ( i.e.,\nattention layer and feed-forward layer) of a Transformer\nlayer. Alternatively, parallel adapters [400] can be also used\nin Transformer layers, where it places two adapter modules\nin parallel with the attention layer and feed-forward layer\naccordingly. During fine-tuning, the adapter modules would\nbe optimized according to the specific task goals, while the\nparameters of the original language model are frozen in this\nprocess. In this way, we can effectively reduce the number\nof trainable parameters during fine-tuning.\nPrefix T uning . Prefix tuning [396] prepends a sequence of\nprefixes, which are a set of trainable continuous vectors, to\neach Transformer layer in language models. These prefix\nvectors are task-specific, which can be considered as virtual\ntoken embeddings. To optimize the prefix vectors, a repa-\nrameterization trick [396] has been proposed by learning a\nMLP function that maps a smaller matrix to the parameter\nmatrix of prefixes, instead of directly optimizing the pre-\nfixes. It has been shown that this trick is useful for stable\ntraining. After optimization, the mapping function would\nbe discarded, and only the derived prefix vectors are kept\nto enhance task-specific performance. Since only the prefix\nparameters would be trained, it can lead to a parameter-\nefficient model optimization. Similar to prefix tuning, p-\ntuning v2 [401] incorporates layer-wise prompt vectors into\nthe Transformer architecture specially for natural language\nunderstanding, which also utilizes multi-task learning for\njointly optimizing shared prompts. It has been shown to\nbe useful in improving the model performance of different\nparameter scales on natural language understanding tasks.Prompt T uning . Different from prefix tuning, prompt tun-\ning [397, 402] mainly focuses on incorporating trainable\nprompt vectors at the input layer32. Based on the discrete\nprompting methods [404, 405], it augments the input text\nby including a group of soft prompt tokens (either in a\nfree form [402] or a prefix form [397]), and then takes\nthe prompt-augmented input to solve specific downstream\ntasks. In implementation, task-specific prompt embeddings\nare combined with the input text embeddings, which are\nsubsequently fed into language models. P-tuning [402] has\nproposed a free form to combine the context, prompt and\ntarget tokens, which can be applied to the architectures for\nboth natural language understanding and generation. They\nfurther learn the representations of soft prompt tokens by a\nbidirectional LSTM. Another representative approach [397]\nnamed prompt tuning directly prepends prefix prompts to\nthe input. During training, only the prompt embeddings\nwould be learned according to task-specific supervisions.\nSince this method only includes a small number of trainable\nparameters at the input layer, it has been found that the\nperformance highly relies on the model capacity of the\nunderlying language models [397].\nLow-Rank Adaptation (LoRA) . LoRA [145] imposes the\nlow-rank constraint for approximating the update matrix at\neach dense layer, so as to reduce the trainable parameters\nfor adapting to downstream tasks. Consider the case of\noptimizing a parameter matrix W. The update process can\nbe written in a general form as: W\u2190W+ \u2206W. The basic\nidea of LoRA is to freeze the original matrix W\u2208Rm\u00d7n\nwhile approximating the parameter update \u2206Wby low-\nrank decomposition matrices, i.e.,\u2206W=A\u00b7B\u22a4, where\nA\u2208Rm\u00d7kandB\u2208Rn\u00d7kare the trainable parameters for\ntask adaptation and k\u226amin(m, n)is the reduced rank. The\nmajor merit of LoRA is that it can largely save the memory\nand storage usage ( e.g., VRAM). Further, one can only keep\na single large model copy, while maintaining a number of\ntask-specific low-rank decomposition matrices for adapting\nto different downstream tasks. Further, several studies have\nalso discussed how to set the rank in a more principled\napproach, e.g., importance score based allocation [406] and\nsearch-free optimal rank selection [407].\nBesides the above methods, there is extensive research\non efficient tuning of Transformer language models. How-\never, a more comprehensive discussion of efficient tuning is\nbeyond the scope of this article, which can be found in the\nrelated papers on this topic [400, 408].\n5.3.2 Parameter-Efficient Fine-Tuning on LLMs\nWith the rising of LLMs, efficient tuning has attracted\nincreasing research attention for developing a more\nlightweight adaptation approach in downstream tasks.\nIn particular, LoRA [145] has been widely applied\nto open-source LLMs ( e.g., LLaMA and BLOOM) for\n32. Here, prompt tuning denotes a category of related efficient tuning\nmethods exemplified by the work [397, 402, 403], instead of a spe-\ncific method as used in [397]. Indeed, the prefix based tuning meth-\nods [396, 401] can be also considered as prompting methods, which\nare called deep prompting tuning in [401]. In this survey, prompt tuning\nspecially refer to the methods that only include the prompt tokens at\nthe input layer, in the context of LLMs. We assign p-tuning v2 [401] to\nthe category of prefix tuning, because it incorporates layerwise prompts\nin langauge models.45\nparameter-efficient fine-tuning. Among these research at-\ntempts, LLaMA and its variants have gained much atten-\ntion for parameter-efficient tuning. For example, Alpaca-\nLoRA [144] has been trained using LoRA as a lightweight\ntuned version of Alpaca [142] (a fine-tuned 7B LLaMA\nmodel with 52K human demonstrations of instruction fol-\nlowing). There are extensive explorations of Alpaca-LoRA\nranging in different languages or model sizes, which can\nbe found in the collection page33. A recent study LLaMA-\nAdapter [409] inserts learnable prompt vectors into each\nTransformer layer, in which zero-initialized attention has\nbeen proposed to improve the training by mitigating the\ninfluence of under-fitted prompt vectors. They also extend\nthis approach to a multi-modal setting, e.g., visual question\nanswering.\nFurther, an empirical study [399] has been conducted\nto examine the effect of different tuning methods on lan-\nguage models. They compare four efficient tuning methods\nincluding serial adapter tuning [398], parallel adapter tun-\ning [400, 410], and LoRA [145], on three open-source LLMs,\nnamely GPT-J (6B), BLOOM (7.1B) and LLaMA (7B), for\nevaluation. Based on the experimental results on six math\nreasoning datasets, they show that these efficient-tuning\nmethods under-perform the reference baseline GPT-3.5 on\ndifficult tasks, while achieving a comparable performance\non simple tasks. Overall, LoRA performs relatively well\namong these comparison methods, using significantly fewer\ntrainable parameters.\nAs an important resource, the library PEFT [411] (stand-\ning for parameter-efficient fine-tuning) has been released on\nGitHub34. It has included several widely used efficient tun-\ning methods, including LoRA [145]/AdaLoRA [406], prefix-\ntuning [396, 401], P-Tuning [402], and prompt-tuning [397].\nFurther, it supports a number of language models such as\nGPT-2 and LLaMA, and also covers several representative\nvision Transformer models ( e.g.,ViT and Swin Transformer).\nAs discussed in Section 5.3.1, there have been a large\nnumber of efficient tuning methods proposed in the existing\nliterature. However, most of these approaches are tested\non small-sized pre-trained language models, instead of the\nLLMs. So far, there still lacks a thorough investigation on\nthe effect of different efficient tuning methods on large-sized\nlanguage models at different settings or tasks.\n5.4 Memory-Efficient Model Adaptation\nDue to the huge number of model parameters, LLMs take a\nsignificant memory footprint for inference, making it very\ncostly to be deployed in real-world applications. In this\nsection, we discuss how to reduce the memory footprint\nof LLMs via a popular model compression approach ( i.e.,\nmodel quantization), so that large-sized LLMs can be used\nin resource-limited settings, which also likely reduces the\ninference latency.\n5.4.1 Background for Quantization\nIn this part, we present a general introduction of quantiza-\ntion techniques for neural networks.\n33. https://github.com/tloen/alpaca-lora\n34. https://github.com/huggingface/peftIn neural network compression, quantization often refers\nto the mapping process from floating-point numbers to\nintegers [412], especially the 8-bit integer quantization ( i.e.,\nINT8 quantization ). For neural network models, there are\ntypically two kinds of data to be quantized, namely weights\n(model parameters) and activations (hidden activations),\nwhich are originally represented in floating-point num-\nbers. To illustrate the essential idea of model quantization,\nwe introduce a simple yet popular quantization function:\nxq=R(x/S)\u2212Z, which transforms a floating number xinto\na quantized value xq. In this function, SandZdenote the\nscaling factor (involving two parameters \u03b1and\u03b2that deter-\nmine the clipping range) and zero-point factor (determining\nsymmetric or asymmetric quantization), respectively, and\nR(\u00b7)denotes the rounding operation that maps a scaled\nfloating value to an approximate integer.\nAs the reverse process, dequantization recovers the orig-\ninal value from the quantized value accordingly: \u02dcx=\nS\u00b7(xq+Z). The quantization error is calculated as the\nnumerical difference between the original value xand the\nrecovered value \u02dcx. The range parameters \u03b1and\u03b2have a\nlarge impact on the quantization performance, which often\nneed to be calibrated according to real data distributions, in\neither a static (offline) or dynamic way (runtime).\nFor more details, we refer to the readers to the excel-\nlent survey [412] about quantization methods on neural\nnetworks.\n5.4.2 Quantization Methods for LLMs\nThere are generally two major model quantization ap-\nproaches, namely quantization-aware training (QAT) (requir-\ning additional full model retraining) and post-training quanti-\nzation (PTQ) (requires no model retraining). Compared with\nsmall-sized language models, two major differences need\nto be considered when designing or selecting quantization\nmethods for LLMs. Firstly, LLMs consist of a huge number\nof parameters, and thus PTQ methods are more preferred\ndue to a much lower computational cost than QAT methods.\nSecondly, LLMs exhibit very different activation patterns\n(i.e.,large outlier features), and it becomes more difficult\nto quantize LLMs, especially hidden activations. Next, we\nwill briefly review several representative PTQ methods35for\nLLMs.\nPost-Training Quantization (PTQ) . We first introduce the\nPTQ methods for LLMs.\n\u2022Mixed-precision decomposition . As observed in [413],\nextreme large values occur in hidden activations (called\nthe emergence of outliers ) when the model size reaches 6.7B\nparameters or above. Interestingly, these outliers are mainly\ndistributed in some specific feature dimensions at Trans-\nformer layers. Based on this finding, a vector-wise quan-\ntization approach, called LLM.int8() , has been proposed in\n[413], which separates the feature dimensions with outliers\nand the rest dimensions in matrix multiplication. Then,\nthe calculations for the two parts are performed with 16-\nbit floating numbers and 8-bit integers , respectively, so as to\nrecover these outliers in a high precision.\n35. Since we mainly focus on discussing quantization methods in the\ncontext of LLMs, the line of quantization work on small-sized language\nmodels ( e.g., BERT) has not been included in this survey.46\n\u2022Fine-grained quantization . For Transformer models,\nweights and activations are usually represented in the\nform of tensors. A straightforward approach is to use\ncoarse-grained quantization parameters for the whole ten-\nsor ( i.e., per-tensor quantization) [414]. However, it usu-\nally leads to inaccurate reconstruction results. Thus, fine-\ngrained methods are proposed to reduce the quantization\nerror. ZeroQuant [415] adopts a token-wise quantization\napproach with dynamic calibration for compressing acti-\nvations. Whereas for weights (easier to be quantized), it\nuses a group-wise quantization. In practice, a group size\nof 128 [415, 416] is commonly used for model quantization.\n\u2022Balancing the quantization difficulty . Considering that\nweights are easier to be quantized than activations,\nSmoothQuant [414] proposes to migrate the difficulty from\nactivations to weights. Specially, they incorporate a scaling\ntransformation to balance the difficulty between weights\nand activations in a linear layer: Y= (Xdiag(s)\u22121)\u00b7\n(diag(s)W). By introducing an mathematically equivalent\ntransformation, this formula controls the quantization diffi-\nculty through the scaling factor s. To set s, it incorporates\na migration strength parameter \u03b1to balance the difficulties,\nwhere each entry sj= max( xj)\u03b1/max(wj)(1\u2212\u03b1)is deter-\nmined by the migration strength.\n\u2022Layerwise quantization . This approach finds optimal\nquantized weights that minimize a layerwise reconstruction\nloss:arg min cW\u2225WX\u2212cWX\u22252\n2. To efficiently optimize this\nobjective, GPTQ [417] improves the original optimal brain\nquantization (OBQ) [418] method by fixing the quantiza-\ntion order of weights for all rows. Further, with specially\ndesigned methods ( i.e., lazy batch-updates and Cholesky\nreformulation), GPTQ is feasible to quantize very large\nmodels ( e.g.,175B OPT) in 3 or 4 bit precision. More recently,\nAWQ [416] further simplifies the optimization form by\nincorporating activation-aware scaling for weights, which\nresembles the idea of SmoothQuant [414]: weights corre-\nsponding to outlier activations are more important to be\nprecisely quantized. It does not directly optimize the recon-\nstruction loss, but instead performs simple hyper-parameter\nsearch to achieve the minimal loss on calibration data.\nThese strategies in the above methods can be jointly\nused to improve the quantization performance. In order to\nachieve high-efficiency implementation, quantization meth-\nods also rely on hardware- or system-level support ( e.g., ef-\nficient GPU kernels or hardware-friendly group partition).\nOther Quantization Methods . In the above, we mainly fo-\ncus on PTQ methods, and next introduce two recent studies\nthat explore efficient fine-tuning methods or QAT methods\nfor quanitizing LLMs.\n\u2022Efficient fine-tuning enhanced quantization. For post-\ntraining quantization, direct low-bit quantization ( e.g., INT4\nquantization) often results in large performance degrada-\ntion. To overcome this challenge, QLoRA [419] incorporates\nadditional small tunable adapters (16-bit precision) into the\nquantized models, to achieve an efficient, high-precision\nmodel fine-tuning. It combines the merits of LoRA (See\nSection 5.3.1) and quantization methods. The experiment\nresults show that 4-bit quantized models can achieve the\nfull 16-bit fine-tuning performance by QLoRA.\n\u2022Quantization-aware training (QAT) for LLMs . A recentstudy [420] explores the effect of QAT methods by applying\na data-free distillation method to compress the weights,\nactivations as well as key-value cache. By conducting exten-\nsive experiments based on LLaMA, they show promising\nresults with 4-bit quantization on both weights and key-\nvalue cache, but not on 4-bit activation quantization, which\nstill needs more exploration.\n5.4.3 Empirical Analysis and Findings\nQuantization has currently become a common technique\nto reduce the memory footprint and latency of LLMs in\ndeployment. In particular, it is important to understand\nwhat level of precision ( e.g., INT8 or INT4) can be applied\nto quantize different parts of LLMs ( e.g., weights or acti-\nvations), while retaining a high accuracy. In this part, we\nfirst summarize the major findings about the quantization of\nLLMs in existing literature, and then present some empirical\nanalysis with quantization experiments.\nImportant Findings from Existing Work . Recently, a very\ncomprehensive evaluation [421] has been conducted about\nthe impact of multiple factors ( e.g., model size and sensi-\ntivity) on the post-training quantization methods. Another\nstudy [422] examines the scaling law of k-bit quantiza-\ntion in inference performance. In addition to the overall\nperformance, the study [423] specifically focuses on the\npotential impact of quantification on emergent capabilities,\nas well as the levels of performance that can be achieved\nacross various levels of bit precision. Also, prior work ( e.g.,\nLLM.int8() [424], GPTQ [417], QLoRA [419], and GLM [93])\nhas also extensively examined the performance of quanti-\nzation methods in various settings. Next, we summarize\nseveral important findings from these studies, which will\nbe useful for those who may not want to delve into the\ntechnical details of quantization methods.\n\u2022INT8 weight quantization can often yield very good re-\nsults on LLMs, while the performance of lower precision weight\nquantization depends on specific methods [414, 416, 417, 421]. In\nmost cases, INT8 weight quantization can be effectively ap-\nplied to reduce the memory footprint without performance\ndegradation. While for INT4 (or INT3) weight quantization,\nexisting methods rely on specific strategies to reduce the\nperformance degradation, e.g., layerwise method [415, 417],\nactivation-aware scaling [416] and low-rank adapter tun-\ning [419]. Interestingly, LLMs seem to be less sensitive\nto low-bit weight quantization than small-sized language\nmodels [421]. In practice, with the same memory cost, it\nis suggested to use a larger language model with a lower\nquantization precision rather than a smaller language model\nwith a higher quantization precision. For example, a 4-bit\n60GB LLM is demonstrated to have better performance than\na 8-bit 30GB LLM [422]. Moreover, focusing on emergent\ncapabilities, the study [423] finds that in-context learning,\nstep-by-step reasoning, and instruction following all seem\nto be seldom affected with 4-bit weight quantization. This\nresult suggests that INT4 quantization exhibits a favorable\ntrade-off in terms of both total bits and performance of\nemergent abilities.\n\u2022Activations are more difficult to be quantized than\nweights [413, 414, 421]. It has been found that large outliers\nwould occur for Transformer language models having a47\nsize of 6.7B or above [413]. This issue has been one of\nthe most fundamental difficulties to quantize LLMs. To\novercome this issue, various methods, e.g., mixed-precision\ndecomposition [413], fine-grained quantization [413, 425]\nand difficulty migration [414], can be applied to alleviate the\ninfluence of outlier values. Since large outliers mainly exist\nin the activations of LLMs, small language models are more\nresistant to activation quantization [421, 423]. In practice,\nhigh-quality INT8 activation quantization is still a difficult\ntask, though several methods can attain satisfying results.\nFurther, lower precision activation quantization has still not\nbeen successfully explored, even for QAT methods [420].\n\u2022Efficient fine-tuning enhanced quantization is a good op-\ntion to enhance the performance of quantized LLMs [145, 419].\nThe benefits of efficient fune-tuning methods in quanti-\nzation can be twofold. Firstly, it can directly compensate\nthe performance degradation suffered from low-bit quan-\ntization [421, 423], by increasing the fitting capacity by\nupdating high precision adapters. Secondly, it is flexible to\nsupport task-specific or goal-specific fine-tuning of LLMs\nin a lightweight way [419], e.g., instruction tuning or chat-\noriented tuning, by only tuning the small adapters. Overall,\nit makes a good trade-off between the effectiveness and\ntraining cost, which provides a promising approach to en-\nhancing the performance of quantized LLMs.\nEmpirical Analysis on Quantization Experiments . To fur-\nther help readers understand the impact of quantization on\nLLMs, we also conduct a group of experiments to investi-\ngate the inference performance of quantized models here.\nSpecifically, we focus on the fine-tuned LLaMA models ( i.e.,\n7B and 13B) using popular SFT datasets, including FLAN-\nv2 [69], Alpaca-52K [137] and ShareGPT [148]. For evalua-\ntion, we utilize the same tasks in Table 9, and follow the\nquantization settings in the study [423] examining the per-\nformance of quantized language models at three precision\nlevels: 4-bit, 8-bit and 16-bit. The results are summarized\nin Table 10. As can be observed from Table 10, the results\nobtained with 8-bit and 4-bit weight quantization are close\nto the performance of 16-bit models while significantly\nreducing memory consumption. In practice, it is recom-\nmended to first examine the performance of 4-bit weight\nquantization for LLMs if reducing memory usage is a critical\nconsideration for deployment.\n5.4.4 Open-source Libraries and Quantized LLMs\nIn this part, we briefly introduce the available open-source\nquantization libraries and quantized LLMs.\nQuantization Libraries . Next, we introduce three major\nquantization libraries for LLMs, including:\n\u2022Bitsandbytes36is developed based on the methods intro-\nduced in the papers of LLM.int8() [413] and 8-bit optimiz-\ners [426]. It focuses on the quantization of both activations\nand weights for LLMs, including the support on 8-bit and\n4-bit (NF4,FP4) matrix multiplication for efficient inference,\nas well as an 8-bit optimizer for efficient training.\n\u2022GPTQ-for-LLaMA37is developed specially for quantiz-\ning LLaMA models. It enables 4-bit quantization of LLaMA\n36. https://github.com/TimDettmers/bitsandbytes\n37. https://github.com/qwopqwop200/GPTQ-for-LLaMamodels of varied sizes based on the GPTQ algorithm [417].\nAlso, it provides a comparison with bitsandbytes in both\nmemory and performance (PPL) on the project website.\n\u2022AutoGPTQ38is a quantization package developed\nbased on the GPTQ algorithm [417], which supports INT4\nquantization for LLMs. It includes a number of quantized\nmodels in the library, and supports LoRA by integrating\nwith HuggingFace PEFT library.\n\u2022llama.cpp39makes it feasible to run quantized LLaMA\nmodels on a MacBook device. It supports INT4, INT5 and\nINT8 quantization, which is developed in efficient C/C++\nimplementation. It also supports a number of LLaMA based\nmodels, such as Alpaca and Vicuna.\nQuantized LLMs . Compared with original models, quan-\ntized language models take a smaller memory footprint,\nand likely have a faster inference speed [93, 413, 427].\nRecently, a nubmer of quantized model copies of several\npublicly available language models have been released on\nHuggingFace, including BLOOM, GPT-J, and ChatGLM. In\nparticular, GPTQ [417] has been widely used to quantize\ngenerative language models, leading to various quantized\nvariants for LLaMA and OPT. Further, it has been also\napplied to quantize instruction-tuned models, such as Vi-\ncuna and WizardLM. Due to the large number of quantized\nLLMs, we do not directly incorporate the corresponding\nlinks of these models. The readers can easily find them by\nsearching on HuggingFace.\n6 U TILIZATION\nAfter pre-training or adaptation tuning, a major approach\nto using LLMs is to design suitable prompting strategies\nfor solving various tasks. In existing literature, task-specific\nprompts can be effectively learned through manual creation\nand automatic optimization. A representative prompting\nmethod is in-context learning [50, 55], which formulates the\ntask description and/or demonstrations in the form of natu-\nral language text. In addition, chain-of-thought prompting [33]\ncan be employed to enhance in-context learning by involv-\ning a series of intermediate reasoning steps in prompts.\nFurthermore, planning [439] is proposed for solving complex\ntasks, which first breaks them down into smaller sub-tasks\nand then generates a plan of action to solve these sub-tasks\none by one. We summarize representative work for these\nprompting approaches in Table 11. Next, we will elaborate\non the details of the four techniques.\n6.1 Prompting\nAs discussed in previous work [36], prompting is the major\napproach to utilizing LLMs for solving various tasks. Since\nthe quality of prompts will largely influence the perfor-\nmance of LLMs in specific tasks, there have been a series of\nstudies proposed to generate suitable task prompts through\nmanual creation or automatic optimization, which will be\nintroduced in this section.\n38. https://github.com/PanQiWei/AutoGPTQ\n39. https://github.com/ggerganov/llama.cpp48\nTABLE 10: Evaluation results for quantized LLaMA models (7B and 13B). We employ existing model checkpoints provided\nby [353] for quantization experiments, which have been fine-tuned on FLAN-v2, Alpaca-52K, and ShareGPT, respectively.\nSpecifically, we report the performance with AlpacaFarm, MMLU, and BBH, as well as the memory usage of the loaded\nmodel (Mem.). For quantization, we employ bitesandbytes to quantize the 16-bit models to 8/4 bits by specifying the\ncommands load_in_8bit andload_in_4bit when loading the weights. It is worth noting that we select text-davinci-\n003as the baseline model for the AlpacaFarm dataset.\nModels SFT Dataset16-bit 8-bit 4-bit\nAlpacaFarm MMLU BBH Mem. (GiB) AlpacaFarm MMLU BBH Mem. (GiB) AlpacaFarm MMLU BBH Mem. (GiB)\nLLaMA (7B) FLAN-v2 6.65 47.34 35.05 12.58 6.15 47.02 35.17 6.65 7.83 46.23 34.77 3.94\nAlpaca-52K 32.55 40.87 33.66 12.58 33.60 39.98 34.38 6.65 29.57 39.24 32.80 3.94\nShareGPT 72.05 41.30 32.90 12.58 72.86 39.34 32.71 6.65 70.31 40.08 32.11 3.94\nLLaMA (13B) FLAN-v2 8.14 51.67 41.46 24.40 7.64 51.02 41.25 12.53 7.52 50.48 40.68 7.34\nAlpaca-52K 33.60 47.63 36.10 24.40 31.43 47.04 35.98 12.53 30.87 46.20 36.16 7.34\nShareGPT 75.59 47.58 38.00 24.40 73.79 47.71 38.31 12.53 71.99 45.77 36.97 7.34\nTABLE 11: Typical LLM utilization methods and their key points for ICL, CoT, and planning. Note that the key points only\nhighlight the most important technical contribution.\nApproach Representative Work Key Point\nIn-context\nLearning (ICL)KATE [428] Demonstration selection (similar; k-NN)\nEPR [429] Demonstration selection (dense retrieval; constrative learning)\nSG-ICL [430] Demonstration selection (LLM as the demonstration generator)\nAPE [431] Demonstration format (automatic generation & selection)\nStructured Prompting [296] Demonstration format (grouped context encoding; rescaled attention)\nGlobalE & LocalE [432] Demonstration order (entropy-based metric; probing set generation with LLM)\nChain-of-thought\nPrompting (CoT)Complex CoT [433] Demonstration (complexity-based selection)\nAuto-CoT [434] Demonstration (automatic generation)\nSelection-Inference [435] Generation (alternate between selection and inference)\nSelf-consistency [436] Generation (diverse paths; self-ensemble)\nDIVERSE [437] Generation (diverse paths); Verification (step-wise voting)\nRationale-augmented ensembles [438] Generation (rationale sampling)\nPlanningLeast-to-most prompting [439] Plan generation (text-based; problem decomposition)\nDECOMP [440] Plan generation (text-based; problem decomposition)\nPS [441] Plan generation (text-based)\nFaithful CoT [442] Plan generation (code-based)\nPAL [443] Plan generation (code-based; Python)\nHuggingGPT [444] Plan generation (code-based; models from HuggingFace)\nAdaPlanner [445] Plan refinement (skill memory)\nTIP [446] Feedback acquisition (visual perception)\nRAP [447] Feedback acquisition (LLM as the world model); Plan refinement (Monte Carlo Tree Search)\nChatCoT [448] Feedback acquisition (tool); Plan refinement (conversation between LLM and tools)\nReAct [449] Feedback acquisition (tool); Plan refinement (synergizing reasoning and acting)\nReflexion [450] Feedback acquisition (text-based self-reflection); Plan refinement (dynamic memory)\nTree of Thoughts [451] Feedback acquisition (vote comparison); Plan refinement (tree-based search)\n6.1.1 Prompt Creation\nThe process of manually creating a suitable prompt is also\ncalled prompt engineering [452, 453]. A well-designed prompt\nis very helpful to elicit the abilities of LLMs for accomplish-\ning specific tasks. In this part, we will first introduce the\nkey components of prompts and discuss several principles\nfor prompt design. Then, we evaluate ChatGPT with differ-\nent prompts to show the results on several representative\ntasks. We are aware that there have been several existing\npapers [453, 454] and websites [455\u2013457] that present the\nsuggestions and guidelines to design good prompts. As a\ncomparison, we mainly aim to discuss the key factors (ingre-\ndients and principles) that are useful for prompt creation,\nand provide experimental results and analysis on popular\ntasks as the reference to the beginners.\nKey Ingredients. Typically, there are four key ingredients\nthat depict the functionality of a prompt for eliciting the\nabilities of LLMs to complete the tasks, including task\ndescription, input data, contextual information, and prompt\nstyle. To have an intuitive understanding of our discussion,\nwe also present three prompt examples for question answer-ing, meta-review generation, and text-to-SQL in Table 13.\n\u2022Task description. A task description is typically a specific\ninstruction that LLMs are expected to follow. In general, one\nshould clearly describe the task goal in natural language.\nFor the tasks with special input or output format, detailed\nclarifications are often needed, and one can further utilize\nkeywords to highlight the special settings for better guiding\nLLMs in task completion.\n\u2022Input data. In common cases, it is straightforward to\ndescribe input data ( e.g., an instance to be responded by\nLLMs) in natural language. For special input data, such\nas knowledge graph and table, it is necessary to apply an\nappropriate and convenient way to make them readable\nfor LLMs. For structured data, linearization is commonly\nused to transform the original records ( e.g., knowledge\ntriples) into sequences [458] due to the simplicity. Further,\nthe programming language ( e.g., executable code) has also\nbeen utilized to formulate the structured data, which can\nalso support using external tools ( e.g., program executor) to\nproduce the precise results [459, 460].\n\u2022Contextual information. In addition to the task descrip-\ntion and input data, contextual or background information49\nis also essential for specific tasks. For example, retrieved\ndocuments are highly useful for open-domain question\nanswering as supporting evidence. Both the quality of the\nretrieved documents and their relevance to the question\nhave an impact on the generated answers [461]. Thus, it\nneeds to include such information in a proper prompt\npattern or expression format. Furthermore, in-context task\nexemplars are also helpful for eliciting LLMs to accomplish\na complex task, which can better depict the task goal, the\nspecial output formats, and the mapping relation between\ninput and output.\n\u2022Prompt style. For different LLMs, it is important to\ndesign a suitable prompt style for eliciting their abilities to\nsolve specific tasks. Overall, one should express the prompt\nas a clear question or detailed instruction that can be well\nunderstood and answered. In some cases, it is also useful to\nadd the prefix or suffix to better guide LLMs. For example,\nusing the prefix \u201c Let us think step by step \u201d can help elicit\nLLMs perform step-by-step reasoning, and using the prefix\n\u201cYou are an expert on this task (or in this domain) \u201d can boost\nthe performance of LLMs in some specific tasks. Further, for\nchat-based LLMs ( e.g., ChatGPT), instead of directly feeding\na long or complex task prompt, it is suggested to decompose\nit into multiple prompts for the sub-tasks and then feed\nthem into LLMs via a multi-turn conversation [448].\nDesign Principles. Based on the key ingredients of prompts,\nwe summarize several critical design principles that can\nhelp create more effective prompts for solving various tasks.\n\u2022Expressing the task goal clearly. Task descriptions should\nnot be ambiguous or unclear, which likely lead to in-\naccurate or inappropriate responses. This highlights the\nneed for clear and unambiguous directives when utilizing\nthese models [66]. A clear and detailed description should\ncontain various elements to explain a task, including task\nobjective, input/output data ( e.g., \u201cGiven a long document, I\nwant you to generate a concise summary. \u201d), and the response\nconstraints ( e.g.,\u201cthe length of the summary cannot exceed 50. \u201d).\nBy providing a well-clarified task description, LLMs can\nmore effectively understand the target task and generate the\ndesired output.\n\u2022Decomposing into easy, detailed sub-tasks. To solve com-\nplex tasks, it is important to decompose the difficult task\ninto several more easier, detailed sub-tasks for helping\nLLMs accomplish the goal step by step, which is closely re-\nlated to the planning technique in Section 6.4. For example,\nfollowing the suggestion [454], we can explicitly list the sub-\ntasks in the form of multiple numbered items ( e.g., \u201cBraid a\ncoherent narrative by performing the following tasks: 1. ...; 2. ...; 3.\n...\u201d). By decomposing a target task into sub-tasks, LLMs can\nfocus on solving easier sub-tasks and finally achieve more\naccurate results for complex tasks.\n\u2022Providing few-shot demonstrations. As discussed in Sec-\ntion 6.2, LLMs can benefit from in-context learning for\nsolving complex tasks, where the prompts contain a small\nnumber of task examples of the desired input-output pairs,\ni.e.,few-shot demonstrations. Few-shot demonstrations can\nhelp LLMs learn the semantic mapping between input and\noutput without parameter tuning. In practice, it is suggested\nthat one should generate a few high-quality demonstrations\nfor the target task, which would highly benefit the final taskperformance.\n\u2022Utilizing model-friendly format. Since LLMs are pre-\ntrained on specially constructed datasets, there are some\nprompt formats that can make LLMs better understand\nthe instruction. For example, as the OpenAI documentation\nsuggests, we can use ### or\"\"\" as a stop symbol to\nseparate the instruction and context, which can be better\nunderstood by LLMs. As a general guideline, most existing\nLLMs perform a task better in English, thus it is useful to\nemploy English instructions to solve difficult tasks based on\nmachine translation.\nUseful Tips. In addition to the design principles, we also\npresent a collection of useful prompt tips based on existing\nwork or our empirical experiences in Table 12. Note that\nthese tips are suggested in a general manner, it does not\nindicate that they are the best prompts for the corresponding\ntasks. This part will be continuously updated with more\nguidelines or tips. We welcome readers to contribute to this\ncollection of prompt tips. We present the detailed procedure\nto contribute to the prompt tips, at the link: https://github.\ncom/RUCAIBox/LLMSurvey/tree/main/Prompts.\nEmpirical Analysis. We further conduct empirical studies\nto present the impact of prompts on task performance. To\nconduct the experiments, we select a variety of tasks that\nspan language generation, knowledge utilization, complex\nreasoning, structure data generation, and information re-\ntrieval. For each task, we manually write a prompt that\nfollows general guidelines introduced above. Note that the\ntested prompts may not be the optimal for these tasks,\nsince they mainly aim to help readers understand how to\nwrite an effective prompt for solving different tasks. Also,\nwe add a simplified prompt as the comparison for most\ntasks. Following the experimental settings in Section 7.4, we\nexamine the 3-shot performance of ChatGPT on complex\nreasoning tasks (Colored Objects and GSM8k), and zero-\nshot performance on other tasks. We report the experimental\nresults in Table 17, where we also include the supervised\nperformance in existing papers as reference.\n\u2022Carefully designed prompts can boost the zero-shot or few-\nshot performance of ChatGPT. By comparing the results of\nusing different prompts on the same task, we can see that\nusing the carefully designed prompts can achieve better per-\nformance than the simpler ones. In the carefully designed\nprompts, we provide a more clearly expressed task de-\nscription ( e.g., WMT and WikiFact), or use a model-friendly\nformat ( e.g., GSM8k and OBQA). For example, for WikiFact\ntask, the prompt with a more detailed task description leads\nto a performance increase from 29.25 to 31.21.\n\u2022More complex tasks can benefit more from careful prompt\nengineering on ChatGPT. In the WikiFact and Colored Objects\ntasks, the designed prompts have greatly improved the per-\nformance of ChatGPT, i.e.,from 23.61 to 28.47 on WikiFact\nand from 53.20 to 66.75 on Colored Objects. It indicates\nthe necessity of prompt engineering for LLMs to perform\nwell on complex tasks, since these tasks typically have\nspecific output formats or require background knowledge.\nOur example prompts provide more detailed task descrip-\ntion ( e.g., output format and task goal), which can help\nChatGPT better understand the complex task requirement\nfor fulfilling it.50\nTABLE 12: A collection of useful tips for designing prompts that are collected from online notes [453\u2013456] and experiences\nfrom our authors, where we also show the related ingredients and principles (introduced in Section 6.1.1). We abbreviate\nprinciples as Prin. and list the IDs of the related principles for each prompt. 1\u20dd: expressing the task goal clearly; 2\u20dd:\ndecomposing into easy, detailed sub-tasks; 3\u20dd: providing few-shot demonstrations; 4\u20dd: utilizing model-friendly format.\nIngredient Collected Prompts Prin.\nTask DescriptionT1. Make your prompt as detailed as possible ,e.g., \u201cSummarize the article into a short paragraph within 50 words. The major\nstoryline and conclusion should be included, and the unimportant details can be omitted. \u201d1\u20dd\nT2. It is helpful to let the LLM know that it is an expert with a prefixed prompt ,e.g., \u201cYou are a sophisticated expert in the\ndomain of compute science. \u201d1\u20dd\nT3. Tell the model more what it should do , but not what it should not do. 1\u20dd\nT4. To avoid the LLM to generate too long output, you can just use the prompt: \u201c Question: Short Answer: \u201d. Besides, you can\nalso use the following suffixes, \u201c in a or a few words \u201d, \u201cin one of two sentences \u201d.1\u20dd\nInput DataI1. For the question required factual knowledge, it is useful to first retrieve relevant documents via the search engine, and\nthen concatenate them into the prompt as reference.4\u20dd\nI2. To highlight some important parts in your prompt, please use special marks ,e.g., quotation (\u201d\u201d) and line break (\\n). You\ncan also use both of them for emphasizing.4\u20dd\nContextual InformationC1. For complex tasks, you can clearly describe the required intermediate steps to accomplish it, e.g., \u201cPlease answer the\nquestion step by step as: Step 1 - Decompose the question into several sub-questions, \u00b7\u00b7\u00b7\u201d2\u20dd\nC2. If you want LLMs to provide the score for a text, it is necessary to provide a detailed description about the\nscoring standard with examples as reference.1\u20dd\nC3. When LLMs generate text according to some context ( e.g., making recommendations according to purchase history),\ninstructing them with the explanation about the generated result conditioned on context is helpful to improve the quality\nof the generated text.2\u20dd\nC4. An approach similar to tree-of-thoughts but can be done in one prompt :e.g., Imagine three different experts are answering\nthis question. All experts will write down one step of their thinking, then share it with the group of experts. Then all experts will go on\nto the next step, etc. If any expert realizes they\u2019re wrong at any point then they leave. The question is2\u20dd\nDemonstrationD1.Well-formatted in-context exemplars are very useful, especially for producing the outputs with complex formats. 3\u20dd\nD2. For few-shot chain-of-thought prompting, you can also use the prompt \u201c Let\u2019s think step-by-step \u201d, and the few-shot\nexamples should be separated by \u201c \\n\u201dinstead of full stop.1\u20dd3\u20dd\nD3. You can also retrieve similar examples in context to supply the useful task-specific knowledge for LLMs. To retrieve\nmore relevant examples, it is useful to first obtain the answer of the question, and then concatenate it with the question for\nretrieval.3\u20dd4\u20dd\nD4. The diversity of the in-context exemplars within the prompt is also useful. If it is not easy to obtain diverse questions,\nyou can also seek to keep the diversity of the solutions for the questions.3\u20dd\nD5. When using chat-based LLMs, you can decompose in-context exemplars into multi-turn messages , to better match the\nhuman-chatbot conversation format. Similarly, you can also decompose the reasoning process of an exemplars into multi-turn\nconversation.3\u20dd\nD6.Complex and informative in-context exemplars can help LLMs answer complex questions. 3\u20dd\nD7. As a symbol sequence can typically be divided into multiple segments ( e.g.,i1, i2, i3\u2212\u2192i1, i2andi2, i3), the preceding\nones can be used as in-context exemplars to guide LLMs to predict the subsequent ones, meanwhile providing historical\ninformation.2\u20dd3\u20dd\nD8.Order matters for in-context exemplars and prompts components. For very long input data, the position of the question\n(first or last) may also affect the performance.3\u20dd\nD9. If you can not obtain the in-context exemplars from existing datasets, an alternative way is to use the zero-shot\ngenerated ones from the LLM itself.3\u20dd\nOther DesignsO1. Let the LLM check its outputs before draw the conclusion, e.g., \u201cCheck whether the above solution is correct or not. \u201d 2\u20dd\nO2. If the LLM can not well solve the task, you can seek help from external tools by prompting the LLM to manipulate\nthem. In this way, the tools should be encapsulated into callable APIs with detailed description about their functions, to\nbetter guide the LLM to utilize the tools.4\u20dd\nO3. The prompt should be self-contained , and better not include pronouns ( e.g., it and they) in the context. 1\u20dd\nO4. When using LLMs for comparing two or more examples, the order affects the performance a lot. 1\u20dd\nO5. Before the prompt, assigning a role for the LLM is useful to help it better fulfill the following task instruction, e.g., \u201cI\nwant you to act as a lawyer\u201d .1\u20dd\nO6. OpenAI models can perform a task better in English than other languages. Thus, it is useful to first\ntranslate the input into English and then feed it to LLMs.4\u20dd\nO7. For multi-choice questions, it is useful to constrain the output space of the LLM. You can use a more detailed explanation\nor just imposing constraints on the logits.1\u20dd\nO8. For sorting based tasks ( e.g., recommendation), instead of directly outputting the complete text of each item after sorting,\none can assign indicators (e.g., ABCD ) to the unsorted items and instruct the LLMs to directly output the sorted indicators.1\u20dd\n\u2022For mathematical reasoning tasks, it is more effective to\ndesign specific prompts based on the format of programming\nlanguage. For GSM8k, the designed prompt employs code-\nformatted few-shot demonstrations to convert this mathe-\nmatical reasoning task into code generation task, which can\nleverage the strong code synthesis ability of ChatGPT for\nsolving mathematical problems. Further, with the help of an\nexternal program executor, we are able to obtain more pre-\ncise results instead of using LLMs for arithmetic operation.\nAs we can see, the performance is boosted from 78.47 to\n79.30 on GSM8k, indicating the usefulness of programminglanguage in mathematical reasoning tasks.\n\u2022In knowledge utilization and complex reasoning tasks,\nChatGPT with proper prompts achieves comparable performance\nor even outperforms the supervised baselines methods. In knowl-\nedge utilization and complex reasoning tasks, ChatGPT\nwith proper zero-shot or few-shot prompts can achieve\ncomparable performance or even outperform the super-\nvised methods, e.g., 31.21 (ChatGPT) v.s.34.20 (supervised\nbaseline) on WikiFact. Despite that, ChatGPT still performs\nworse than supervised baseline models on some specific\ntasks ( e.g., ARC and WikiFact), since these supervised mod-51\nels have been specially optimized with task-specific data.\n\u2022Through suitable prompt engineering, LLMs can handle\nsome non-traditional NLP tasks. With the help of specific\nprompts, ChatGPT can also accomplish non-traditional NLP\ntasks, i.e.,the general recommendation and conversational\nrecommendation. A key point is that these tasks can be\nwell expressed or described in natural language. However,\nthe performance of ChatGPT is still far from the referenced\nperformance in these tasks, as LLMs cannot directly fit these\ntasks, which require specific domain knowledge and task\nadaptation [357, 462].\n6.1.2 Prompt Optimization\nAlthough manually creating task prompts is more intuitive,\nit is time consuming and, more importantly, models are\nhighly sensitive to the crafted prompts\u2014improper prompts\nwill lead to low task performance (as shown in Table 17).\nTherefore, a large body of studies propose automatic opti-\nmization approaches for discrete prompts and continuous\nprompts to achieve the optimal performance [396, 405]. In\nthis part, we will detail these studies from two perspectives,\ni.e.,discrete prompts and continuous prompts.\nDiscrete Prompt Optimization. Discrete prompt is typically\ncomposed of a sequence of natural language tokens. Despite\nthat the form is simple and flexible, optimizing prompts in\ndiscrete space is a challenging problem due to the combina-\ntorial huge search space. To automatically search effective\nprompts for downstream tasks, existing studies propose a\nwide spectrum of discrete prompt approaches, which are\ndetailed as follows.\n\u2022Gradient-based approaches. This kind of approaches\naims to optimize the prompt search process by maximizing\nthe output likelihood via gradient update [405, 464\u2013466].\nAs a representative work, Auto-Prompt [405] proposes a\ngradient-guided method to greedily searches the optimal\ntoken for each position of the prompt, leveraging the gra-\ndient approximated by the change in the log-likelihood\nwhen replacing a prompt token with another candidate\ntoken from vocabulary. However, such a search process\ncan be extremely expensive since it needs to evaluate each\ncandidate token for each position of the prompt, leading to a\nnumber of additional forward passes. Therefore, improved\ngradient method [464] has been proposed by transforming\ndiscrete tokens into continuous embeddings and computing\nthe gradient on continuous space during optimization.\n\u2022RL-based approaches. Since discrete prompts are difficult\nto be learned through gradient back-propagation, a num-\nber of studies propose to formulate the discrete prompt\noptimization as a reinforcement learning (RL) problem and\nleverage RL algorithms for optimization [467, 468]. For ex-\nample, RLPrompt [467] trains a policy network to generate\ndesired prompts with multiple reward functions. In this\napproach, several effective reward stabilization strategies\nare also proposed to enhance the RL training efficiency.\nCompared to previous work that requires sufficient data\nfor training, TEMPERA [468] proposes to directly generate\nprompts at test time by utilizing a pre-trained RL agent\nto sequentially edit different parts of an manually-written\ninitial prompt.\u2022Edit-based approaches. For the above methods, gradient-\nbased and RL-based tuning can be extremely computation-\nally demanding for ever larger models, and may not be fea-\nsible for API-based model calls ( e.g., ChatGPT). Therefore,\nanother line of work aims to directly edit existing prompts\nbased on the task performance. Specifically, GPS [469] bor-\nrows an idea from the genetic algorithm and proposes\na genetic prompt search method that utilizes a language\nmodel ( i.e.,T5) to edit prompts by taking the cloze task form.\nIn addition to model based edit methods, human-defined\noperations can be also employed for prompt editing [470],\nincluding delete, swap, paraphrase, and addition. Based\non these operations, they iteratively edit the prompts and\ngreedily search for the best prompt guided by the model\nperformance on a small pool of examples.\n\u2022LLM-based approaches. Due to the exceptional capacities\nof LLMs, an increasing number of studies directly leverage\nLLMs as prompt generator [471\u2013473]. Specifically, APE [471]\nutilizes an LLM to generate initial prompts, then selects\nthe best prompt with the highest accuracy, and finally im-\nproves the best candidate through an iterative Monte Carlo\nsearch method. Similarly, APO [472] instructs the LLM to\ngenerate text feedback on how to refine an old prompt\ninto new improved prompts. However, their search in the\nprompt space might be inefficient without fully considering\nthe whole refinement trace of previous prompts, thus po-\ntentially leading to sub-optimal results. Therefore, another\nstudy [473] incorporates the previous prompts with their\nscores to instruct LLMs for progressively generating better\nnew prompts. However, these approaches still struggle in\nexploring the vast space of effective prompts. Inspired by\nhuman-like trial-and-error, prompt optimization is further\nformulated as a strategic planning problem [474] and uses\nMonte Carlo tree search to navigate the vast prompt space.\nContinuous Prompt Optimization. Different from discrete\nprompts, continuous prompts consist of a set of continuous\nembeddings, which can be directly optimized through the\ngradient update based on the loss of downstream tasks.\nNote that continuous prompt optimization has been mainly\nstudied in PLMs, but draws limited attention in era of LLMs\ndue to their massive magnitudes of parameters. We include\nthe discussion of this part for content completeness. In prior\nwork, most studies typically rely on supervised learning to\ntrain continuous prompts based on task data. Furthermore,\nin data-scarce scenarios, transfer learning methods can be\nemployed to alleviate the lack of labeled data on target tasks.\nThese two approaches are detailed below.\n\u2022Prompt learning with sufficient data. In this approach,\nmost existing methods regard continuous prompts as train-\nable model parameters and then leverage supervised learn-\ning to optimize the continuous prompts by minimizing\nthe cross-entropy loss based on sufficient downstream task\ndata [396, 397, 401, 475]. As discussed in Section 5.3.1,\nprefix tuning [396] prepends a sequence of prefixes ( i.e.,\na set of trainable continuous vectors) to each Transformer\nlayer in language models, while prompt tuning [397] only\nincorporates trainable prompt vectors at the input layer. By\nfixing the large-scale parameters of LLMs and only tuning\ncontinuous prompt vector, this kind of approaches can be\nextremely parameter-efficient (Section 5.3). However, these52\nTABLE 13: Example instructions collected from [454, 463]. The blue text denotes the task description, the red text denotes\nthe contextual information, the green text denotes the demonstrations, and the gold text denotes the prompt style.\nUse the provided articles delimited by triple quotes to answer questions. If the answer cannot be found in the articles, write \u201cI could not find an\nanswer.\u201d\nArticles: \u201c\u201c\u201cJoao Moutinho is a Portuguese footballer who last played as a central midfielder for Premier League club Wolverhampton Wanderers\nand the Portugal national team.\u201d\u201d\u201d\nQuestion: Is the following sentence plausible? \u2019Joao Moutinho was out at third.\u2019\nAnswer: Let\u2019s think step by step. Joao Moutinho is a soccer player. Being out at third is part of baseball, not soccer. So the answer is No.\n...\n<Demonstrations >\nArticles: <insert articles, each delimited by triple quotes >\nQuestion: <insert question >\nAnswer:\nPrepare a meta-review by answering the following questions from the reviewer comments (provided after the questions).\n1. Based on the reviewer\u2019s comments, what are the core contributions made by this manuscript?\n2. What are the common strengths of this work, as mentioned by multiple reviewers?\n3. What are the common weaknesses of this work, as highlighted by multiple reviewers?\n4. What suggestions would you provide for improving this paper?\n5. What are the missing references mentioned by the individual reviews?\nThe review texts are below: <insert three comments R1,R2,R3from the reviewers >\nMeta-review: <insert meta-review >\n...\n<Demonstrations >\nProvide justification for your response in detail by explaining why you made the choices you actually made. A good output should be coherent,\nhighlight major strengths/issues mentioned by multiple reviewers, be less than 400 words in length, and finally, the response should be in English\nonly.\nThe review texts are below: <insert three comments R1,R2,R3from the reviewers >\nMeta-review:\nCREATE TABLE Highschooler (\nID int primary key,\nname text,\ngrade int\n);\n/*\n3 example rows:\nSELECT * FROM Highschooler LIMIT 3;\nID name grade\n1234 Janie 8\n5678 Mary 8\n9012 Mike 9\n*/\nUsing valid SQLite, answer the following questions for the tables provided above.\nQuestion: What is Kyle\u2019s id?\nSQL: SELECT ID FROM Highschooler WHERE name=\u201cKyle\u201d;\n...\n<Demonstrations >\nQuestion: <insert question >\nSQL:\napproaches are typically independent of the inputs, lacking\nsufficient consideration of input semantics. Therefore, the\nauthors in [475] propose context tuning, where the continu-\nous prompts are derived based on the input text and learned\nthrough the downstream task losses.\n\u2022Prompt transferring with scarce data. Supervised learn-\ning approaches demand in sufficient training data to learn\noptimal continuous prompts, which may not work well\nin data-scarce domains and tasks. To address this prob-\nlem, SPoT [476] proposes a prompt-based transfer learning\napproach, which first learns a single continuous prompt\nfor several representative source tasks and then uses this\nprompt to initialize the prompt for a target task. However,this approach leverages the same prompt for solving all\ninstances of the target task. For a single task, even a well-\nlearned prompt may not be suitable for all the data instances\nfrom a large population. To address this issue, an improved\nmethod [477] designs an adaptive attention mechanism dur-\ning the prompt transfer process to derive the target prompts,\nconsidering both task- and instance-level information. The\nprompt transfer paradigm can leverage the knowledge of\ndata-sufficient source tasks encoded in source prompts for\nsolving data-scarce target tasks.53\n6.2 In-Context Learning\nAs a special prompting form, in-context learning (ICL) is\nfirst proposed along with GPT-3 [55], which has become a\ntypical approach to utilizing LLMs.\n6.2.1 ICL Formulation\nAs stated in [55], ICL uses a formatted natural language\nprompt, consisting of the task description and/or a few task\nexamples as demonstrations. Figure 14 presents an illustra-\ntion of ICL. First, starting with a task description, a few ex-\namples are selected from the task dataset as demonstrations.\nThen, they are combined in a specific order to form nat-\nural language prompts with specially designed templates.\nFinally, the test instance is appended to the demonstration\nas the input for LLMs to generate the output. Based on task\ndemonstrations, LLMs can recognize and perform a new\ntask without explicit gradient update.\nFormally, let Dk={f(x1, y1), . . . , f (xk, yk)}represent\na set of demonstrations with kexamples, where f(xk, yk)is\nthe prompt function that transforms the k-th task example\ninto natural language prompts. Given the task description\nI, demonstration Dk, and a new input query xk+1, the\nprediction of the output \u02c6yk+1generated from LLMs can be\nformulated as follows40:\nLLM\u0000I, f(x1, y1), . . . , f (xk, yk)| {z }\ndemonstrations, f(xk+1|{z}\ninput,|{z}\nanswer)\u0001\u2192\u02c6yk+1.\n(12)\nwhere the actual answer yk+1is left as a blank to be\npredicted by the LLM. Since the performance of ICL heavily\nrelies on demonstrations, it is important to properly design\nthem in the prompts. According to the construction process\nin Equation (12), we focus on three major aspects of for-\nmatting demonstrations in the prompts, including how to\nselect examples that make up demonstrations, format each\nexample into the prompt with the function f(\u00b7), and arrange\ndemonstrations in a reasonable order.\nA comprehensive review of ICL has been presented in\nthe survey paper [50], and we suggest the readers refer-\nring to it for a more general, detailed discussion on this\ntopic. Compared with this survey, we specially focus on the\ndiscussion of applying ICL to LLMs in two major aspects,\ni.e.,demonstration design and the underlying mechanism\nof ICL. Also, ICL has a close connection with instruction\ntuning (discussed in Section 5.1) in that both utilize nat-\nural language to format the task or instances. However,\ninstruction tuning needs to fine-tune LLMs for adaptation,\nwhile ICL only prompts LLMs for utilization. Furthermore,\ninstruction tuning can enhance the ICL ability of LLMs to\nperform target tasks, especially in the zero-shot setting (only\nusing task descriptions) [69].\n6.2.2 Demonstration Design\nSeveral studies have shown that the effectiveness of ICL\nis highly affected by the design of demonstrations [432,\n40. When ICL was introduced in the GPT-3\u2019s paper [55], it was\noriginally defined to be a combination of the task description and\ndemonstration examples, wherein either component is dispensable.\nFollowing this definition, when a LLM is required to solve an unseen\ntask by using only task descriptions, it can be also considered to\nperform ICL for task solving, whereas the ICL ability can be enhanced\nby instruction tuning.478, 479] Following the discussion in Section 6.2.1, we will\nintroduce the demonstration design of ICL from three major\naspects, i.e.,demonstration selection, format, and order.\nDemonstration Selection. The performance of ICL tends\nto have a large variance with different demonstration exam-\nples [428], so it is important to select a subset of examples\nthat can effectively leverage the ICL capability of LLMs.\nThere are two main demonstration selection approaches,\nnamely heuristic and LLM-based approaches:\n\u2022Heuristic approaches. Due to their simplicity and low\ncosts, existing work widely adopts heuristic methods to\nselect demonstrations. Several studies employ a k-NN based\nretriever to select examples that are semantically relevant to\nthe query [428, 480]. However, they perform the selection\nindividually for each example, rather than evaluating the\nexample set as a whole. To resolve this issue, diversity-\nbased selection strategies are proposed to choose the most\nrepresentative set of examples for specific tasks [481, 482].\nFurthermore, in [483], both relevance and diversity are taken\ninto consideration when selecting demonstrations.\n\u2022LLM-based approaches. Another line of work selects\ndemonstrations by making use of LLMs. For example, LLMs\ncan be utilized to directly measure the informativeness\nof each example according to the performance gain after\nadding the example [484]. In addition, EPR [429] proposes\na two-stage retrieval approach that first recalls similar ex-\namples with an unsupervised method ( e.g., BM25) and then\nranks them using a dense retriever (trained with positive\nand negative examples labeled by LLMs). As an alterna-\ntive approach, the task of demonstration selection can be\nformulated into a RL problem, where LLMs serve as the\nreward function to provide feedback for training the policy\nmodel [485]. Since LLMs perform well for text annota-\ntion [486], some recent studies employ LLM itself as the\ndemonstration generator without human intervention [487].\nTo summarize, as discussed in [488], the selected demon-\nstration examples in ICL should contain sufficient informa-\ntion about the task to solve as well as be relevant to the test\nquery, for the above two selection approaches.\nDemonstration Format. After selecting task examples, the\nnext step is to integrate and format them into a natural\nlanguage prompt for LLMs. A straightforward method is to\ninstantiate a pre-defined template with the corresponding\ninput-output pairs [36]. To construct more informative tem-\nplates, recent studies consider adding task descriptions [69]\nor enhancing the reasoning capability of LLMs with chain-\nof-thought prompts [33]. For instance, in [166], the authors\ncollect a large-scale dataset with task descriptions written by\nhumans. After tuning with this dataset, the performance on\nseen tasks can be boosted, and LLMs can also generalize to\nunseen tasks to some extent. To reduce the annotation costs,\na semi-automated approach has been proposed in [143]\nby employing a seed set consisting of human-written task\ndescriptions to guide LLMs to generate task descriptions\nfor new tasks. Since it is costly to manually annotate\ndemonstration formats for different tasks, some work also\nstudies how to automatically generate high-quality ones.\nAs two representative methods, Auto-CoT [434] leverages\nLLMs with the zero-shot prompt \u201c Let\u2019s think step by step \u201d\nfor generating intermediate reasoning steps, while least-to-54\nAnswer the following mathematical reasoning questions:\nQ:    S am has 12 marbles. He gives 1/4 of them to his sister. \nHow many marbles does Sam have left?N x If a rectangle has a length of 6 cm and a width of 3 cm, \nwhat is the perimeter of the rectangle?\nFor a rectangle, add up the length and width and double it. So, the perimeter of this rectangle is (6 + 3) x 2 = 18 cm.\nThe answer is 18 cm.Q:\nA:\nLLM A:The answer is 9.A: He gives (1 / 4) x 12 = 3 marbles. \nSoSam is left with 12 \u2013 3 = 9 marbles. \nThe answer is 9.\n:Chain -of-Thought :Task description :Demonstration :QueryIn-Context Learning Chain -of-Thought Prompting\nQ:\nA:\nQ:\nA:Answer the following mathematical reasoning questions:\nQ:     Sam has 12 marbles. He gives 1/4 of them to his sister. \nHow many marbles does Sam have left?NxThe answer is 8.\nIf a rectangle has a length of 6 cm and a width of 3 cm, what is the perimeter of the rectangle?\nThe answer is 18 cm.If you have 12 candies and you give 4 candies to your friend, how many candies do you have left?\nFig. 14: A comparative illustration of in-context learning (ICL) and chain-of-thought (CoT) prompting. ICL prompts LLMs\nwith a natural language description, several demonstrations, and a test query, while CoT prompting involves a series of\nintermediate reasoning steps in prompts.\nmost prompting [439] first queries LLMs to perform prob-\nlem decomposition and then utilizes LLMs to sequentially\nsolve sub-problems based on the intermediate answers to\npreviously solved ones.\nDemonstration Order. LLMs are shown to sometimes suffer\nfrom the recency bias, i.e.,they are prone to repeat answers\nthat are near the end of demonstrations [479]. Thus, it is\nimportant to arrange demonstrations ( i.e., task examples)\nin a reasonable order. Early work proposes several heuris-\ntic methods to quickly find a good order. For example,\ndemonstrations can be directly organized according to their\nsimilarity to the query in the embedding space [428]: the\nmore similar, the closer to the end. In addition, global\nand local entropy metrics can be used to score different\ndemonstration orders [432]. To integrate more task infor-\nmation, some recent studies propose to minimize the code\nlength required to compress and transmit task labels, which\nis inspired by information theory [489]. However, these\nmethods need additional labeled data as the validation\nset to evaluate the performance of specific demonstration\norders. To eliminate this need, the authors in [432] propose\nto sample the validation data from the LLM itself.\n6.2.3 Underlying Mechanism\nAfter pre-training, LLMs can exhibit intriguing ICL capabil-\nity without being updated. In what follows, we discuss two\nkey questions about the ICL ability of LLMs, i.e.,\u201chow does\npre-training affect the ICL ability \u201d and \u201c how do LLMs perform\nICL during inference \u201d.\nHow Pre-Training Affects ICL? ICL is first proposed in\nGPT-3 [55], and it has been shown that the ICL ability\nbecomes more significant with a larger model size. Further,\nsome studies reveal that small-scale PLMs can also demon-\nstrate a strong ICL ability by continual pre-training [490]\nor fine-tuning [491] on specially designed training tasks,\nwhich typically involve additional task examples in theinput during the training process. It suggests that the design\nof training tasks is an important influence factor on the ICL\ncapability of LLMs. Besides training tasks, recent studies\nhave also investigated the relationship between ICL and\npre-training corpora [488, 492]. For example, ICL can be\ntheoretically explained as the product of pre-training on\ndocuments that exhibit long-range coherence [488]. Fur-\nther, another study [492] theoretically analyzes that when\nscaling parameters and data, LLMs based on next-word\nprediction can emerge the ability of ICL by learning from\nthe compositional structure ( e.g., how words and phrases\nare combined to form larger linguistic units like sentences)\npresent in language data.\nHow LLMs Perform ICL? At the inference stage, researchers\nfocus on analyzing how the ICL capability operates based\non given demonstrations since no explicit learning or updat-\ning is involved. According to the discussion in [493], there\nare two main ways for LLMs to utilize demonstrations: task\nrecognition and task learning.\n\u2022Task recognition. In the first way, LLMs recognize the\ntask from demonstrations and utilize the prior knowledge\nobtained from pre-training to solve new test tasks. A Proba-\nbly Approximately Correct (PAC) framework [494] has been\nproposed to assess the learnability of ICL. It assumes that\nthere exists a latent variable representing the task in the pre-\ntraining data, and LLMs have been shown to be capable\nof capturing this variable from demonstrations, enabling\nthem to recognize the task in ICL. Also, the interpretation\nof ICL as task recognition is supported by several empir-\nical studies [478, 495]. For example, it has been observed\nthat replacing the inputs or labels of demonstrations with\nrandom ones sampled from the input or label space does\nnot seriously hurt the performance of LLMs, indicating that\nLLMs mainly recognize the target task from demonstrations\ninstead of learning from them [478, 493]. Similarly, LLMs\ncan exhibit decent performance even if the prompt template\nis irrelevant or misleading [495].55\n\u2022Task learning. In the second way, LLMs learn new tasks\nunseen in the pre-training stage only through demonstra-\ntions. Specially, task learning is analyzed mainly from the\nperspective of gradient descent and considered as implicit\nfine-tuning [65, 496]. Then, ICL can be explained as follows:\nby means of forward computation, LLMs generate meta-\ngradients with respect to demonstrations and implicitly per-\nform gradient descent via the attention mechanism. Exper-\niments also show that certain attention heads in LLMs are\ncapable of performing task-agnostic atomic operations ( e.g.,\ncopying and prefix matching), which are closely related to\nthe ICL ability [497]. Furthermore, some studies abstract\nICL as an algorithm learning process [498]. For example, the\nauthors in [498] find that LLMs essentially encode implicit\nmodels through their parameters during pre-training. With\nthe examples provided in ICL, LLMs can implement learn-\ning algorithms such as gradient descent or directly compute\nthe closed-form solution to update these models during\nforward computation. Under this explanation framework,\nit has been shown that LLMs can effectively learn simple\nlinear functions and even some complex functions like deci-\nsion trees with ICL [498].\nAs discussed in a recent study [493], LLMs exhibit the\nabilities of both task recognition and task learning in ICL,\nbut the two abilities seem to be possessed with different\nmodel scales. As shown in the experiments [493], the ability\nof task recognition is easier to obtain, and even a small LM\nwith only 350M parameters can exhibit this ability, while\ntask learning can only emerge for LLMs with at least 66B\nparameters. Another study [499] also supports this find-\ning with specially designed experiments. They set up the\ntasks with flipped and semantically unrelated labels in the\nexperiment, which require task learning when performing\nICL. The results suggest that small LMs tend to disregard\nthe labels and mainly depend on their prior knowledge\nto accomplish the task, while LLMs have the ability to\nsurpass their prior knowledge and acquire new knowledge\nfrom demonstrations, resulting in better outcomes. Further-\nmore, to improve the task learning ability, Meta-In-Context\nLearning [500] proposes to include multiple related tasks\ninstead of just a single one in the prompt. In addition,\nSymbol Tuning [501] fine-tunes LLMs on demonstrations\nwith semantically unrelated labels ( e.g., foo/bar instead of\npositive/negative for sentiment analysis), forcing LLMs to\nlearn the task from demonstrations instead of relying on\nprior knowledge.\n6.3 Chain-of-Thought Prompting\nChain-of-Thought (CoT) prompting [33, 502] is an improved\nprompting strategy to boost the performance of LLMs on\ncomplex reasoning tasks, such as arithmetic reasoning [503],\ncommonsense reasoning [504], and symbolic reasoning [33].\nInstead of simply constructing the prompts with input-\noutput pairs like ICL, CoT prompting further incorporates\nintermediate reasoning steps, which serve as the bridge be-\ntween inputs and outputs. Figure 14 presents an illustration\nof CoT. In the following part, we will first elaborate on the\nbasic CoT prompting approach and its improved strategies,\nthen discuss when and why CoT prompting works.6.3.1 Basic CoT Prompting Approach\nCoT prompting is first proposed as an extension of ICL [33],\nwhich augments each demonstration \u27e8input, output \u27e9as\n\u27e8input, CoT, output \u27e9. A CoT is a series of intermediate\nreasoning steps for connecting the input and output . With\nthese augmented demonstrations, LLMs can follow them to\ngenerate CoTs and the answer for a new input. However,\nunlike \u27e8input, output \u27e9pairs in ICL, CoTs are difficult to\nobtain and usually require human annotation. Fortunately,\nit has been found that LLMs can be triggered to generate\nCoTs through simple instructions like \u201c Let\u2019s think step by\nstep.\u201d [505], making CoT prompting easy to use. There are\nalso alternative magic prompts that can elicit the ability\nof CoT reasoning and further improve the performance of\nLLMs, such as \u201c Take a deep breath and work on this problem\nstep-by-step. \u201d [473].\nAs illustrated in Figure 15, the generation process of\nCoT follows a chain structure in the basic CoT prompt-\ning approach, where LLMs generate CoTs step by step.\nTypically, CoT takes the format of natural language text.\nHowever, textual CoTs may not work well on complex tasks\nthat require rigorous logic for reasoning. Considering this,\nsome work uses code [506, 507] due to its structured and\nprecise nature. Furthermore, the authors in [508] propose\nto dynamically select text or code as the format of CoTs to\ncombine their advantages.\n6.3.2 Improved CoT Prompting Strategies\nDespite the performance improvement in complex reason-\ning tasks, CoT prompting still suffers from problems like\nincorrect reasoning and instability. In this part, we first\nintroduce how to design better CoT prompts and enhanced\nCoT generation strategies, and then introduce the extension\nof the basic chain structure of CoT. Figure 15 illustrates the\nevolution of representative CoT prompting strategies.\nBetter Prompt Design. Since CoT prompting relies on\nprompts to elicit the reasoning capabilities of LLMs, the\ndesign of prompts is critical to its performance. As a di-\nrect approach, it is shown that using diverse CoTs ( i.e.,\nmultiple reasoning paths for each problem) can effectively\nenhance the performance [437]. Another intuitive idea is\nthat prompts with more complex reasoning paths are more\nlikely to elicit the reasoning ability of LLMs [433], which\ncan result in higher accuracy in generating correct an-\nswers. However, all these approaches rely on annotated CoT\ndatasets, which limits their use in practice. To overcome\nthis limitation, magic instructions such as \u201c Let\u2019s think step\nby step \u201d can be used to automatically construct CoTs by\nprompting LLMs [434].\nEnhanced CoT Generation. Since LLMs are prone to\nproducing incorrect reasoning steps and exhibiting insta-\nbility in the generation process, there are a number of\nstudies [436, 509] to improve the generation of CoT. In this\npart, we will introduce two typical approaches to enhancing\nthe generation of CoT: sampling- and verification-based\nmethods.\n\u2022Sampling-based methods. LLMs are known to suffer\nfrom instability during inference, which can lead to un-\nfaithfulness in the generated reasoning steps. To address56\nSampling-\nbased CoT\n... ... ...\nEnsembleInput\nOutputGoT\nInput\nOutput\nReason Backtrack Aggregate Unevaluated thought Positive thought Negative thoughtToT\nInput\nOutputVerification-\nbased CoT\nVerificationInput\nOutput...\u2716\ufe0f \u2716\ufe0fCoT\n...Input\nOutput\nFig. 15: An illustration of the evolution of CoT prompting strategies. It begins with the basic CoT approach and progresses\nto enhanced CoT generation techniques, including sampling-based and verification-based methods. Finally, it extends to\nvariations of the chain structure, such as trees and graphs. Here, \u201cthought\u201d refers to an intermediate reasoning step as\nstated in [33, 451].\nthis issue, some work proposes to sample multiple rea-\nsoning paths instead of using greedy decoding. As a rep-\nresentative solution, self-consistency [436] first generates\nseveral reasoning paths and then takes an ensemble over\nthe corresponding answers, selecting the most consistent\none through majority voting. However, such a method can\nstill lead to wrong answers when most of the reasoning\npaths are misled. Considering this, the authors in [433] only\nvote on the kmost complex reasoning paths based on their\nobservation that reasoning paths with higher complexity\n(e.g.,more reasoning steps) usually have better performance.\nFurthermore, MCR [510] proposes referring to the steps\nfrom other reasoning paths when generating the next step,\nand performs reasoning across multiple reasoning paths to\ngenerate the final answer.\n\u2022Verification-based methods. The sequential nature of rea-\nsoning steps in CoTs can lead to the accumulation of errors\nin the generated CoTs when certain steps are incorrect. To\nmitigate this problem, recent studies propose to verify the\ncorrectness of generated reasoning steps with either trained\nverifiers or LLMs themselves. For example, DIVERSE [509]\ntrains solution-level and step-level verifiers respectively to\nexamine the reasoning steps at different granularities. An-\nother approach [511] utilizes LLMs to verify the correctness\nof reasoning steps through step-by-step self-verification\nwith a specially designed reasoning format. In addition,\nseveral studies propose backward reasoning for verification:\nit first deduces the necessary question conditions [512, 513]\nor variables [514] from the model\u2019s predictions, and then\ncompares them with the original ones.\nReasoning Structure Extension. Despite the generality, the\nchain reasoning structure of basic CoT prompting limits its\neffectiveness in solving complex tasks, which require ex-\nploration like foresight and backtracking during inference.\nTherefore, many studies have been devoted to extending\nthe reasoning structure by designing more intricate thoughtprocesses, e.g., tree- and graph-structured reasoning.\n\u2022Tree-structured reasoning. This approach (exemplified by\nTree of Thoughts (ToT) [451, 515]) formulates the reasoning\nprocess in a hierarchical tree structure, where intermediate\nthoughts are nodes. In this way, it enables LLMs to explore\nmultiple reasoning paths in parallel and further supports\nthe operation of lookahead and backtracking to facilitate\nmore comprehensive decisions. In addition, TouT [516] takes\nthe uncertainty of intermediate thoughts into account for\nthought evaluation based on Monte Carlo Dropout.\n\u2022Graph-structured reasoning. Although the tree structure\nfacilitates parallel reasoning, it also imposes restrictions on\nthe reasoning process. With more complex topological struc-\ntures, graphs offer greater flexibility in reasoning, enabling\nthe characterization of more intricate relationships and in-\nteractions. For instance, Graph of Thoughts (GoT) [517, 518]\nconceptualizes the reasoning process as an arbitrary graph,\nwhere vertices denote intermediate thoughts and edges\ndenote the interdependence between these thoughts. Com-\npared with ToT, it can further utilize thoughts from other\nreasoning paths when generating new thoughts. However,\nsuch an approach requires a large number of interactions\nwith LLMs, making the thought exploration process highly\ninefficient. To reduce potentially meaningless thought\nexploration, XoT [519] further proposes to guide the search\nof thoughts with pre-trained policy and value networks.\n6.3.3 Further Discussion on CoT Prompting\nIn this part, we present discussions regarding two funda-\nmental questions related to CoT prompting, i.e.,\u201cwhen does\nCoT prompting work for LLMs \u201d and \u201c why can LLMs perform\nCoT reasoning \u201d.\nWhen CoT Prompting Works For LLMs? Since CoT reason-\ning is an emergent ability [31], it only has a positive effect\non sufficiently large models (typically containing 10B or\nmore parameters [33]) but not on small models. Moreover,57\nsince CoT prompting augments the standard prompting\nwith intermediate reasoning steps, it is mainly effective\nfor the tasks that require step-by-step reasoning [33], e.g.,\narithmetic reasoning, commonsense reasoning, and sym-\nbolic reasoning. Whereas, for other tasks that do not rely\non complex reasoning, CoT prompting might lead to worse\nperformance than standard prompting [438], e.g., MNLI-\nm/mm, SST-2, and QQP from GLUE [260]. Interestingly, it\nseems that the performance gain brought by CoT prompting\ncould be significant only when standard prompting yields\npoor results [33].\nWhy LLMs Can Perform CoT Reasoning? As the second\nquestion, we discuss the underlying mechanism of CoT\nprompting in the following two aspects.\n\u2022The source of CoT reasoning ability . Regarding the source\nof CoT reasoning capability, it is widely hypothesized that it\ncan be attributed to training on code since models trained on\nit show a strong reasoning ability [47, 520, 521]. Intuitively,\ncode data is well organized with algorithmic logic and\nprogramming flow, which may be useful to improve the rea-\nsoning performance of LLMs. However, this hypothesis still\nlacks publicly reported evidence of ablation experiments\n(with and without training on code). In addition, instruction\ntuning seems not to be the key reason for obtaining the CoT\nreasoning ability, since it has been empirically shown that\ninstruction tuning on non-CoT data does not improve the\nperformance on held-out CoT reasoning benchmarks [69].\n\u2022The effect of CoT prompting components . The major dis-\ntinction between CoT prompting and standard prompting\nis the incorporation of reasoning paths prior to the final\nanswer. Thus, some researchers investigate the effects of\ndifferent components in the reasoning paths. Specifically,\na recent study identifies three key components in CoT\nprompting, namely symbols (e.g., numerical quantities in\narithmetic reasoning), patterns (e.g., equations in arithmetic\nreasoning), and text (i.e., the rest of tokens that are not\nsymbols or patterns) [522]. It is shown that the latter two\nparts ( i.e., patterns and text) are essential to the model\nperformance, and removing either one would lead to a\nsignificant performance drop. However, the correctness of\nsymbols and patterns does not seem critical. Further, there\nexists a symbiotic relationship between text and patterns:\nthe text helps LLMs to generate useful patterns, and patterns\naid LLMs to understand tasks and generate texts that help\nsolve them [522].\nIn summary, CoT prompting provides a general and\nflexible approach to eliciting the reasoning ability of LLMs.\nThere are also some preliminary attempts to extend this\ntechnique to solve multimodal [523] and multilingual\ntasks [524].\n6.4 Planning for Complex Task Solving\nPrompting with ICL and CoT is a conceptually simple yet\ngeneral approach to solving various tasks. However, this\napproach struggles with complex tasks like mathematical\nreasoning [525] and multi-hop question answering [526]. As\nan enhanced approach, prompt-based planning has been\nproposed to break down complex tasks into smaller sub-\ntasks and generate a plan of actions to accomplish the task.\nPlan ExecutorTask Planner\n(LLM)\nEnvironmentTask Result\nPlan\n(generate & refine)\nFeedback Action\nInternal External\nLLM World\n\u2026\nOthersPlanning\nFramework\nMemory Tool\nHumanFig. 16: An illustration of the formulation for prompt based\nplanning by LLMs for solving complex tasks.\n6.4.1 The Overall Framework\nIn this part, we first formulate the general planning\nparadigm of LLMs for solving complex tasks, which is\nillustrated in Figure 16.\nIn this paradigm, there are typically three components:\ntask planner ,plan executor , and environment41. Specifically,\ntask planner, which is played by LLMs, aims to generate the\nwhole plan to solve a target task. The plan can be presented\nin various forms, e.g., an action sequence in the form of\nnatural language [439] or an executable program written in\nprogramming language [443]. The LLM-based task planner\ncan be enhanced with the memory mechanism for plan\nstorage and retrieval, which is helpful for long-horizon\ntasks. Then, plan executor is responsible for executing the\nactions in the plan. It can be implemented by models like\nLLMs for textual tasks [441] or by tools like code interpreters\nfor coding tasks [450]. Furthermore, environment refers to\nwhere the plan executor carries out the actions, which can\nbe set differently according to specific tasks, e.g., the LLM\nitself [527] or an external virtual world like Minecraft [528].\nIt provides feedback about the execution result of the action to\nthe task planner, either in the form of natural language [450]\nor from other multimodal signals [446].\nFor solving a complex task, the task planner first needs to\nclearly understand the task goal and generate a reasonable\nplan based on the reasoning of LLMs (See Section 6.4.2).\nThen, the plan executor acts according to the plan in the\nenvironment, and the environment will produce feedback\nfor the task planner (See Section 6.4.3). The task planner\ncan further incorporate the feedback obtained from the\nenvironment to refine its initial plan and iteratively perform\nthe above process to get better results as the task solution\n(See Section 6.4.4).\n41. Despite the similarity with RL, our formulation decouples the\nplanning and execution phases, whereas in RL, they are typically\ninterleaved in the agent. This paradigm is defined in a general yet\nslightly loose way, and it mainly aims to help readers understand the\nkey idea underlying the planning approaches of LLMs.58\n6.4.2 Plan Generation\nPlan generation focuses on directly generating action se-\nquences by prompting LLMs. Based on the format of the\ngenerated plans, existing work can be divided into two\ngroups: text-based and code-based approaches.\nText-based Approaches. It is straightforward for LLMs to\ngenerate plans in the form of natural language. In this\napproach, LLMs are prompted to generate a sequence of\nactions for the plan executor to perform and solve the com-\nplex task. For example, Plan-and-Solve [441] adds explicit\ninstructions like \u201c devise a plan \u201d to directly prompt\nthe LLM for planning in a zero-shot manner, while Self-\nplanning [529] and DECOMP [440] add demonstrations in\nthe prompt to guide the LLM to devise a plan through ICL.\nFollowing this way, some work further considers incorpo-\nrating extra tools or models when planning. For example,\nToolFormer [80] first annotates a pre-training corpus with\npotential API calls using LLMs, and then fine-tunes LLMs\non it, so that LLMs can learn when and how to call APIs\nand incorporate the results returned by APIs during gener-\nation. HuggingGPT [444] introduces the models available in\nHuggingFace and regards LLMs as the controller to select\nsuitable models based on their descriptions and aggregate\ntheir results as the final solution.\nCode-based Approaches. Although text-based approaches\nsound intuitive, they cannot guarantee faithful execution of\nthe plan, which may lead to failure even when the plan is\nsound. To address this issue, code-based approaches have\nbeen proposed to generate more verifiable plans in the\nform of executable code in programming languages, e.g.,\nPython or PDDL. In this way, LLMs are first prompted\nto generate the program and then utilize a deterministic\nsolver to execute it. For example, Faithful CoT [442] and\nPAL [443] decompose a reasoning task into two stages: at\nthe first stage, the LLM generates a plan conditioned on the\nquery; at the second stage, a deterministic solver executes\nthe plan to derive the final answer. Furthermore, code-based\napproaches can be applied to embodied agents in a similar\nway. For example, PROGPROMPT [530] and LLM+P [531]\nfirst utilize LLMs to generate plans in the form of python\nfunctions or PDDL files, and then leverage a virtual agent\nor classical planner to solve the problem according to the\ncode-based plans.\n6.4.3 Feedback Acquisition\nAfter executing the generated plan, the environment would\nproduce the feedback signal to the LLM-based task planner,\nwhich can be used to refine its initial plan for better results.\nIn existing work, there are typically two sources of feedback\nfrom the environment, depending on their relationship with\nthe LLM-based task planner: internal ( i.e.,the LLM itself)\nand external ( e.g., tools or virtual worlds) feedback.\nInternal Feedback. The LLM itself can be utilized as a\nfeedback provider. One straightforward way is to directly\nevaluate the quality of the generated plans through prompt-\ning. For example, RAP [447] evaluate the likelihood that\neach candidate plan can lead to task success, while Tree of\nThoughts [527] proposes to vote across plans by making\ncomparisons between them. Further, LLMs can providefeedback based on the intermediate results from the plan\nexecutor. For example, Reflexion [450] utilizes LLMs to\ntransform sparse result signals ( e.g., success or failure) into\nconcrete text-based feedback ( e.g., \u201cYou should recommend\ncomedies that the user mentions in the query instead of horror\nmovies \u201d) and stores this feedback in long-term memory for\nfuture planning.\nExternal Feedback. In addition to LLMs, external objects\ncan also provide feedback signals. For example, tools like\ncode interpreters are widely used in programming tasks to\nprovide real-time error messages [450], models like stable\ndiffusion [532] can be used in multimodal tasks to provide\nvisual perception [446], and virtual worlds like Minecraft\ncan provide immersive experiences [528]. Besides, some\nwork ( e.g., Generative Agents [533]) explores multi-agent\ncollaboration in simulated environments, where each agent\nreceives feedback not only from interaction with the envi-\nronment but also from communication with other agents.\n6.4.4 Plan Refinement\nWith access to feedback from the environment, the task\nplanner can accordingly refine its current plan and itera-\ntively go through the \u201c planning \u2013 execution \u2013 refinement \u201d loop\nfor better results. In this part, we summarizes three major\nrefinement approaches in existing work.\nReasoning. The feedback data from the environment may\nnot be directly suitable to be utilized by LLMs for plan\nrefinement, e.g., containing irrelevant information or taking\na non-language form. To solve this, some work adds the\nexplicit reasoning process to extract critical information\nfrom feedback [448, 449]. For example, React [449] prompts\nLLMs with demonstrations to generate reasoning traces\nover feedback. It has been widely used in autonomous agent\nprojects, such as AutoGPT [534], which can automatically\nreason over the observed feedback to revise the initial\nplan for solving various user requests. However, these ap-\nproaches typically fix the order of reasoning and planning.\nTo support flexible switching between the two processes for\nbetter performance, ChatCoT [448] further unifies the tool-\naugmented reasoning process into a multi-turn conversation\nbetween the LLM-based task planner and the tool-based\nenvironment.\nBacktracking. Early methods mainly consider planning\nforward actions while maintaining the existing plan, thus\nlikely leading to local optimal plans based on a short-term\nevaluation. To solve this, Tree of Thoughts [527] allows back-\ntracking with search algorithms like breadth-first and depth-\nfirst search to make global planning. It refines the plan\nstep by step by backtracking to the last state in the initial\nplan and choosing the next unexplored action. Furthermore,\nsome studies [446, 535] utilize feedback signals to revise the\nentire plan. For example, DEPS [535] selects a better plan\naccording to feedback signals, while TIP [446] adds feedback\nsignals to prompts for the LLM-based planner to revise each\nstep in the initial plan.\nMemorization. In order to handle long-horizon tasks, it has\nbecome a key approach to aid plan refinement with long-\nterm memory in addition to utilizing the short-term memory of59\nLLMs through ICL. For example, Reflexion [450] stores the\nfeedback from self-reflection into the memory, so previous\nfeedback can be retrieved for plan refinement. Generative\nAgents [533] designs the memory stream mechanism as the\ncore component of agents for action planning and reflection.\nFurther, the skill library mechanism [445, 528] is proposed\nto store successful plans in the library, which can be reused\nand synthesized as complex plans for novel tasks. To imple-\nment the long-term memory mechanism, tools like vector\ndatabases ( e.g., milvus [536]) can be used to encode plans or\nfeedbacks into high-dimensional vectors for efficient storage\nand retrieval at a large scale. MemoryBank [537] further\nproposes the memory updating mechanism to allow mem-\nory forgetting and strengthening following the Ebbinghaus\nForgetting Curve theory.\n7 C APACITY AND EVALUATION\nTo examine the effectiveness and superiority of LLMs, a\nsurge of tasks and benchmarks have been proposed for\nconducting empirical ability evaluation and analysis. In this\nsection, we first introduce three types of basic ability evalu-\nation of LLMs for language generation and understanding,\nthen present several advanced ability evaluations with more\ncomplicated settings or goals, and finally discuss existing\nbenchmarks, evaluation approaches, and empirical analysis.\n7.1 Basic Ability\nIn this part, we mainly focus on three basic types of ability\nevaluation for LLMs, i.e.,language generation, knowledge\nutilization, and complex reasoning. It is noted that we do not\nintend to have complete coverage of all the related tasks, but\ninstead only focus on the most widely discussed or studied\ntasks for LLMs. Next, we introduce these tasks in detail.\n7.1.1 Language Generation\nAccording to the task definition, existing tasks about lan-\nguage generation can be roughly categorized into language\nmodeling, conditional text generation, and code synthesis\ntasks. Note that code synthesis is not a typical NLP task, we\ninclude it for discussion because it can be directly solved\nby a number of LLMs (trained on code data) in a similar\ngeneration approach as natural language text.\nLanguage Modeling. As the most fundamental ability of\nLLMs, language modeling aims to predict the next token\nbased on the previous tokens [1], which mainly focuses\non the capacity of basic language understanding and gen-\neration. For evaluating such an ability, typical language\nmodeling datasets that existing work uses include Penn\nTreebank [538], WikiText-103 [539], and the Pile [161], where\nthe metric of perplexity is commonly used for evaluating the\nmodel performance under the zero-shot setting. Empirical\nstudies [55, 93] show that LLMs bring substantial per-\nformance gains over the previous state-of-the-art methods\non these evaluation datasets. To better test the modeling\ncapacity of long-range dependencies in text, the LAMBADA\ndataset [233] has been introduced, where LLMs are required\nto predict the last word of sentences based on a paragraph of\ncontext. Then, the accuracy and perplexity of the predicted\nlast words are employed to evaluate LLMs. As shown inexisting work, the performance on the language modeling\ntasks typically follows the scaling law [30], which means\nthat scaling language models would improve the accuracy\nand reduce the perplexity.\nConditional Text Generation. As an important topic in\nlanguage generation, conditional text generation [48] fo-\ncuses on generating texts satisfying specific task demands\nbased on the given conditions, typically including machine\ntranslation [624], text summarization [548], and question\nanswering [557]. To measure the quality of the generated\ntext, automatic metrics ( e.g., Accuracy, BLEU [625] and\nROUGE [626]) and human ratings have been typically used\nfor evaluating the performance. Due to the powerful lan-\nguage generation capabilities, LLMs have achieved remark-\nable performance on existing datasets and benchmarks. For\ninstance, GPT-4 exhibits comparable performance as com-\nmercial translation products, even for the translation task of\nlanguages that are with significant linguistic distance [627].\nOn news summarization tasks ( i.e.,CNN/DM and XSUM),\nLLMs also demonstrate comparable performance with hu-\nman freelance writers [628]. Despite the rapid progress\non model capacity, there are increasing concerns on the\nfeasibility of existing automatic metrics to faithfully assess\nthe performance of LLMs in conditional text generation\ntasks [628\u2013630]. As the alternatives to automatic metrics,\nrecent studies also propose to incorporate LLMs as gener-\nation evaluators to examine the quality of the generated\ncontent [138, 631, 632]. Moreover, researchers also explore\nmore challenging language generation tasks for LLMs, such\nas structured data generation [458] and long text genera-\ntion [46, 633, 634].\nCode Synthesis. In addition to generating high-quality nat-\nural language text, existing LLMs also show strong abilities\nto generate formal language, especially computer programs\n(i.e.,code) that satisfy specific conditions, called code syn-\nthesis [635]. Unlike natural language generation, as the gen-\nerated code can be directly checked by execution with cor-\nresponding compilers or interpreters, existing work mostly\nevaluates the quality of the generated code from LLMs by\ncalculating the pass rate against the test cases, i.e.,pass@ k42.\nRecently, several code benchmarks focusing on functional\ncorrectness are proposed to assess the code synthesis abil-\nities of LLMs, such as APPS [378], HumanEval [105], and\nMBPP [208]. Typically, they consist of diverse programming\nproblems, with text specification and test cases for cor-\nrectness checking. To improve such an ability, it is key to\nfine-tuning (or pre-training) LLMs on code data, which can\neffectively adapt LLMs to code synthesis tasks [86]. In addi-\ntion, existing work has proposed new strategies to generate\ncode, e.g., sampling multiple candidate solutions [208] and\nplanning-guided decoding [636], which can be considered\nas the imitation of bug-fixing and code-planning processes\nby programmers. Impressively, LLMs have recently shown\ncompetitive performance with humans by achieving a rank-\ning of the top 28% among users on the programming contest\nplatform Codeforces [114]. Further, GitHub Copilot has been\nreleased to assist programming in coding IDEs ( e.g., Visual\n42. Given kprograms generated by the LLM, pass@ kis computed as\n1 when at least one program passes all test cases, or else 060\nTABLE 14: Representative basic and advanced abilities and corresponding representative datasets for evaluating.\nLevel Ability Task Dataset\nBasicLanguage GenerationLanguage Modeling Penn Treebank [538], WikiText-103 [539], the Pile [161], LAMBADA [233]\nConditional Text GenerationWMT\u201914,16,19,20,21,22 [540\u2013545], Flores-101 [546], DiaBLa [547],\nCNN/DailyMail [548], XSum [549], WikiLingua [550]\nOpenDialKG [551]\nCode SynthesisAPPS [378], HumanEval [105], MBPP [208], CodeContest [114], MTPB [86],\nDS-1000 [552], ODEX [553]\nKnowledge UtilizationClosed-Book QANatural Questions [554], ARC [555], TruthfulQA [556], Web Questions [557],\nTriviaQA [558], PIQA [559], LC-quad2.0 [560], GrailQA [561], KQApro [562],\nCWQ [563], MKQA [564], ScienceQA [565]\nOpen-Book QANatural Questions [554], OpenBookQA [566], ARC [555], TriviaQA [558],\nWeb Questions [557], MS MARCO [567], QASC [568], SQuAD [569],\nWikiMovies [570]\nKnowledge CompletionWikiFact [571], FB15k-237 [572], Freebase [573], WN18RR [574],\nWordNet [575], LAMA [576], YAGO3-10 [577], YAGO [578]\nComplex ReasoningKnowledge ReasoningCSQA [504], StrategyQA [185], HotpotQA [579], ARC [555], BoolQ [580],\nPIQA [559], SIQA [581], HellaSwag [582], WinoGrande [583], COPA [584],\nOpenBookQA [566], ScienceQA [565], proScript [585], ProPara [586],\nExplaGraphs [587], ProofWriter [588], EntailmentBank [589],\nProOntoQA [590]\nSymbolic ReasoningCoinFlip [33], ReverseList [33], LastLetter [33], Boolean Assignment [591],\nParity [591], Colored Object [70], Penguins in a Table [70],\nRepeat Copy [443], Object Counting [443]\nMathematical ReasoningMATH [364], GSM8k [184], SVAMP [592], MultiArith [593], ASDiv [503],\nMathQA [594], AQUA-RAT [595], MAWPS [596], DROP [597],\nNaturalProofs [598], PISA [599], miniF2F [600], ProofNet [601]\nAdvancedHuman AlignmentHonestness TruthfulQA [556], HaluEval [602]\nHelpfulness HH-RLHF [170]\nHarmlessnessHH-RLHF [170], Crows-Pairs [603]\nWinoGender [604], RealToxicityPrompts [605]\nInteraction with\nExternal EnvironmentHousehold VirtualHome [606], BEHAVIOR [607], ALFRED [608],ALFWorld [609]\nWebsite Environment WebShop [610], Mind2Web [611]\nOpen World MineRL [612], MineDojo [613]\nTool ManipulationSearch Engine HotpotQA [579], TriviaQA [558], Natural Questions [554]\nCode Executor GSM8k [184], TabMWP [614], Date Understanding [70]\nCalculator GSM8k [184], MATH [364], CARP [615]\nModel Interface GPT4Tools [616], Gorilla [617]\nData InterfaceWebQSP [618], MetaQA [619], WTQ [620]\nWikiSQL [621], TabFact [622], Spider [623]\nStudio and JetBrains IDEs), which can support a variety\nof languages including Python, JavaScript, and Java. A\nviewpoint article entitled \u201c The End of Programming \u201d [637] in\nCommunications of the ACM has discussed the impact of AI\nprogramming in the field of computer science, emphasizing\nan important shift towards the highly adaptive LLM as a\nnew atomic unit of computation.\nMajor Issues. Although LLMs have achieved splendid per-\nformance in generating human-like text, they are susceptible\nto suffering from two major issues in language generation\nas discussed below.\n\u2022Unreliable generation evaluation. With the advancement\nof language generation ability of LLMs, existing studies\nfind that the generated texts from LLMs have reached a\ncomparable quality to the reference texts on a variety of text\ngeneration tasks. However, due to the intrinsic weakness\nof existing evaluation benchmarks, there exists pronouncedinconsistency between human evaluation and automatic\nreference-based metrics [628\u2013630, 638]. For example, in\nOpenDialKG [551], ChatGPT underperforms a fine-tuned\nGPT-2 on BLEU and ROUGE-L metrics, while earning more\nfavor from human judgment [638]. Furthermore, existing\nwork argues that even human evaluation may not be robust\nenough [628, 629, 639, 640]. In some cases, it is difficult\nto achieve a high level of consensus among human an-\nnotators [629], and there is also a large gap between the\nannotation quality of crowdworkers and experts [639, 640].\nThus, how to conduct reliable evaluation for language gen-\neration tasks in the era of LLMs has become a fundamental\nyet challenging research topic. Recently, increasing research\nwork proposes to leverage LLMs to improve the evaluation\nquality of the generated texts. Specially, LLMs can be used\nto improve the evaluation quality of existing metrics. For ex-\nample, Para-Ref [641] augments various automatic metrics\nby leveraging LLMs to paraphrase existing references into61\nsemantically equivalent references with diverse expressions.\nFurther, LLMs are widely employed as the evaluators of text\ngeneration in a reference-free manner, including evaluating\na single prediction [631, 632, 642] or comparing several\ncandidates [138, 643\u2013645]. Nevertheless, LLMs may expose\nbias ( e.g., order bias or preference for LLM-generated texts\nover human-written texts) as language generation evalua-\ntors, demonstrating disparities when compared to human\nevaluation [632, 646, 647].\nUnreliable Generation Evaluation\nLLMs have been capable of generating texts with\na comparable quality to human-written texts,\nwhich however might be underestimated by au-\ntomatic reference-based metrics. As an alterna-\ntive evaluation approach, LLMs can serve as lan-\nguage generation evaluators to evaluate a single\ntext, compare multiple candidates, and improve\nexisting metrics. However, this evaluation ap-\nproach still needs more inspections and exami-\nnations in real-world tasks.\n\u2022Underperforming specialized generation . Although LLMs\nhave learned general language patterns to generate coherent\ntext, their proficiency in generation might be constrained\nwhen dealing with a specialized domain or task. For in-\nstance, a language model that has been trained on gen-\neral web articles may face challenges when generating a\nmedical report which involves many medical jargon and\nmethods. Intuitively, domain knowledge should be critical\nfor model specialization. However, it is not easy to inject\nsuch specialized knowledge into LLMs. As discussed in\nrecent analyses [47, 648], when LLMs are trained to exhibit\nsome specific ability that allows them to excel in some areas,\nthey might struggle in others. Such an issue is related to\ncatastrophic forgetting [649, 650] in training neural networks,\nwhich refers to the conflict phenomenon of integrating new\nand old knowledge. Similar cases also occur in human align-\nment of LLMs, where \u201c alignment tax \u201d [66] ( e.g., a potential\nloss in the in-context learning ability) has to be paid for\naligning to human values and needs. Moreover, due to\nthe limitations of sequence modeling architecture, LLMs\nstill face challenges in the understanding and generation\nof structured data. Consequently, they often fall behind\ntask-specific models on complex structured data tasks, such\nas knowledge-base question answering and semantic pars-\ning [458, 651]. Therefore, it is important to develop effective\nmodel specialization methods that can flexibly adapt LLMs\nto various task scenarios, meanwhile retaining the original\nabilities as possible.\nUnderperforming Specialized Generation\nLLMs may fall short in mastering generation\ntasks that require domain-specific knowledge or\ngenerating structured data. It is non-trivial to\ninject specialized knowledge into LLMs, mean-\nwhile maintaining the original abilities of LLMs.7.1.2 Knowledge Utilization\nKnowledge utilization is an important ability of intelligent\nsystems to accomplish knowledge-intensive tasks ( e.g., com-\nmonsense question answering and fact completion) based\non supporting factual evidence. Concretely, it requires LLMs\nto properly utilize the rich factual knowledge from the pre-\ntraining corpus or retrieve external data when necessary. In\nparticular, question answering (QA) and knowledge com-\npletion have been two commonly used tasks for evaluating\nthis ability. According to the test tasks (question answering\nor knowledge completion) and evaluation settings ( with or\nwithout external resources), we categorize existing knowl-\nedge utilization tasks into three types, namely closed-book\nQA, open-book QA43, and knowledge completion.\nClosed-Book QA. Closed-book QA tasks [652] test the\nacquired factual knowledge of LLMs from the pre-training\ncorpus, where LLMs should answer the question only based\non the given context without using external resources. For\nevaluating this ability, there are several datasets that can\nbe leveraged, including Natural Questions [554], Web Ques-\ntions [557], and TriviaQA [558], where the accuracy metric is\nwidely adopted. Empirical results have revealed that LLMs\ncan perform well in this setting and even match the per-\nformance of state-of-the-art open-domain QA systems [56].\nAlso, the performance of LLMs on closed-book QA tasks\nshows a scaling law pattern in terms of both model size\nand data size: scaling the parameters and training tokens\ncan increase the capacity of LLMs and help them learn (or\nmemorize) more knowledge from the pre-training data [56].\nFurther, under a similar parameter scale, LLMs with more\npre-training data relevant to the evaluated tasks would\nachieve better performance [81]. Also, the closed-book QA\nsetting provides a testbed for probing the accuracy of the\nfactual knowledge encoded by LLMs. However, as shown\nin existing work [55], LLMs might perform less well on QA\ntasks relying on fine-grained knowledge, even when it exists\nin the pre-training data.\nOpen-Book QA. Unlike closed-book QA, in open-book QA\ntasks, LLMs can extract useful evidence from the external\nknowledge base or document collections, and then answer\nthe question based on the extracted evidence [653\u2013656]. Typ-\nical open-book QA datasets ( e.g., Natural Questions [554],\nOpenBookQA [566], and SQuAD [569]) have overlap with\nclosed-book QA datasets, but they incorporate external data\nsources, e.g., Wikipedia. The metrics of accuracy and F1\nscore are widely used in open-book QA tasks for evalua-\ntion. To select relevant knowledge from external resources,\nLLMs are often paired with a text retriever (or even a\nsearch engine), which is trained independently or jointly\nwith LLMs [81, 653, 657]. Also, previous work [658\u2013660]\nhas indicated that retrievers can assist LLMs in verifying\nand rectifying the reasoning path. In evaluation, existing\nstudies mainly focus on testing how LLMs utilize the ex-\ntracted knowledge to answer the question and show that\n43. In this part, open-book QA refers to the QA tasks that require\nto extract and utilize useful information from external knowledge\nresources, as the antithesis of closed-book QA (only using the encoded\ninformation from pre-training corpus). Note that there is a dataset also\nnamed OpenBookQA [566], which follows the settings of open-book\nQA tasks by extracting and utilizing external science facts.62\n(b) Extrinsic hallucination(a) Intrinsic hallucinationExplain RLHF for LLMs.RLHF stands for \"Rights, Limitations, Harms, and Freedoms\"and is a framework for \u2026\u2026 models like LLMs (Large Language Models).Bob\u2019s wife is Amy. Bob\u2019s daughter is Cindy.Who is Cindy to Amy?Cindy is Amy\u2019s daughter-in-law.\nFig. 17: Examples of intrinsic and extrinsic hallucination for a public LLM (access date: March 19, 2023). As an example\nof intrinsic hallucination, the LLM gives a conflicting judgment about the relationship between Cindy and Amy, which\ncontradicts the input. For extrinsic hallucination, in this example, the LLM seems to have an incorrect understanding of\nthe meaning of RLHF (reinforcement learning from human feedback), though it can correctly understand the meaning of\nLLMs (in this context).\nthe retrieved evidence can largely improve the accuracy\nof the generated answers, even enabling a smaller LLM to\noutperform 10\u00d7larger ones [653, 657]. Further, open-book\nQA tasks can be also employed to evaluate the recency\nof knowledge information. Pre-training or retrieving from\noutdated knowledge resources may cause LLMs to generate\nincorrect answers for time-sensitive questions [653].\nKnowledge Completion. In knowledge completion tasks,\nLLMs might be (to some extent) considered as a knowledge\nbase [576], which can be leveraged to complete or predict the\nmissing parts of knowledge units ( e.g., knowledge triples).\nSuch tasks can probe and evaluate how much and what kind\nofknowledge LLMs have learned from the pre-training\ndata. Existing knowledge completion tasks can be roughly\ndivided into knowledge graph completion tasks ( e.g.,FB15k-\n237 [572] and WN18RR [574]) and fact completion tasks ( e.g.,\nWikiFact [571]), which aim to complete the triples from a\nknowledge graph and incomplete sentences about specific\nfacts, respectively. Empirical studies have revealed that it\nis difficult for existing LLMs to accomplish knowledge\ncompletion tasks related to specific relation types [520].\nAs shown in the evaluation results on WikiFact, LLMs\nperform well on several frequent relations that occur in\nthe pre-training data ( e.g.,currency andauthor ), while\nnot well on rare ones ( e.g.,discoverer_or_inventor\nandplace_of_birth ). Interestingly, under the same eval-\nuation settings ( e.g., in-context learning), InstructGPT ( i.e.,\ntext-davinci-002) outperforms GPT-3 in all subsets of\nWikiFact.\nMajor Issues . Although LLMs have achieved key progress\nin capturing and utilizing knowledge information, they\nsuffer from two major issues as discussed below.\n\u2022Hallucination . In generating factual texts, a challeng-\ning issue is hallucination generations [638, 661], where the\ngenerated information is either in conflict with the existing\nsource ( intrinsic hallucination ) or cannot be verified by the\navailable source ( extrinsic hallucination ), which are illustrated\nby two examples in Figure 17. Hallucination widely occurs\nin existing LLMs, even the most superior LLMs such as\nGPT-4 [46]. Furthermore, existing work shows that LLMs\nencounter difficulties in recognizing the hallucinated con-\ntent in text [602], even the powerful ChatGPT. Additionally,beyond language tasks, a recent study has shown that large\nvision-language models (LVLM) also face challenges with\nhallucination, i.e.,generating objects that are not present in\nthe accompanying images [662]. In essence, LLMs seem\nto \u201cunconsciously\u201d utilize the knowledge in task solving,\nwhich still lack an ability to accurately control the use\nof internal or external knowledge. Hallucinations would\nmislead LLMs to generate undesired outputs and mostly\ndegrade the performance, leading to potential risks when\ndeploying LLMs in real-world applications. To alleviate\nthis problem, alignment tuning strategies (as discussed in\nSection 5.2) have been widely utilized in existing work [66],\nwhich rely on tuning LLMs on high-quality data or using\nhuman feedback. Moreover, the integration of external\ntools for the provision of credible information sources can\nhelp alleviate the hallucination issue [81, 602, 659]. Another\nline of research work leverages uncertainty estimation of\nLLMs to identify hallucinations [663, 664]. For instance,\nconsidering that hallucinated facts are prone to exhibit\ninconsistency across different sampled outputs, SelfCheck-\nGPT [664] detects hallucination by measuring information\ninconsistency within sampled outputs. For the evaluation\nof the hallucination problem, a set of hallucination de-\ntection tasks have been proposed, e.g., TruthfulQA [556]\nfor detecting human falsehood mimicked by models. More\nrecently, HaluEval [602] creates a large-scale LLM-generated\nand human-annotated hallucinated samples to evaluate the\nability of language models to recognize hallucination in both\ntask-specific and general scenarios.\nHallucination\nLLMs are prone to generate untruthful informa-\ntion that either conflicts with the existing source\nor cannot be verified by the available source.\nEven the most powerful LLMs such as ChatGPT\nface great challenges in migrating the hallucina-\ntions of the generated texts. This issue can be\npartially alleviated by special approaches such as\nalignment tuning and tool utilization.\n\u2022Knowledge recency . As another major challenge, LLMs\nwould encounter difficulties when solving tasks that require63\nthe latest knowledge beyond the training data. To tackle\nthis issue, a straightforward approach is to regularly update\nLLMs with new data. However, it is very costly to fine-tune\nLLMs, and also likely to cause the catastrophic forgetting\nissue when incrementally training LLMs. Therefore, it is\nnecessary to develop efficient and effective approaches that\ncan integrate new knowledge into existing LLMs, making\nthem up-to-date. Existing studies have explored how to\nutilize the external knowledge source ( e.g., search engine)\nto complement LLMs, which can be either jointly optimized\nwith LLMs [653] or used as a plug-and-play module [659].\nFor instance, ChatGPT utilizes a retrieval plugin to access\nup-to-date information sources [665]. By incorporating the\nextracted relevant information into the context [666\u2013668],\nLLMs can acquire new factual knowledge and perform\nbetter on relevant tasks. However, such an approach seems\nto be still at a superficial level. In addition, existing studies\nalso explore editing parameters of language models to up-\ndate intrinsic knowledge [669\u2013671]. Nevertheless, previous\nwork [672] has shown that several parameter editing meth-\nods perform not well on LLMs, though they can improve\nthe performance of small language models. Therefore, it\nis still difficult to directly amend intrinsic knowledge or\ninject specific knowledge into LLMs, which remains an\nopen research problem [672]. Recently, a useful framework\nEasyEdit [673] has been released to facilitate the research of\nknowledge editing for LLMs.\nKnowledge Recency\nThe parametric knowledge of LLMs is hard to be\nupdated in a timely manner. Augmenting LLMs\nwith external knowledge sources is a practical\napproach to tackling the issue. However, how\nto effectively update knowledge within LLMs\nremains an open research problem.\n7.1.3 Complex Reasoning\nComplex reasoning refers to the ability of understanding\nand utilizing supporting evidence or logic to derive con-\nclusions or make decisions [51, 52]. According to the type\nof involved logic and evidence in the reasoning process,\nwe consider dividing existing evaluation tasks into three\nmajor categories, namely knowledge reasoning, symbolic\nreasoning, and mathematical reasoning.\nKnowledge Reasoning. The knowledge reasoning tasks\nrely on logical relations and evidence about factual\nknowledge to answer the given question. Existing work\nmainly uses specific datasets to evaluate the reasoning\ncapacity of the corresponding type of knowledge, e.g.,\nCSQA [504]/StrategyQA [185] for commonsense knowledge\nreasoning and ScienceQA [565] for science knowledge rea-\nsoning. In addition to the accuracy of the predicted results,\nexisting work [565] has also evaluated the quality of the\ngenerated reasoning process, via automatic metrics ( e.g.,\nBLEU) or human evaluation. Typically, these tasks require\nLLMs to perform step-by-step reasoning based on factual\nknowledge, until reaching the answer to the given ques-\ntion. To elicit the step-by-step reasoning ability, chain-of-\nthought (CoT) prompting strategy [33] has been proposedfor enhancing the complex reasoning capacity of LLMs.\nAs discussed in Section 6.3, CoT involves the intermediate\nreasoning steps, which can be manually created [33] or\nautomatically generated [674], into the prompts to guide\nLLMs to perform multi-step reasoning. Such a way largely\nimproves the reasoning performance of LLMs, leading to\nnew state-of-the-art results on several complex knowledge\nreasoning tasks [33, 56, 526]. Further, after reformulating\nknowledge reasoning tasks into code generation tasks, re-\nsearchers have found that the performance of LLMs can\nbe further improved [211], especially with the LLMs pre-\ntrained on code. However, due to the complexity of knowl-\nedge reasoning tasks, the performance of current LLMs still\nlags behind human results on tasks such as commonsense\nreasoning [33, 56, 675]. As a common type of mistakes, LLMs\nmight generate inaccurate intermediate steps, leading to a\nwrong final result. To address this issue, existing work has\nproposed special decoding or ensemble strategies to im-\nprove the accuracy of the whole reasoning chain [436, 437].\nSymbolic Reasoning44.The symbolic reasoning tasks\nmainly focus on manipulating the symbols in a formal rule\nsetting to fulfill some specific goal [51], where the operations\nand rules may have never been seen by LLMs during pre-\ntraining. Existing work [33, 439, 505] commonly evaluates\nLLMs on the task of last letter concatenation and coin flip,\nwhere the evaluation examples require the same reasoning\nsteps as the in-context examples (called in-domain test ) or\nmore steps (called out-of-domain test ). For an example of\nthe out-of-domain test, LLMs could only see the examples\nwith two words in context, but it requires LLMs to concate-\nnate the last letters of three or more words. Typically, the\naccuracy of the generated symbols is adopted to evaluate\nthe performance of LLMs on these tasks. Thus, LLMs need\nto understand the semantic relations among the symbolic\noperations and their composition in complex scenarios.\nHowever, under the out-of-domain setting, as LLMs have\nnot seen the complex compositions of symbolic operations\nand rules ( e.g., twice the number of operations in context\nexamples), it is hard for LLMs to capture their accurate\nmeanings. To solve this issue, existing studies incorporate\nscratchpad [591, 676] and tutor [677] strategies to help\nLLMs better manipulate symbolic operations, for generating\nlonger and more complex reasoning processes. Another\nline of research work utilizes the formal programming\nlanguage to represent the symbolic operations and rules,\nwhich requires LLMs to generate code and perform the\nreasoning process by executing it with external interpreters.\nSuch a way can decompose the complex reasoning process\ninto code synthesis and program execution for LLMs and\ninterpreters, respectively, leading to a simplified reasoning\nprocess with yet more accurate results [443].\nMathematical Reasoning. The mathematical reasoning\ntasks need to comprehensively utilize mathematical knowl-\nedge, logic, and computation for solving problems or gen-\nerating proof statements. Existing mathematical reasoning\ntasks can be mainly categorized into math problem solv-\n44. Following [33], we mainly discuss symbolic reasoning tasks spe-\ncially designed for evaluating LLMs. We do not consider symbolic\nreasoning methods in traditional NLP tasks, such as deducing logical\nrules from the knowledge graphs in KBQA.64\ning and automated theorem proving. For math problem\nsolving tasks, SVAMP [592], GSM8k [184] and MATH [364]\ndatasets are commonly used for evaluation, where LLMs\nneed to generate accurate concrete numbers or equations\nto answer the mathematical problem. As these tasks also\nrequire multi-step reasoning, the CoT prompting strategy\nhas been widely adopted for LLMs to improve the reasoning\nperformance [33]. As another practical strategy, continu-\nally pre-training LLMs on large-scale mathematical corpora\ncan largely boost their performance on mathematical rea-\nsoning tasks [35, 203, 678]. Further, since math problems\nin different languages share the same mathematical logic,\nresearchers also propose a multilingual math word problem\nbenchmark [524] to evaluate the multilingual mathematical\nreasoning capacity of LLMs. As another challenging task,\nautomated theorem proving (ATP) [598, 600, 679] requires\nthe reasoning model to strictly follow the reasoning logic\nand mathematical skills. To evaluate the performance on\nthis task, PISA [599] and miniF2F [600] are two typical ATP\ndatasets with the proof success rate as the evaluation metric.\nAs a typical approach, existing work on ATP utilizes LLMs\nto aid the search for proofs using an interactive theorem\nprover (ITP), such as Lean, Metamath, and Isabelle [680\u2013\n682]. A major limitation of ATP research is the lack of related\ncorpora in formal language. To tackle it, several studies\nutilize LLMs to convert informal statements into formal\nproofs for augmenting new data [683] or generate drafts and\nproof sketches to reduce the search space of the proofs [684].\nMajor Issues. In spite of the advancements, LLMs still have\nseveral limitations in solving complex reasoning tasks.\n\u2022Reasoning inconsistency . With improved reasoning\nstrategies ( e.g., CoT prompting), LLMs can solve some com-\nplex reasoning tasks, by performing step-by-step reasoning\nbased on the supporting logic and evidence. Despite the\neffectiveness, the reasoning inconsistency issue often occurs in\nthe decomposed reasoning process. Concretely, LLMs may\ngenerate the correct answer following an invalid reasoning\npath, or produce a wrong answer after a correct reason-\ning process [33, 442], leading to inconsistency between the\nderived answer and the reasoning process. To alleviate\nthis problem, existing work has proposed to guide the\nwhole generation process of LLMs via external tools or\nmodels [437, 451, 636], to re-check the reasoning process\nand final answer for correcting the potential errors [685\u2013687]\nor fine-tune LLMs with process-based feedback [688, 689].\nFor instance, Tree of Thoughts (ToT) [451] empowers LLMs\nto engage in the decision-making process by concurrently\nexploring and self-evaluating various reasoning paths. To\nrefine the reasoning processes, Self-Refine [685] elicits feed-\nback from LLMs on self-generated solutions, enabling the\niterative refinement of solutions based on the feedback.\nMoreover, several studies improve the consistency in the\nreasoning chain of LLMs through the integration of process-\nbased supervision during training [688, 689]. As a promis-\ning solution, recent approaches reformulate the complex\nreasoning tasks into code generation tasks, where the strict\nexecution of the generated code ensures the consistency\nbetween the reasoning process and the outcome. Also,\nit has been revealed that there might exist inconsistency\nbetween tasks with similar inputs, where small changesin the task description may cause the model to produce\ndifferent results [49, 592]. To mitigate this problem, self-\nconsistency [436] adopts the ensemble of multiple reasoning\npaths to enhance the decoding process of LLMs.\nReasoning Inconsistency\nLLMs may generate the correct answer following\nan invalid reasoning path, or produce a wrong\nanswer after a correct reasoning process, leading\nto inconsistency between the derived answer and\nthe reasoning process. The issue can be alleviated\nby fine-tuning LLMs with process-level feedback,\nusing an ensemble of diverse reasoning paths,\nand refining the reasoning process with self-\nreflection or external feedback.\n\u2022Numerical computation . For complex reasoning tasks,\nLLMs still face difficulties in the involved numerical com-\nputation, especially for the symbols that are seldom en-\ncountered during pre-training, such as arithmetic with large\nnumbers [49, 677, 690]. To tackle this issue, a direct way is\nto tune LLMs on synthesized arithmetic problems [361, 691].\nAlso, a surge of studies improve the numerical computation\nperformance by tracing intermediate calculation steps in\ntraining and inference stages [361, 676, 692], e.g., scratchpad\ntracing. In addition, existing work [80] has also incorpo-\nrated external tools ( e.g., calculator), especially for handling\narithmetic operations. More recently, ChatGPT has provided\na plugin mechanism to use external tools [665]. In this\nway, LLMs need to learn how to properly manipulate the\ntools. For this purpose, researchers have augmented the\nexamples using tools (even the LLM itself) for tuning the\nLLM [80, 693], or devised instructions and exemplars for\nin-context learning [443]. In addition to the aid of ex-\nternal tools, recent studies find that tokenizing digits into\nindividual tokens ( e.g., LLaMA and Galactica tokenizers)\nis a useful approach to enhancing the inherent arithmetic\nability of LLMs [361, 690]. One possible explanation is that\nsubword tokenization techniques can result in inconsistent\nsequences when tokenizing numbers. For instance, with\na subword tokenizer the integer 7481 may be tokenized\nas7481, while 74815 may be tokenized as 74815(the\nsame numerical substrings with different splits) [361]. As a\ncomparison, digit-based tokenization for numbers can avoid\nsuch an inconsistency, thus likely improving the numerical\ncomputation ability of LLMs.\nNumerical Computation\nLLMs face difficulties in numerical computation,\nespecially for the symbols that are seldom en-\ncountered during pre-training. In addition to us-\ning mathematical tools, tokenizing digits into in-\ndividual tokens is also an effective design choice\nfor improving the arithmetic ability of LLMs.\n7.2 Advanced Ability\nIn addition to the above basic evaluation tasks, LLMs also\nexhibit some superior abilities that require special consider-65\nations for evaluation. In this part, we discuss several rep-\nresentative advanced abilities and the corresponding eval-\nuation approaches, including human alignment, interaction\nwith the external environment, and tool manipulation. Next,\nwe discuss these advanced abilities in detail.\n7.2.1 Human Alignment\nIt is desired that LLMs could well conform to human values\nand needs, i.e.,human alignment, which is a key ability for\nthe broad use of LLMs in real-world applications.\nTo evaluate this ability, existing studies consider multiple\ncriteria for human alignment, such as helpfulness, honesty,\nand safety [46, 170, 368]. For helpfulness and honesty, adver-\nsarial question answering tasks ( e.g., TruthfulQA [556]) can\nbe utilized to examine LLM\u2019s ability in detecting possible\nfalsehood in the text [46, 81]. Furthermore, harmlessness\ncan be also evaluated by several existing benchmarks, e.g.,\nCrowS-Pairs [603] and Winogender [604]. Despite the auto-\nmatic evaluation with the above datasets, human evaluation\nis still a more direct way to effectively test the human\nalignment ability of LLMs. OpenAI invites many experts\nin domains related to AI risks to evaluate and improve the\nbehaviors of GPT-4 when encountering risky contents [46].\nIn addition, for other aspects of human alignment ( e.g.,\ntruthfulness), several studies propose to use specific instruc-\ntions and devise annotation rules to guide the annotation\nprocess [81]. Empirical studies have revealed that these\nstrategies can greatly improve the human alignment ability\nof LLMs [170]. For instance, after alignment tuning on data\ncollected through interactions with experts, the incorrect\nbehavior rate of GPT-4 can be largely reduced when it deals\nwith sensitive or disallowed prompts. In addition, high-\nquality pre-training data can reduce the effort required for\nalignment [46]. For instance, Galactica is potentially more\nharmless due to the less biased contents in the scientific\ncorpus [35].\n7.2.2 Interaction with External Environment\nIn addition to standard evaluation tasks, LLMs have the\nability to receive feedback from the external environment\nand perform actions according to the behavior instruction,\ne.g., generating action plans in natural language to manip-\nulate agents [694, 695]. Such an ability is also emergent in\nLLMs that can generate detailed and highly realistic action\nplans, while smaller models ( e.g., GPT-2) tend to generate\nshorter or meaningless plans [694].\nTo test this ability, several embodied AI environments\nand benchmarks can be used for evaluation, described\nas follows. VirtualHome [606] builds a 3D simulator for\nhousehold tasks such as cleaning and cooking, in which\nthe agent can execute natural language actions generated\nby LLMs. ALFRED [608] includes more challenging tasks\nthat require LLMs to accomplish compositional targets. BE-\nHAVIOR [607] focuses on everyday chores in simulation\nenvironments and requires LLMs to generate complex so-\nlutions, e.g., changing the internal status of objects. Apart\nfrom restricted environments such as household tasks, a\nline of research work investigates the proficiency of LLM-\nbased agents to explore open-world environments, such as\nMinecraft and the Internet [696, 697]. Voyager [697] intro-\nduces an automatic curriculum module that enables LLMsto continuously acquire new skills based on feedback from\nthe environment. GITM [696] focuses on solving various\nchallenges in Minecraft based on LLM, through task de-\ncomposition, planning, and invocation of interfaces. Based\non the generated action plans or task completions, existing\nwork either adopts the regular metrics ( e.g., executability\nand correctness of the generated action plans) [694] in the\nbenchmark or directly conducts real-world experiments and\nmeasures the success rate [698], to evaluate such ability. It\nhas been shown that LLMs are capable in interacting with\nthe external environment and generating accurate action\nplans [699]. Recently, several improvement methods have\nbeen proposed to enhance the interaction ability of LLMs,\ne.g., designing code-like prompts [530] and providing real-\nworld grounding [698].\nIn addition, recent work also explores multi-agent col-\nlaboration based on LLMs in simulated environments [533,\n700, 701]. These studies simulate human social behaviors\nby instantiating multiple LLM-based agents with observa-\ntions, planning, and memories in a sandbox environment.\nIn controlled evaluation, the abilities of generative agents\nto search, plan, and think are evaluated by humans in an\ninterview-like manner. Further, they also conduct descrip-\ntive measurements on multiple agents within a simulated\nenvironment to examine emergent social behaviors.\n7.2.3 Tool Manipulation\nWhen solving complex problems, LLMs can turn to external\ntools if they determine it is necessary. By encapsulating\navailable tools with API calls, existing work has involved\na variety of external tools, e.g., search engine [81], calcula-\ntor [80], and compiler [443], to enhance the performance of\nLLMs on several specific tasks. Recently, OpenAI has sup-\nported the use of plugins in ChatGPT [665], which can equip\nLLMs with broader capacities beyond language modeling.\nFor example, the web browser plugin enables ChatGPT\nto access fresh information. Further, incorporating third-\nparty plugins is particularly key for creating a prosperous\necosystem of applications based on LLMs.\nTo examine the ability of tool manipulation, existing\nwork mostly adopts complex reasoning tasks for evaluation,\nsuch as mathematical problem solving ( e.g., GSM8k [184]\nand SVAMP [592]) or knowledge question answering ( e.g.,\nTruthfulQA [556]), where the successful utilization of tools is\nvery important for enhancing the required skills that LLMs\nare incapable in ( e.g., numerical calculation). In this way, the\nevaluated performance on these tasks can reflect the ability\nof LLMs in tool manipulation. To teach LLMs to utilize tools,\nexisting studies add exemplars using tools in context to elicit\nLLMs [443], or fine-tune LLMs on simulated data about\ntool utilization [80, 693]. It has been found that with the\nhelp of tools, LLMs become more capable of handling the\nissues that they are not good at, e.g., equation calculation\nand answering timely questions [80, 448]. However, as\nthe number of available tools increases, the limited context\nlength of LLMs may pose challenges in describing and\ndemonstrating extensive tool APIs. To address this issue,\nexisting work retrieves the usage of relevant tools, or en-\ncoding tool information as tokens within the embedding\nspace [702\u2013704].66\nIn addition to existing tools developed by humans,\nLLMs possess the capability to make their own tools for\nspecific tasks autonomously [705]. This enables the models\nto independently explore and manipulate these self-created\ntools, thereby expanding their potential for autonomous\nexploration in solving a wide range of real-world tasks.\nSummary . The above three abilities are of great value to\nthe practical performance of LLMs: conforming to human\nvalues and preferences (human alignment), acting properly\nin real-world scenarios (interaction with the external envi-\nronment), and expanding the ability scope (tool manipu-\nlation). In addition to the above three advanced abilities,\nLLMs might also show other abilities that are specially\nrelated to some tasks ( e.g., data annotation [486]) or learning\nmechanisms ( e.g.,self-improvement [706]). It will be an open\ndirection to discover, measure and evaluate these newly\nemerging abilities, so as to better utilize and improve LLMs.\n7.3 Benchmarks and Evaluation Approaches\nIn the above, we have discussed the basic and advanced\nabilities of LLMs. Next, we will introduce existing evalua-\ntion benchmarks and approaches [733, 734].\n7.3.1 Comprehensive Evaluation Benchmarks\nRecently, several comprehensive benchmarks [70, 364, 520]\nhave been released for the evaluation of LLMs. In this\npart, we introduce several widely used benchmarks, i.e.,\nMMLU, BIG-bench, HELM, and a series of human exam\nbenchmarks.\n\u2022MMLU [364] is a versatile benchmark for large-scale\nevaluation of multi-task knowledge understanding, cover-\ning a wide range of knowledge domains from mathematics\nand computer science to humanities and social sciences. The\ndifficulties of these tasks vary from basic to advanced. As\nshown in existing work, LLMs mostly outperform small\nmodels by a substantial margin on this benchmark [35, 56,\n57, 69], which shows the scaling law in model size. More\nrecently, GPT-4 achieves a remarkable record (86.4% in 5-\nshot setting) in MMLU, which is significantly better than\nthe previous state-of-the-art models [46].\n\u2022BIG-bench [70] is a collaborative benchmark intended\nto probe existing LLMs from various aspects. It comprises\n204 tasks that encompass a broad range of topics, includ-\ning linguistics, childhood development, mathematics, com-\nmonsense reasoning, biology, physics, social bias, software\ndevelopment, and so on. By scaling the model size, LLMs\ncan even outperform the average human performance under\nthe few-shot setting on 65% of tasks in BIG-bench [56].\nConsidering the high evaluation cost of the entire bench-\nmark, a lightweight benchmark BIG-bench-Lite has been\nproposed, which contains 24 small yet diverse and challeng-\ning tasks from BIG-bench. Additionally, the BIG-bench hard\n(BBH) benchmark [365] has been proposed to concentrate\non investigating the currently unsolvable tasks of LLMs by\nselecting the challenging tasks in which LLMs exhibit infe-\nrior performance compared to humans. Since BBH becomes\nmore difficult, small models mostly achieve performance\nclose to random. As a comparison, CoT prompting can\nelicit the abilities of LLMs to perform step-by-step reasoningfor enhancing the performance, even exceeding the average\nhuman performance in BBH.\n\u2022HELM [520] is a comprehensive benchmark that cur-\nrently implements a core set of 16 scenarios and 7 categories\nof metrics. It is built on top of many prior studies, conduct-\ning a holistic evaluation of language models. As shown in\nthe experimental results of HELM, instruction tuning can\nconsistently boost the performance of LLMs in terms of\naccuracy, robustness, and fairness. Further, for reasoning\ntasks, the LLMs that have been pre-trained on the code\ncorpus show superior performance.\n\u2022Human-level test benchmarks aim to evaluate the compre-\nhensive ability of LLMs with questions designed for testing\nhumans, such as AGIEval [708], MMCU [709], M3KE [710],\nC-Eval [711] and Xiezhi [712]. These benchmarks encompass\na wide range of domains, difficulty levels, and languages\nto provide a comprehensive evaluation of LLMs\u2019 general\ncapabilities. Compared to publicly available models, models\noffering API services ( e.g.,GPT-4, ChatGPT, Claude) demon-\nstrate superior performance compared to publicly avail-\nable models on these evaluation benchmarks. As the best-\nperforming model in evaluations, GPT-4 surpasses average\nhuman performance in AGIEval [708]. However, it still lags\nbehind the top human performance on these challenging\nbenchmarks. Hence, there remains ample room for further\nenhancements in the overall abilities of LLMs, particularly\nfor publicly accessible models.\nThe above benchmarks cover a variety of mainstream\nevaluation tasks and real-world human exam questions for\nthe evaluation of LLMs. Also, there are several benchmarks\nthat focus on evaluating specific abilities of LLMs, such\nas TyDiQA [735] for multilingual knowledge utilization\nand MGSM [524] for multilingual mathematical reasoning.\nTo conduct the evaluation, one can select suitable bench-\nmarks according to specific goals. In addition, there are also\nseveral open-source evaluation frameworks for researchers\nto evaluate LLMs on existing benchmarks or extend new\ntasks for customized evaluations, such as Language Model\nEvaluation Harness [736] and OpenAI Evals [46]. Fur-\nther, some researchers also construct continuously updated\nleaderboards by aggregating representative benchmarks, to\ncompare the performance of existing LLMs, such as Open\nLLM Leaderboard [707]. The above benchmarks and leader-\nboards provide important references to demonstrate the ba-\nsic and advanced abilities of LLMs. We will give more deep\ndiscussions on pros and cons on evaluation approaches in\nSection 7.3.2.\n7.3.2 Evaluation Approaches\nAfter introducing existing benchmarks, in this part, we\nwill review existing evaluation approaches for assessing\nthe performance of LLMs. To organize our discussion, we\ncategorize LLMs into three different types: base LLMs (pre-\ntrained model checkpoints), fine-tuned LLMs (instruction or\nalignment fine-tuned model checkpoints), and specialized\nLLMs (adapted model checkpoints for some specific task\nor domain). Here, we keep both fine-tuned LLMs and\nspecialized LLMs, to distinguish the different purposes of\nLLMs: general or specific task solvers. To evaluate the three\ntypes of LLMs, we can test the LLM\u2019s performance related\nto different abilities ( e.g., basic or advanced abilities as67\nTABLE 15: A category of existing evaluation work. \u201cGeneral\u201d denotes that the evaluation focuses on an overall performance\nof multiple abilities. The evaluated abilities are not limited to the representative basic and advanced abilities mentioned in\nSection 7.1 and 7.2.\nMethod Evaluation Model Types Abilities/Domain Data Source\nBenchmarkMMLU [364] Base/Fine-tuned/Specialized General Human exam/practice\nBIG-bench [70] Base/Fine-tuned/Specialized General Human annotation\nHELM [520] Base/Fine-tuned/Specialized General Benchmark collection\nOpen LLM Leaderboard [707] Base/Fine-tuned/Specialized General Benchmark collection\nAGIEval [708] Base/Fine-tuned/Specialized General Human exam/practice\nMMCU [709] Base/Fine-tuned/Specialized General Human exam/practice\nM3KE [710] Base/Fine-tuned/Specialized General Human exam/practice\nC-Eval [711] Base/Fine-tuned/Specialized General Human exam/practice\nXiezhi [712] Base/Fine-tuned/Specialized General Human exam/practice\nOpenCompass [713] Base/Fine-tuned/Specialized General Benchmark collection\nChain-of-Thought Hub [714] Base/Fine-tuned General Benchmark collection\nKoLA [715] Base/Fine-tuned Knowledge utilization Web\nARB [716] Fine-tuned Complex reasoning Human exam/practice\nAPIBench [717] Base/Fine-tuned Tool manipulation Web\nAPIBank [718] Fine-tuned Tool manipulation Synthesis\nToolAlpaca [719] Base/Fine-tuned Tool manipulation Synthesis\nT-Bench [720] Fine-tuned Tool manipulation Synthesis\nToolBench [721] Fine-tuned Tool manipulation Synthesis\nBOLAA [722] Base/Fine-tuned Environment interaction Benchmark collection\nAgentBench [723] Base/Fine-tuned Environment interaction Human annotation/Synthesis\nHaluEval [602] Base/Fine-tuned Human alignment Human annotation/Synthesis\nPromptBench [724] Base/Fine-tuned Robustness Benchmark collection\nHumanEval [105] Base/Fine-tuned/Specialized Code synthesis Human annotation\nMultiMedQA [356] Specialized Healthcare Benchmark collection\nFLUE [725] Specialized Finance Benchmark collection\nLegalBench [726] Specialized Legal Human annotation\nHumanChatbot Arena [727] Base/Fine-tuned/Specialized Human Alignment Human annotation\nSciBench [728] Fine-tuned Complex reasoning Human exam/practice\nModelAlpacaEval [729] Fine-tuned Instruction following Synthesis\nMT-bench [727] Fine-tuned Human alignment Human annotation\nTrustGPT [730] Base/Fine-tuned Human alignment Benchmark collection\nLMExamQA [731] Base/Fine-tuned Knowledge utilization Synthesis\nChatEval [732] Base/Fine-tuned Knowledge utilization Benchmark collection\ndiscussed in Section 7.1 and 7.2). In general, there are three\nmain approaches to evaluating LLMs, namely benchmark-\nbased approach [364], human-based approach [727], and\nmodel-based approach [729]. Table 15 shows an illustration\nof the relationship among LLM type, evaluation approach,\nand tested abilities. Next, we will discuss the evaluation\napproaches for different types of LLMs.\nEvaluation of Base LLMs. Base LLMs refer to the model\ncheckpoints obtained right after pre-training. For base\nLLMs, we mainly focus on examining the basic abilities\n(Section 7.1), such as complex reasoning and knowledge\nutilization. Since most of these basic abilities can be assessed\nwith well-defined tasks, benchmark-based approaches have\nbeen widely used to evaluate base LLMs. Next, we will\nintroduce common evaluation benchmarks and evaluation\nprocedures for base LLMs.\n\u2022Common benchmarks. To evaluate base LLMs, typical\nbenchmarks are designed in the form of close-ended prob-\nlems like multiple-choice questions. These commonly used\nbenchmarks can be mainly divided into two categories:\nknowledge-oriented and reasoning-oriented benchmarks.\nKnowledge-oriented benchmarks ( e.g., MMLU [364] and C-\nEval [711]) aim to evaluate the capacity of world knowledge,\nwhile reasoning-oriented benchmarks ( e.g., GSM8K [643],\nBBH [365], and MATH [364]) focus on evaluating the ca-\npability of solving complex reasoning tasks. Further, somerecently proposed benchmarks ( e.g., OpenCompass [713])\ncombine these two types for a comprehensive comparison.\n\u2022Benchmark based evaluation procedure. To perform the\nbenchmark evaluation, each problem will first be formatted\ninto a prompt for LLMs to generate the result text. Then,\nthe generated result text will be parsed with human-written\nrules to get the predicted answer. Finally, the performance\nof LLMs can be automatically calculated using standard\nmetrics like accuracy by comparing the predicted answer\nwith the ground-truth one. The evaluation approach can be\nconducted in either the few-shot or zero-shot setting, which\nmight lead to different evaluation results or rankings. Since\nbase LLMs have not been instruction fine-tuned (with rela-\ntively weak task generalization ability), the few-shot setting\nis often more suitable for evaluation. For some complex\nreasoning tasks, CoT prompts also need to be used to fully\nexhibit the capacity during evaluation. Another note is that\nthis evaluation approach can also be applied to assess the\nabilities of fine-tuned LLMs. Actually, several leaderboards\n(e.g., Open LLM Leaderboard [707]) are built upon this\napproach, evaluating both base and fine-tuned LLMs.\nEvaluation of Fine-tuned LLMs. Fine-tuned LLMs in this\npart refer to the model checkpoints obtained after in-\nstruction tuning or alignment tuning based on pre-trained\nmodel weights45. Typically, fine-tuned LLMs will be tested\n45. In some cases, it is also called chat models .68\non various abilities ( e.g., knowledge utilization and hu-\nman alignment), and thus it is common that they are as-\nsessed with multiple evaluation approaches. In addition\nto benchmark-based evaluation, human-based and model-\nbased approaches have also been widely used to evaluate\nthe advanced abilities of fine-tuned LLMs. Next, we will\nintroduce the two evaluation methods.\n\u2022Human-based evaluation. Unlike automatic evaluation\nfor basic abilities, human evaluation typically considers\nmore factors or abilities in real-world use, such as hu-\nman alignment and tool manipulation. In this evaluation\napproach, test tasks are usually in the form of open-\nended questions, and human evaluators are invited to make\njudgments on the quality of answers generated by LLMs.\nTypically, there are two main types of scoring methods\nfor human evaluators: pairwise comparison and single-\nanswer grading. In pairwise comparison, given the same\nquestion, humans are assigned two answers from different\nmodels to determine which one is better, while in single-\nanswer grading, they only need to score a single answer\nat a time. For example, HELM [520] employs humans\nto perform single-answer grading on summarization and\ndisinformation tasks, while Chatbot Arena [727] constructs\na crowdsourcing platform that allows users to engage in\nconversations with two anonymous chat LLMs and report\npairwise comparison results.\n\u2022Model-based evaluation. Since human-based evaluation\nis both expensive and time-consuming, some work has\nproposed leveraging powerful closed-source LLMs such\nas ChatGPT and GPT-4 as a surrogate for human evalu-\nators [727, 729]. For example, AlpacaEval [729] collects a\nset of instructions and utilizes a capable LLM ( e.g., GPT-4)\nas the judge to perform pair-wise comparisons against the\nreference outputs. Furthermore, MT-bench [727] collects a\nset of multi-turn questions for evaluation and improves the\nreliability of LLM-based evaluators through methods like\nICL and CoT. Compared with human evaluators, LLMs such\nas ChatGPT and GPT-4 can achieve high agreement with\nhumans, in both small-scale handcrafted and large-scale\ncrowdsourced evaluation tasks. Despite this, these closed-\nsource LLMs are limited in access and have the potential\nrisk of data leakage. To address this, recent work [727] has\nexplored fine-tuning open-source LLMs ( e.g., Vicuna [138])\nas model evaluators using scoring data from human eval-\nuators, which has narrowed the gap with powerful closed-\nsource LLMs ( e.g., GPT-4).\nEvaluation of Specialized LLMs. Specialized LLMs refer\nto the model checkpoints specially adapted to some do-\nmains or applications like healthcare [356] and finance [737].\nAs special task solvers, specialized LLMs will be tested\nnot only on general abilities ( e.g., basic ability like com-\nplex reasoning and advanced ability like human align-\nment), but also on specific abilities related to their des-\nignated domains or applications. For this purpose, one\noften needs to construct specific benchmarks tailored for the\ntarget domains or applications. Then, these domain-specific\nbenchmarks can be combined with general benchmarks to\nconduct both comprehensive and targeted evaluation for\nspecialized LLMs. For example, MultiMedQA [356] is a\nspecific benchmark in healthcare, which includes medicalexaminations and healthcare questions. In this work [356],\nMultiMedQA has been combined with MMLU [364] to\nassess the performance of specialized LLMs for healthcare,\nsuch as Med-PaLM [356]. Similarly, FLUE [737] constructs a\nbenchmark for finance, spanning from financial sentiment\nanalysis to question answering. It has been used collab-\noratively with BBH [365] to evaluate finical LLMs like\nBloombergGPT [360].\nPros and Cons of Different Evaluation Approaches . In the\nabove, we have discussed different evaluation approaches\nto assess the abilities of LLMs. Next, we simply analyze the\npros and cons of each evaluation approach.\n\u2022Benchmark-based approach . This evaluation approach can\nleverage existing benchmarks for assessing the performance\nof LLMs. The tasks involved in these benchmarks often\ncontain sufficient test samples to measure the core abilities\n(e.g., reasoning). The whole evaluation procedure can be\n(almost) automatic, and it is convenient to carry out test\nexperiments for various base LLMs, especially useful for\nmonitoring the performance of model checkpoints during\npre-training. However, LLMs are often sensitive to the eval-\nuation settings, including the question prompts, zero-shot or\nfew-shot tests, and the answer parsing methods. Thus, one\nshould take possible influencing factors into consideration\nwhen conducting the evaluation experiments. The evalua-\ntion results should be noted with the adopted evaluation\nsettings. Another issue is the data contamination [56, 738],\ni.e.,the test data itself or relevant content has been contained\nin the pre-training corpora. This phenomenon has become\nincreasingly severe since more and more open data has been\ncollected for developing LLMs.\n\u2022Human-based approach . Human evaluation offers several\nadvantages when assessing the capabilities of LLMs to solve\nreal-world tasks. One of the key benefits is its ability to\ndirectly reflect the actual abilities of LLMs. Based on feed-\nback and experiences from real users, human evaluation\nprovides a more direct measure of LLMs\u2019 performance in\nreal-world scenarios. Further, it can conduct more flexible\nand diverse evaluation tasks based on human evaluators.\nFor instance, users can submit various queries and test the\nabilities of LLMs according to their own task cognition. It\nallows for a deep understanding of the strengths and weak-\nnesses of LLMs across different types of tasks and contexts.\nHowever, human evaluation also has inherent limitations\nthat could potentially affect its accuracy and consistency.\nFactors such as personalized tastes and varying education\nlevels among evaluators can introduce biases or even incon-\nsistencies in the evaluation process. In some cases, users\u2019\njudgments are likely to be subjective, which may not reflect\nthe true capabilities of the LLMs. Moreover, conducting\nrobust and reliable human evaluations often requires a large\nnumber of evaluators, which can be very expensive and\ntime-consuming. In addition, human evaluation is often\nnot reproducible, making it infeasible to extend existing\nevaluation results or track the progress of LLMs.\n\u2022Model-based approach . As a surrogate for human-based\napproaches, model-based approaches serve to diminish the\nreliance on human involvement, and enable more efficient\nand scalable evaluation. In addition, LLMs can provide\nmeaningful explanations for the assigned rating scores,69\nthereby enhancing the interpretability of evaluations. De-\nspite their scalability and explanability, model-based ap-\nproaches have been found to suffer from several issues, in-\ncluding position, verbosity, and self-enhancement bias [727].\nSpecially, position bias ( i.e., the order to present the re-\nsponses) refers to the fact that LLMs tend to assign high\nscores for the answers at specific positions over others,\nverbosity bias means that LLMs favor verbose answers even\nif they are short in quality compared with shorter answers,\nand self-enhancement bias indicates that LLMs often over-\nrate in their own generations. In addition, since LLMs have\nlimited capacities in solving complex reasoning problems,\nthey cannot serve as qualified evaluators for some difficult\ntasks ( e.g., mathematical reasoning). These limitations can\nbe mitigated to some extent by specific prompt engineering\nand fine-tuning strategies [727].\nTo summarize, our categorization (Table 15) of existing\nwork on LLM evaluation is mainly based on two major di-\nmensions, namely evaluation methodology and model type,\nwhich are further extended with the test abilities. There\nare some recent work [733, 734] that also has discussed\nthe categorization or taxonomies of existing work for LLM\nevaluation.\n7.4 Empirical Evaluation\nThe above evaluation benchmarks and approaches are\nmainly employed to evaluate the overall abilities of LLMs.\nIn this part, we conduct a fine-grained evaluation of the\nabilities discussed in Section 7.1 and Section 7.2. For each\nkind of ability, we select representative tasks and datasets\nfor conducting evaluation experiments to examine the cor-\nresponding performance of LLMs.\n7.4.1 Experimental Settings\nIn this part, we introduce the experimental settings for our\nevaluation.\nEvaluation Models. To conduct the evaluation, we consider\nrepresentative LLMs from open-source models to closed-\nsource API-accessing models as follows:\n\u2022Open-source models. Existing open-source models can be\ncategorized into base models and instruction-tuned models.\nBase models are only pre-trained on a large general-purpose\ncorpus with the language modeling objective, but without\nfurther supervised fine-tuning. In our evaluation, we select\nfour representative base models including LLaMA (7B) [57],\nLLaMA 2 (7B) [99], Pythia (7B and 12B) [96], and Falcon\n(7B) [747]46. Instruction-tuned models are those fine-tuned\nusing instructions ( i.e., task datasets, daily chat, or syn-\nthetic instructions). In our experiments, we select four rep-\nresentative instruction-tuned models including Vicuna (7B\nand 13B) [138], Alpaca (7B) [137], and ChatGLM (6B) [93].\nIn addition, we also include LLaMA 2-Chat (7B) [99] for\ncomparison, and it is a representative model that has been\naligned with human via instruction tuning and RLHF, based\non LLaMA 2 (7B).\n\u2022Closed-source models. In addition to the open-source\nmodels, there are also closed-source models that can only\n46. Experiments with larger models are still in schedule due to the\nlimit of computational resources.be accessed via APIs, which have gained much attention\nfrom both developers and researchers. Here, we select four\nrepresentative closed-source models including text-davinci-\n002/003 (short as Davinci002/003 ), ChatGPT, Claude, and\nClaude 2, where the first three models are developed by\nOpenAI and the other two are developed by Anthropic.\nTasks and Datasets. Next, we set up the evaluation tasks\nand datasets for the abilities discussed in Section 7.1 and\nSection 7.2. We mainly evaluate the zero-shot performance\nof LLMs on these datasets. For more complex tasks that are\nhard to be solved in the zero-shot manner ( e.g., mathemati-\ncal reasoning and tool manipulation), we mainly report the\n3-shot performance, considering the context length limit of\nopen-source models.\n\u2022Language generation. As discussed before, for language\ngeneration, we consider evaluating three kinds of tasks,\ni.e., language modeling, conditional text generation, and\ncode synthesis. Specially, we select four commonly-used\ndatasets, namely LAMBADA [233] (language modeling),\nWMT\u201922 [545] (machine translation), XSum [549] (text sum-\nmarization), and HumanEval [105] (code synthesis) for eval-\nuation. In WMT\u201922, we construct a new evaluation set\nby selecting 1000 examples for each language pair from\nthe original large-scale test set to examine the average\nperformance of LLMs in machine translation. We evaluate\nthe zero-shot performance of LLMs on these datasets, and\ncompute the accuracy of predicting words for LAMBADA,\nBLEU-4 for WMT\u201922, ROUGE-L for XSum, and pass@ 10for\nHumanEval.\n\u2022Knowledge utilization. To evaluate the ability of knowl-\nedge utilization, we select four question answering datasets\n(i.e., TriviaQA [558], Natural Questions [554], Web Ques-\ntions [557], and ARC [555]), and a fact extraction dataset,\nWikiFact [571]. We also report the zero-shot performance of\nLLMs on these datasets, and compute accuracy for ARC and\nexact match for other datasets.\n\u2022Complex reasoning. For complex reasoning, we eval-\nuate the comparison models on OpenbookQA [566], Hel-\nlaSwag [582], and SocialIQA [581] for knowledge reason-\ning; Colored Objects [70] and Penguins in the Table [70]\nfor symbolic reasoning; GSM8k [184] and MATH [364] for\nmathematical reasoning. We compute the accuracy for Open-\nbookQA, HellaSwag, and SocialIQA; solve rate for Colored\nObjects and Penguins in the Table; and accuracy for GSM8k\nand MATH. For knowledge reasoning tasks, we evaluate\nthe zero-shot performance, since they are all QA tasks that\ncan be solved in a zero-shot setting. For complex symbolic\nreasoning and mathematical reasoning tasks, we leverage\n3-shot in-context exemplars to better elicit LLMs to accom-\nplish them. Following existing work [33, 443], we also utilize\nthe chain-of-thought prompting strategy for better solving\nthe mathematical reasoning tasks.\n\u2022Human alignment. For human alignment, we select\nTruthfulQA [556] to measure whether a LLM is truth-\nful in generating answers to questions, CrowS-Pairs [603]\nand WinoGender [604] to assess the stereotypes in LLMs,\nRealToxityPrompts [605] to evaluate the extent to which\nLLMs generate toxic language, and HaluEval [602] to test\nthe ability of LLMs to recognize hallucination. As the test\nset of Real-Toxicity-Prompts is too large, we randomly70\nTABLE 16: Evaluation on the eight abilities of LLMs with specially selected tasks. The shade of the Orange and Blue\nfonts denote the performance orders of the results in closed-source and open-source models, respectively. This table will\nbe continuously updated by incorporating the results of more models.\nModelsLanguage Generation Knowledge Utilization\nLBD\u2191 WMT\u2191 XSum\u2191 HumanEval \u2191TriviaQA \u2191NaturalQ \u2191 WebQ\u2191 ARC\u2191 WikiFact \u2191\nChatGPT 55.81 36.44 21.71 79.88 54.54 21.52 17.77 93.69 29.25\nClaude 64.47 31.23 18.63 51.22 40.92 13.77 14.57 66.62 34.34\nClaude 2 45.20 12.93 19.13 78.04 54.30 21.30 21.06 79.97 35.83\nDavinci003 69.98 37.46 18.19 67.07 51.51 17.76 16.68 88.47 28.29\nDavinci002 58.85 35.11 19.15 56.70 52.11 20.47 18.45 89.23 29.15\nLLaMA 2-Chat (7B) 56.12 12.62 16.00 11.59 38.93 12.96 11.32 72.35 23.37\nVicuna (13B) 62.45 20.49 17.87 20.73 29.04 10.75 11.52 20.69 28.76\nVicuna (7B) 63.90 19.95 13.59 17.07 28.58 9.17 6.64 16.96 26.95\nAlpaca (7B) 63.35 21.52 8.74 13.41 17.14 3.24 3.00 49.75 26.05\nChatGLM (6B) 33.34 16.58 13.48 13.42 13.42 4.40 9.20 55.39 16.01\nLLaMA 2 (7B) 66.39 11.57 11.57 17.07 30.92 5.15 2.51 24.16 28.06\nLLaMA (7B) 67.68 13.84 8.77 15.24 34.62 7.92 11.12 4.88 19.78\nFalcon (7B) 66.89 4.05 10.00 10.37 28.74 10.78 8.46 4.08 23.91\nPythia (12B) 61.19 5.43 8.87 14.63 15.73 1.99 4.72 11.66 20.57\nPythia (7B) 56.96 3.68 8.23 9.15 10.16 1.77 3.74 11.03 15.75\nModelsKnowledge Reasoning Symbolic Reasoning Mathematical Reasoning Interaction with Environment\nOBQA \u2191HellaSwag \u2191 SocialIQA \u2191 C-Objects \u2191Penguins \u2191 GSM8k \u2191 MATH \u2191 ALFW \u2191 WebShop \u2191\nChatGPT 81.20 61.43 73.23 53.20 40.27 78.47 33.78 58.96 45.12/15.60\nClaude 81.80 54.95 73.23 59.95 47.65 70.81 20.18 76.87 47.72/23.00\nClaude 2 71.60 50.75 58.34 66.76 74.50 82.87 32.24 77.61 34.96/19.20\nDavinci003 74.40 62.65 69.70 64.60 61.07 57.16 17.66 65.67 64.08/32.40\nDavinci002 69.80 47.81 57.01 62.55 67.11 49.96 14.28 76.87 29.66/15.20\nLLaMA 2-Chat (7B) 45.62 74.01 43.84 43.40 38.93 9.63 2.22 11.19 24.51/5.60\nVicuna (13B) 43.65 70.51 45.97 53.55 36.91 18.50 3.72 8.96 22.74/5.00\nVicuna (7B) 43.84 69.25 46.27 44.25 36.24 14.03 3.54 1.49 6.90/1.40\nAlpaca (7B) 47.82 69.81 47.55 39.35 40.27 4.93 4.16 4.48 0.00/0.00\nChatGLM (6B) 30.42 29.27 33.18 14.05 14.09 3.41 1.10 0.00 0.00/0.00\nLLaMA 2 (7B) 44.81 74.25 41.72 43.95 35.75 10.99 2.64 8.96 0.00/0.00\nLLaMA (7B) 42.42 73.91 41.46 39.95 34.90 10.99 3.12 2.24 0.00/0.00\nFalcon (7B) 39.46 74.58 42.53 29.80 24.16 1.67 0.94 7.46 0.00/0.00\nPythia (12B) 37.02 65.45 41.53 32.40 26.17 2.88 1.96 5.22 3.68/0.60\nPythia (7B) 34.88 61.82 41.01 29.05 27.52 1.82 1.46 7.46 10.75/1.80\nModelsHuman Alignment Tool Manipulation\nTfQA\u2191 C-Pairs \u2193 WinoGender \u2191 RTP\u2193 HaluEval \u2191HotpotQA \u2191Gorilla-TH \u2191Gorilla-TF \u2191 Gorilla-HF \u2191\nChatGPT 69.16 18.60 62.50/72.50/79.17 3.07 66.64 23.80 67.20 44.53 19.36\nClaude 67.93 32.73 71.67/55.00/52.50 3.75 63.75 33.80 22.04 7.74 7.08\nClaude 2 71.11 10.67 60.00/60.00/55.83 3.20 50.63 36.4 61.29 22.19 23.67\nDavinci003 60.83 0.99 67.50/68.33/79.17 8.81 58.94 34.40 72.58 3.80 6.42\nDavinci002 53.73 7.56 72.50/70.00/64.17 10.65 59.67 26.00 2.69 1.02 1.00\nLLaMA 2-Chat (7B) 69.77 48.54 47.50/46.67/46.67 4.61 43.82 4.40 0.00 0.00 0.22\nVicuna (13B) 62.30 45.95 50.83/50.83/52.50 5.00 49.01 11.20 0.00 0.44 0.89\nVicuna (7B) 57.77 67.44 49.17/49.17/49.17 4.70 43.44 6.20 0.00 0.00 0.33\nAlpaca (7B) 46.14 65.45 53.33/51.67/53.33 4.78 44.16 11.60 0.00 0.00 0.11\nChatGLM (6B) 63.53 50.53 47.50/47.50/46.67 2.89 41.82 4.00 0.00 0.00 0.00\nLLaMA 2 (7B) 50.06 51.39 48.83/48.83/50.83 6.17 42.23 3.80 0.00 0.00 0.11\nLLaMA (7B) 47.86 67.84 54.17/52.50/51.67 5.94 14.18 1.60 0.00 0.00 0.11\nFalcon (7B) 53.24 68.04 50.00/50.83/50.00 6.71 37.41 1.00 0.00 0.00 0.00\nPythia (12B) 54.47 65.78 49.17/48.33/49.17 6.59 27.09 0.40 0.00 0.00 0.00\nPythia (7B) 50.92 64.79 51.67/49.17/50.00 13.02 25.84 0.20 0.00 0.00 0.00\nsample 10000 examples from it for evaluation. We fol-\nlow LLaMA [57] to report the zero-shot performance, and\ncompute the accuracy of identifying a claim as true for\nTruthfulQA, accuracy of recognizing biased sentences (high\nperplexity) for CrowS-Pairs, coreference resolution accuracy\n(he/she/they) for WinoGender, toxicity score for RealToxi-\ntyPrompts, and average accuracy of recognizing hallucina-\ntions for HaluEval. For TruthfulQA, we follow existing\nwork [57] that utilizes text-davinci-003 to replace humans\nfor scoring. For Crows-Pairs and WinoGender, we follow\nthe experimental settings of LLaMA [57] to compute theperplexity and coreference resolution score. For RealTox-\nityPrompts, we utilize the Perspective-API47for toxicity\nevaluation.\n\u2022Interaction with environment. To test this ability, we\nselect ALFWorld [609] and WebShop [610] for evaluation,\nwhich simulate real-world scenarios such as household\nand e-commerce environments. We follow the setting of\nReAct [449] that evaluate the 1-shot and 2-shot performance\nof LLMs on WebShop and ALFWorld respectively, and com-\n47. https://perspectiveapi.com/71\nTABLE 17: Prompt examples and their performance of ChatGPT on representative tasks. For most tasks, we compare the\nperformance for simple and complex prompts. We also present the reported performance of supervised methods. \u201cLG\u201d,\n\u201cKU\u201d, \u201cCR\u201d, \u201cSDG\u201d, \u201cIR\u201d are short for \u201clanguage generation\u201d, \u201cknowledge utilization\u201d, \u201ccomplex reasoning\u201d, \u201cstructured\ndata generation\u201d, \u201cinformation retrieval\u201d. \u201c-\u201d means there is no reported supervised result previously on this dataset.\nTasks Datasets Instructions ChatGPT Supervised\nLGTranslation WMTI want you to act as a translator. Please translate the English\nsentence into Czech.20.66\n41.40 [739]\nI want you to act as a translator. Translate the given English\nsentence into Czech, and ensure that the translated sentence is\nsemantically consistent with the given sentence. \\n Sentence:\n{source sentence } \\n Translation:21.12\nSummarization XSumPlease generate a one-sentence summary for the given document. 21.71\n42.08 [740] {document }Try your best to summarize the main content of the given\ndocument. And generate a short summary in 1 sentence for it. \\n\nSummary:23.01\nKUClosed-Book QA ARCChoose your answer to the question. {query} {options } 85.19\n92.00 [741]\nChoose a correct answer according to the given question, and output\nthe corresponding id, do not answer other content except the answer\nid.85.86\nOpen-Book QA OBQAChoose your answer to the question: {question } {choices }. You must\nonly output A, B, C, or D without any extra explanation. The answer\nis81.20\n87.20 [741]\nFollowing is a question that requires multi-step reasoning, use\nof additional common and commonsense knowledge, and rich text\ncomprehension. Choose your answer to the question: \\n Question:\nFrilled sharks and angler fish live far beneath the surface of the\nocean, which is why they are known as \\n Choices: \\n A. Deep sea\nanimals \\n B. fish \\n C. Long Sea Fish \\n D. Far Sea Animals \\n You\nmust only output A, B, C, or D without any extra explanation. The\nanswer is82.20\nFact Extraction WikiFComplete the sentence with one or a few words. 29.25\n34.20 [520]\nComplete the given sentence with one entity name in Wikipedia (MUST\nbe a noun) as short as possible, and ensure that the completed\nsentence conforms to the facts.31.21\nCRSymbolic Reasoning C-ObjectsProblem: {problem }\\n Answer: 53.20\n\u2014\nYou are an expert in reasoning problem. Here are some examples\nabout symbolic reasoning. You can use the knowledge in examples and\nsolve the last problem. You should follow the examples and generate\nthe final answer without external solution or words.66.75\nMath Word Problems GSM8kProblem: {problem }\\n Solution: Let\u2019s think step by step. 78.47\n63.20 [742] Let\u2019s use python to solve math problems. Here are three examples\nhow to do it, \\n Q: Olivia has $23. She bought five bagels for $3\neach. How much money does she have left? \\n\u2018\u2018\u2018def solution(): \\n\n\"\"\"Olivia has $23. She bought five bagels for $3 each. How\nmuch money does she have left?\"\"\" \\n money_initial = 23 \\n\nbagels = 5 \\n bagel_cost = 3 \\n money_spent = bagels *\nbagel_cost \\n money_left = money_initial - money_spent \\n\nresult = money_left \\n return result\u2018\u2018\u2018 \\n ...... \\n How about\nthis question? \\n Q:79.30\nSDGCode Synthesis HumanEval I want you act as a code completer. Given a code snippet, your\nobjective is to complete the code and ensure that it can achieve\nthe described functionality.79.88 48.20 [743]\nText-to-SQL Spider ### Complete sqlite SQL query only and with no explanation. \\n\n#\\n### Sqlite SQL tables, with their properties: \\n#\\n{table}\\n#\n{foreign_key }\\n#\\n###{question }\\n SELECT70.10 84.10 [744]\nIRRecommendation MovieLens I\u2019ve watched the following movies in the past in order: \\n\n{user_his_text } \\n\\n Now there are {recall_budget }candidate movies\nthat I can watch next: \\n{candidate_text_order } \\n Please rank\nthese {recall_budget }movies by measuring the possibilities that I\nwould like to watch next most, according to my watching history.\nPlease think step by step. \\n Note that my most recently watched\nmovie is {recent_item }. Please show me your ranking results with\norder numbers. Split your output with line break. You MUST rank the\ngiven candidate movies. You can not generate movies that are not in\nthe given candidate list.48.80 76.25 [745]\nConversational\nRecommenda-\ntionReDial Recommend 10 items that are consistent with user preference. The\nrecommendation list can contain items that the dialog mentioned\nbefore. The format of the recommendation list is: no. title (year).\nDon\u2019t mention anything other than the title of items in your\nrecommendation list17.20 25.60 [746]72\npute success rate for ALFWorld and average score/success rate\nfor WebShop. Further, we also follow ReAct [449] to reduce\nthe length of the input prompt and utilize line break as the\nEOS token.\n\u2022Tool manipulation. For tool manipulation, we consider\ntwo kinds of tools including search engine and model in-\nterfaces. Therefore, we adopt two tool manipulation bench-\nmarks, i.e., HotpotQA [579] and Gorilla [617]. HotpotQA\nrequires LLMs to use search engine to retrieve documents\nfrom the web, and Gorilla to invoke model APIs from\nthree hubs of TorchHub, TensorHub and HuggingFace. We\ncompute exact match for HotpotQA and accuracy for Gorilla.\nFor HotpotQA, we follow ReAct [449] to report the 3-shot\nperformance. For Gorilla, we follow the code released by its\npaper [617], and evaluate the zero-shot performance.\nImplementation Details. For each task and dataset, we\nevaluate the compared LLMs using the same prompts and\nresults parsing method provided by existing work ( i.e.,\nTruthfulQA, HotPotQA, Gorilla, HaluEval) or designed ac-\ncording to our empirical experience ( i.e., TriviaQA, Nat-\nural Questions, Web Questions, ARC, WikiFact, GSM8k,\nMATH, C-Objects, Penguins, LAMBADA, WMT\u201922, XSum,\nHumanEval, CrowS-Pairs, WinoGender, RealToxityPrompt).\nSpecifically, all the experiments about closed-source models\nare based on invoking their official APIs, while for open-\nsource models, we utilize their publicly available code and\nmodel parameters, and perform the inference on 8 A800-\n80G GPUs. For TriviaQA, OpenbookQA, HellaSwag, and\nSocialIQA, we experiment on the development set since the\ntest set is not publicly released. While for other datasets,\nwe experiment on the test set. To reproduce our experi-\nments, we also publicly release our experimental code and\ndata in https://github.com/RUCAIBox/LLMSurvey/tree/\nmain/Experiments.\n7.4.2 Results Analysis and Findings\nWe report the experimental results in Table 16, and analyze\nthe results in the following.\nAnalysis of Closed-Source Models. We summarize our\nanalysis and findings of the four closed-source models ( i.e.,\nChatGPT, Claude, Davinci003 and Davinci002) as follows:\n\u2022These five closed-source models achieve promising results\nas general-purpose task solvers, in which ChatGPT mostly per-\nforms the best. ChatGPT, Claude, Claude 2, Davinci003 and\nDavinci002 perform well in most of tasks, including com-\nplex tasks ( e.g., GSM8k), which have shown great potential\nto be general-purpose task solvers. Among them, ChatGPT\nexhibits a more superior model capacity on the evaluation\ntasks, winning the most across all tasks. In some evaluation\ntasks, the performance gap between ChatGPT and other\nclosed-source models is very large, especially for complex\ntasks e.g.,78.47 (ChatGPT) v.s.49.96 (Davinci002) on GSM8k,\nand 79.88 (ChatGPT) v.s.51.22 (Claude) on HumanEval.\n\u2022Claude 2, ChatGPT and Davinci003 perform better on inter-\naction with environment and tool manipulation tasks. On the two\nevaluation tasks, Claude 2, ChatGPT and Davinci003, per-\nform better than other models by a large margin, e.g., 36.40\n(Claude 2) v.s.26.00 (Davinci002) on HotpotQA, 44.53 (Chat-\nGPT) v.s.7.74 (Claude) on Gorilla-TF, and 72.58 (Davinci003)\nv.s.22.04 (Claude) on Gorilla-TH. A possible reason is thatthese three models have been specially optimized towards\nthese advanced abilities, e.g., supporting the use of external\nplugins.\n\u2022All the comparison models perform not well on very diffi-\ncult reasoning tasks. On MATH and HotpotQA, all models\n(including ChatGPT) perform not well. The two tasks are\nvery difficult to solve, requiring accurate understanding of\ncomplex mathematical knowledge and performing multi-\nhop reasoning across documents, respectively. Further, these\nmodels also have a relatively weak performance on machine\ntranslation task (WMT). A possible reason is that WMT also\ncontains many evaluation examples in minor languages,\nwhich might not be well covered in the pre-training data\nof these LLMs.\nAnalysis of Open-Source Models. Next, we continue to\nshow our analysis and findings about eight open-source\nmodels ( i.e., LLaMA 2-Chat, Vicuna, Alpaca, ChatGLM,\nLLaMA 2, LLaMA, Pythia and Falcon) as follows:\n\u2022Instruction-tuned models mostly perform better than the\nbase models. Among all the compared open-source methods,\nthe instruction-tuned models ( i.e.,LLaMA 2-Chat, Vicuna,\nAlpaca and ChatGLM) mostly perform better than non-\ninstruction-tuned models ( i.e., LLaMA 2, LLaMA, Pythia\nand Falcon). It indicates that instruction tuning is generally\ncapable of improving the few-shot or zero-shot ability of\nLLMs in solving various tasks. However, after instruction\ntuning, Vicuna (7B) and Alpaca (7B) suffer from perfor-\nmance degradations on LAMBADA, a language modeling\ntask. The reason may be that the instruction data mainly\nfocuses on enabling LLMs to follow human instructions,\nwhich is not always useful for the general language gen-\neration task.\n\u2022These small-sized open-source models perform not well on\nmathematical reasoning, interaction with environment, and tool\nmanipulation tasks. On the tasks of mathematical reasoning,\ninteraction with environment and tool manipulation, all\nthese evaluated open-source models perform not well, in-\ncluding instruction-tuned ones. A possible reason is that the\ninstruction data for fine-tuning these models is not specif-\nically designed for these tasks. In addition, these closed-\nsource models may have limited model capacities due to\nsmall model sizes.\n\u2022The top-performing model varies on different human align-\nment tasks. For different human alignment tasks, we can see\nthat these models achieve inconsistent performance rank-\nings. For example, LLaMA 2-Chat (7B) performs the best\namong the compared open-source models on TruthfulQA,\nwhile Vicuna (13B) performs the best on CrowS-Pairs. A\npossible reason is that these tasks are designed with spe-\ncific purposes for evaluating different aspects of human\nalignment, and these models exhibit varied performance\non different tasks, even for the variants of the same model\n(e.g., Pythia (7B) and Pythia (12B)). More experiments and\nanalysis on human alignment evaluation are needed to\nreveal more detailed findings.\n\u2022As a more recently released model, LLaMA 2 (7B) overall\nachieves a good performance, especially on complex reasoning\ntasks. For complex reasoning tasks, LLaMA 2 (7B) mostly\nperforms better than other base models, e.g., 43.95 (LLaMA\n2 (7B)) v.s. 29.80 (Falcon (7B)) in C-Objects. For other73\ntasks ( e.g., language generation and knowledge utilization),\nLLaMA 2 (7B) can also achieve comparable performance\nas the best-performing base models. It has used more data\nfor pre-training ( i.e.,about 2 trillion tokens), which mainly\ncontributes to the excellent performance. Furthermore, it\nalso conducts a more robust data cleaning process.\n\u2022Scaling the open-source modes can improve the performance\nconsistently. By comparing the performance of Vicuna (7B)\nand Vicuna (13B), Pythia (7B) and Pythia (13B), we can see\nthat the models with larger scales mostly perform better\nthan smaller ones on these evaluation tasks, indicating the\neffectiveness of scaling up the model size. Across different\ntasks, scaling model is more beneficial for more complex\ntasks ( e.g.,symbolic and mathematical reasoning), where the\nlarger models mostly outperform smaller ones in a large\nmargin.\nThe readers should be note that these findings about\nopen-source language models are limited to the model sizes.\nWe will continually update this part by including the results\nof larger versions of these models, and also call for the\nsupport of computational resources for more experiments.\n8 A PPLICATIONS\nIn this section, we briefly review the recent progress on the\napplications of LLMs in two aspects, namely the impact to\nresearch community and representative domains. Figure 18\nshows a content organization of this section48.\n8.1 LLM for Research Community\nAs LLMs have revolutionized the way how we develop\nAI algorithms, it poses significant impact on the research\ncommunity. In this part, we briefly review the advances that\nled by LLMs for several representative research directions.\n8.1.1 LLM for Classic NLP Tasks\nAs pre-trained language models ( e.g.,BERT) have originated\nin the field of NLP , the technical advances of language\nmodels has an important impact on the research of NLP . In\nthis part, we discuss the application of LLMs on five kinds\nof classic NLP tasks, including word-level, sentence-level,\nsequence tagging, relation extraction, and text generation\ntasks, which had been the foundation of many existing NLP\nsystems and applications. Note that we do not intend to\ncomprehensively cover all NLP tasks, but instead try to\nanalyze the impact of LLMs for fundamental NLP research\nthrough the basic tasks. We also omit the discussion of sev-\neral tasks ( e.g.,language modeling) that have been discussed\nearly in this survey.\nWord/Sentence-level Tasks. As long-standing NLP tasks,\nword-level ( e.g., word clustering [748] and sense disam-\nbiguation [749]) and sentence-level tasks (sentence match-\ning [750] and sentiment classification [751]) have been\nwidely studied in the literature and applied in real-world\nplatforms. To solve these tasks, the key is to accurately\nunderstand the semantic information about the words or\n48. Note that we don\u2019t aim to cover all the related research directions\nor domains, but instead demonstrating the use or impact of LLMs via\nthese selected examples.sentences. As rich high-quality labeled data about these\ntasks has been accumulated so far, existing work [23, 39]\nfinds that small language models can achieve very good\nperformance by fine-tuning on it. Recent studies [55, 752]\nhave also tested the performance of LLMs on these tasks,\nshowing that LLMs can also perform well via in-context\nlearning (with very few examples). Whereas, as small mod-\nels can be specially optimized on these tasks to learn the\nspecific task requirement and domain knowledge, full-data\nfine-tuned small models can mostly outperform LLMs using\nin-context learning on several classic tasks [753, 754], e.g.,\nsemantic matching and sentiment analysis.\nSequence Tagging. The sequence tagging tasks, e.g., named\nentity recognition (NER) [755] and part-of-speech (POS)\ntagging [756], are also fundamental tasks. Typically, such\ntasks require assigning each token in the input sequence a\nproper semantic category label, e.g., the classic B-I-O ( Be-\nginning ,Inside and Outside ) tagging scheme for NER tasks.\nIn the era of deep learning, early efforts [757, 758] mainly\nintegrate the learned sequence representations ( e.g., using\nCNN, LSTM, and BERT) into the classic conditional random\nfield model (CRF), which performs the tagging task based\non structural prediction. Recently, researchers have tested\nthe performance of LLMs in sequence tagging tasks, but ob-\nserved that LLMs still face challenges in solving them using\nin-context learning [753], especially for special categories\nwith ambiguous or rare names, e.g., the \u201cMISC\u201d ( miscella-\nneous entity ) and \u201cORG\u201d ( organization ) classes. A possible\nreason is that LLMs may misunderstand the meanings of\nthese classes in the human-annotated dataset, making it\ndifficult to accurately understand their semantics according\nto the instruction and limited examples in the context.\nInformation Extraction. The information extraction task\nfocuses on automatically extracting useful structured infor-\nmation from unstructured text data, such as relation extrac-\ntion [759] and event extraction [760], which is also a crucial\ntask relating to many NLP applications. Typically, previous\nstudies formulate this task as a text classification task or\na sequential labeling task. As information extraction often\nneeds to accurately understand and process complex se-\nmantic relations (multiple relations within one sentence), in-\ncontext learning with LLMs typically underperform state-\nof-the-art full-data fine-tuning methods [761, 762]. Whereas,\nit is shown that enabling collaboration between LLMs and\nsmall models can further boost the performance of specific\ntasks [762, 763]. In addition, a recent study [425] also reveals\nthat LLMs can achieve competitive zero-shot performance\nfor information extraction with a two-stage workflow, mak-\ning this approach attractive in future applications.\nText Generation. Text generation tasks, e.g., machine trans-\nlation [624] and automatic summarization [548], are long-\nstanding NLP tasks that have been widely studied, and\nthere have been a number of deployed products and sys-\ntems based on fine-tuned small models [311, 764]. Since the\npre-training of LLMs is established on text prediction, they\nexhibit strong language generation abilities as commercial\nproducts [627] and humans [628], with the help of proper\nprompts [765, 766]. Additionally, LLMs are flexible to effec-\ntively handle special requirement in real-world application74\nLLM for \nApplicationResearch \nDirections\nSpecific DomainsScientific \nResearchFinance Law Education HealthcareLLM for EvaluationLLM -based AgentKG Enhanced LLMMultimodal LLMs\u2022Vision -Language Alignment Pre -Training\n\u2022Visual Instruction Tuning\n\u2022Evaluation of  MLLM\n\u2022Retrieval -augmented LLM\n\u2022Synergy Augmented LLM\n\u2022Components: Memory/Planning/Execution\n\u2022Single/Multi -agent based Application\n\u2022Score/Language -based Evaluation\n\u2022Instruction Design, Multiple Feedback s, Debate Agent\n\u2022Meta -EvaluationLLM for IRLLM for Classic NLP Tasks\nLLM for Recommendation\u2022LLM as Recommendation Model\n\u2022LLM -enhanced Recommendation Models\n\u2022LLM as Recommendation Simulator\u2022LLM as IR Model\n\u2022LLM -Enhanced IR Models\u2022Word/Sentence -level Tasks\n\u2022Sequence Tagging\n\u2022Information Extraction\n\u2022Text Generation\nClassic Scenarios\nEnhanced Capabilities\nNew Scenarios\nFig. 18: The applications of LLMs in representative research directions and downstream domains.\nscenarios, e.g., document-level translation [767], and also\nenable natural language interaction with users to further\nimprove the generation quality [768]. Despite the above\nsuccess, recent work also reveals that LLMs are hard to well\naddress the generation tasks about low-resource languages\nand domains, e.g., Marathi-to-English translation [769], due\nto their unbalanced training data across different languages.\nSummary . Based on the above discussion, we summarize\nthe suggestions, and future direction about the use of LLMs\nin classic NLP tasks as follows:\n\u2022Suggestions: LLMs and small models have their own\nmerits in different aspects: LLMs are can provide unified\nsolutions to various NLP tasks and achieve competitive\nperformance (especially in the zero/few-shot setting), while\nsmall models are economical to develop and can be specially\ntuned according to target tasks, which can achieve good\nperformance with sufficient high-quality labeled data [753,\n754, 770, 771]. In applications, one can make suitable choices\nbased on the actual needs, comprehensively considering\nflexibility, data availability, training compute, and efficiency.\n\u2022Future direction: Despite the excellent general capac-\nities, LLMs still cannot effectively process the NLP tasks\nin low-resource domains, e.g., minor language translation.\nTo tackle such tasks, it needs to develop effective ap-\nproaches to injecting necessary task information or domain-\nspecific knowledge into LLMs, either through fine-tuning\nor prompting. In addition, it is still challenging for LLMs to\nhandle complex semantic relations in classic NLP tasks ( e.g.,\nnested entity extraction), which is worth more exploration\nfrom the underlying working mechanism of LLMs. It is also\npromising to combine LLMs and fine-tuned small language\nmodels for complementing with each other in solving com-\nplex cases of classic NLP tasks [772]. Another promising di-\nrection is to conduct human-machine collaborative research\n(e.g., conversational translation [768]) on NLP tasks, sinceLLMs can effectively understand human instructions and\nmake meaningful responses.\n8.1.2 LLM for Information Retrieval\nThe goal of information retrieval (IR) systems is to assist\nusers in discovering ideal information resources (typically\ndocuments) and mitigating the information overload issue.\nTypically, contemporary IR systems adopt a retrieve-then-\nrerank pipeline framework [54]. Within this framework,\nthe retriever initially retrieves relevant information from a\nlarge-scale corpus, and the reranker subsequently performs\nmulti-stage ranking procedure to acquire the most relevant\ninformation [773]. Since the advent of LLMs has significant\nimpact on the way of information access, we discuss how\nit advances the development of IR from two main aspects,\nnamely LLMs as IR models and LLM-enhanced IR models.\nLLMs as IR Models. Existing IR models can be overall\ncategorized into sparse models (relying on term-based lex-\nical similarity) and dense models (relying on embedding\nbased semantic similarity) [740]. Specially, dense models\nare mainly implemented by fine-tuned PLMs ( e.g., BERT).\nCompared to PLMs, LLMs have more strong model capac-\nities in capturing text semantics, thus having the potential\nto improve existing dense IR models. However, due to the\nhigh overhead of LLMs, the majority of studies concentrate\non employing LLMs as rerankers, aiming to refine the rank-\ning of retrieved candidates. To achieve this, recent efforts\noften formulate special instructions that enable LLMs to\nperform reranking on a small set of provided candidate\ndocuments. Typically, such an approach does not necessitate\nmodel training, and achieve promising results compared\nwith well-trained reranking methods [774, 775]. Specially,\nthe LLM-based reranking approach can be implemented\nin different ways by zero-shot or few-shot instruction, in-\ncluding pointwise ( estimating the relevance scores for query-\ndocument pairs ) [776], pairwise ( determining the relevance order75\nof two documents ) [775], or listwise ranking ( sorting a subset of\ncandidate documents ) [777]. The essence of these methods lies\nin the special design of instructions for text reranking, such\nas sliding window strategy for document lists [774, 778],\nsetwise selection prompting [779], fine-grained relevance la-\nbels incorporation [780], and pairwise comparison prompt-\ning [775]. In addition, recent efforts employ LLMs to gen-\nerate intermediate texts ( e.g., URLs) as retrieval results us-\ning few-shot demonstrations [781]. To further enhance the\nmodel performance, LLMs can be specially fine-tuned as\nbackbones for reranking [782, 783] or retrieval (including\ndense retrieval [54] and model-based retrieval [784, 785]),\nsimilar to the fine-tuning process for traditional PLM-based\nIR models [782]. However, fine-tuning LLMs as IR models\nentails considerable expenses given the huge parameter\nscale of LLMs.\nLLM-Enhanced IR Models. As another major research\ndirection, LLMs can be employed to improve existing IR\nmodels ( e.g., small models). A common challenge faced\nby existing IR models is the lack of relevant judgment\nannotation [786, 787]. To tackle this problem, LLMs can be\ninstructed to annotate positive or negative documents for\na given query [788], or to generate corresponding queries\nbased on a set of documents in the corpus by referring to a\nfew demonstrations [789, 790]. In addition to training data\naugmentation, LLM has the potential to improve existing\nIR models by refining the search-oriented informativeness\nof both queries and documents. In IR systems, the in-\nput queries may be constrained by a user\u2019s cognitive and\ncultural competency, making it challenging to accurately\nexpress the real intent, and irrelevant content present in\ndocuments can also impact the relevance evaluation with\nthe query. As a solution, LLM can be utilized to rewrite the\nquery for enhancing the understanding of the query intent\nand incorporating additional knowledge into the query\nthrough well-designed instructions. The rewritten query\ncan take the form of an improved version of the original\nquery [791], a document in the corpus that related to the\nquery [792], or an expansion of the query that concatenated\nwith a pseudo generated document [793]. In addition, docu-\nments can also be expanded with queries that are generated\nbased on the original documents using LLMs for context\nextension [794].\nRemaining Issues. In this part, we further discuss several\nimportant issues to apply LLMs to improve IR systems.\nFirst, though LLMs are capable of being as general-purpose\ntask solvers, they are not directly well suited for existing\nIR systems: they require high overhead for inference [774,\n782], have limitations in modeling long texts or document\nlists [778], and need special adaptation ( e.g., instruction\ntuning) to perform the text ranking task [795]. Therefore,\nmore systematic approaches to adapt LLMs for modern IR\nsystems should be investigated, to leverage their benefits\nand meanwhile overcome these limitations. Secondly, the\nadvent of LLMs sheds lights on the development of new\ninformation seeking ways ( e.g., New Bing). It is meaningful\nto explore how to reshape the architecture and paradigm\nof IR by integrating the LLMs\u2019 capacities and the merits\nof existing IR systems [796]. Thirdly, existing work mainlyfocuses on text retrieval tasks, lacking a comprehensive\nconsideration of multimodal information sources. As will\nbe discussed in Section 8.1.4, multimodal large language\nmodels [797] are also widely studied, making it feasible to\ndevelop more powerful multimedia retrieval systems.\n8.1.3 LLM for Recommender Systems\nUnlike IR systems that analyze user search queries to\nretrieve relevant documents, recommender systems (RS)\naim to capture the underlying user preference and pro-\nvide appropriate information resources to users [798\u2013801].\nTypically, existing studies train a recommendation model\n(either classic or deep learning model) by fitting it over\nthe user\u2019s logged data ( e.g., click data) [745, 802]. However,\nthese models often suffer from a series of technical issues,\ne.g., cold-start recommendation, domain transfer, and poor\nexplainability. Recently, LLMs have demonstrated the po-\ntential to alleviate these issues of recommendation mod-\nels [357, 803, 804], due to the strong capacities of domain\ngeneralization and language generation. In this part, we\nbriefly review the recent progress of LLMs in recommender\nsystems, from the following three aspects, namely LLMs as\nrecommendation models, LLM-enhanced recommendation\nmodels, and LLMs as recommendation simulators.\nLLMs as Recommendation Models. With specific methods\nor mechanisms, LLMs can be adapted to serve as recom-\nmendation models. Existing work along this line can be\ngenerally divided into two main categories. First, some\nmethods prompt LLMs for completing the recommendation\ntask in a zero-shot paradigm ( i.e.,without parameter tun-\ning) [805, 806]. A series of prompt engineering methods like\nrecency-focused and in-context learning are introduced to\nimprove recommendation performance as well as alleviate\nthe potential model biases [807, 808]. Second, another cat-\negory of studies aim to specialize LLMs for personalized\nrecommendation through instruction tuning [357, 809]. Spe-\ncially, high-quality instruction data is key to adapt LLMs\nto the recommendation tasks, which can be constructed\nbased on user-item interactions with heuristic templates. To\nfurther improve the instruction diversity, InstructRec [357]\nemploys self-instruct technique to simulate large amounts of\npotential user instructions in various scenarios like product\nsearch and personalized recommendations. In addition to\nrepresenting each item by its text description, there is also\ngrowing attention on extending LLM\u2019s vocabulary with\nsemantic identifiers in recommender systems [810, 811], to\nincorporate collaborative semantics into LLMs.\nLLM-enhanced Recommendation Models. In addition to\ninstructing LLMs to directly provide recommendations, re-\nsearchers also propose leveraging the universal knowledge\nencoded in LLMs to improve traditional recommender sys-\ntems. Existing approaches in this line can be divided into\nthree main categories. The first category employs LLMs to\ninfer users\u2019 potential intention from their historical interac-\ntion data. Furthermore, traditional recommendation/search\nmodels employ the inferred intentions to improve the re-\ntrieval of relevant items [812, 813]. Additionally, several\nstudies explore the use of LLMs as feature encoders. They\nemploy LLMs to encode the side information of items and76\nusers ( e.g., item\u2019s descriptions and user\u2019s reviews), thus de-\nriving more informative representations of users and items.\nThese representations are then fed into traditional recom-\nmender systems as augmented input [814, 815]. As an-\nother alternative approach, several studies [816, 817] adopt\na distillation-like way to transfer LLM\u2019s capacities ( e.g.,\nsemantic encoding) to improve traditional recommenders\n(i.e.,small models). Specially, they align the hidden states\nof LLMs and traditional recommendation models via joint\ntraining. After training, since only the enhanced small\nmodel will be deployed online, it can avoid the huge over-\nhead of LLMs in online service.\nLLM as Recommendation Simulator. Inspired by the recent\nsuccess of autonomous AI agents [818], LLMs have been\nalso utilized to develop recommendation simulators [819,\n820] (exemplified by RecAgent [819]), showing great po-\ntential to simulate user real behaviors in recommender\nsystems [819, 821, 822]. Specifically, to make personalized\nsimulation, an agent will be equipped with a profiling\nmodule that encompasses relevant identity information.\nThen, a memory module is introduced to store agents\u2019 past\ninteraction experiences. During the process of simulation,\nagents are further prompted to conduct self-reflection based\non their past experiences, to capture their underlying user\npreference. Most of existing recommendation simulators are\nconducted in a user-oriented way, without explicitly mod-\neling the items in the interaction process. To address this,\nAgentCF [821] models both users and items as agents, and\nfurther facilitates collaborative reflections to simulate user-\nitem interactions, so as to capturing the two-sided relations\nbetween users and items.\nRemaining Issues. Despite these efforts, there are still\nseveral challenges to address when applying LLMs in\nrecommender systems. First, existing studies have shown\nthat LLM-based recommendation models in zero/few-shot\nsettings tend to perform worse than traditional ID-based\nrecommenders [806, 807]. This indicates that LLMs might\nlack an understanding of personalized user behaviors and\ndomain-specific collaborative semantics. Although instruc-\ntion tuning alleviates this issue to some extent [357, 809],\nit can\u2019t fully reduce the semantic gap between LLMs and\nrecommender systems, and also suffers from high tuning\ncosts. Furthermore, recommender systems prioritize min-\nimizing inference latency to enhance users\u2019 experience in\nlow-resourced environments ( e.g., phones), which poses a\nchallenge to LLMs\u2019 inference speed as well as memory\noverhead. Therefore, it is important to explore improvement\ntechniques, such as efficient tuning and quantization meth-\nods, to deploy LLMs efficiently and effectively in real-world\nrecommender systems. In addition, existing LLMs have\nlimited capacities in long context modeling, make it difficult\nto process the huge amount of user-item interaction data.\nImproved context length extension and context information\nutilization approaches should be developed to improve the\nmodeling capacities of LLMs in long interaction sequences.\n8.1.4 Multimodal Large Language Model\nIn existing literature [823, 824], multimodal models mainly\nrefer to the models that can process and integrate informa-\ntion of various modalities ( e.g., text, image, and audio) frominput, and further produce corresponding output in certain\nmodalities. In this part, we mainly focus on the multimodal\nextension of LLMs by enabling the information modeling\nof non-textual modalities, especially the vision modality,\ncalled multimodal large language models (MLLMs) [797]49. To\nstart our discussion, we specify the input to be text-image\npairs and the output to be text responses. Similar discus-\nsions can be made for other modalities, e.g., language-audio\nmodels [825], which is beyond our scope here. In essence,\nMLLMs are developed by adapting the information from\nother modalities to the text modality, so as to leverage the\nexcellent model capacities of LLMs that are learned based on\nworld text. Typically, a MLLM comprises an image encoder\nfor image encoding and a LLM for text generation, associ-\nated by a connection module that aligns vision and language\nrepresentations. During generation, the image is first split\ninto patches, and then transformed into patch embeddings\nby the image encoder and the connection module, to derive\na visual representation that can be understood by the LLM.\nSubsequently, the patch embeddings and text embeddings\nare concatenated, and fed into the MLLM, allowing the\nlanguage model to generate the response autoregressively.\nIn the following, we will discuss the training, evaluation,\nand key points to develop capable MLLMs.\nTraining Process. The training process of the MLLM in-\ncludes two major stages: vision-language alignment pre-\ntraining and visual instruction tuning.\n\u2022Vision-language alignment pre-training. To develop\nMLLMs, existing work mostly initializes the vision encoder\nand the LLM with pre-trained models [149, 150, 826]. These\nmodels retain excellent vision and language capacities, but\nspan different semantic spaces. Thus, the goal of vision-\nlanguage alignment pre-training ( i.e.,the first-stage training)\nis to align the vision encoder and the LLM through end-to-\nend training on large-scale image-text pairs [827, 828]. How-\never, directly tuning these two models on image-text pairs\nmay cause the degradation of the original representation ca-\npacities. To improve the alignment performance, it is crucial\nto design effective training strategies and select appropriate\npre-training data [829, 830]. Existing work mainly employs\nthe following strategies for cross-modality alignment: (1) if\nthe number of image-text pairs is not sufficiently large ( e.g.,\nless than 1M), it is often suggested to only update the\nconnection module [831]; (2) if the training data includes\nhigh-quality text corpora [832] or image-text pairs with\nfine-grained annotations [833], fine-tuning the LLM can be\nconducted to boost the performance; (3) if the number of\nimage-text pairs is very large ( e.g., about 1B), fine-tuning\nthe vision encoder is also plausible [829, 830], but the benefit\nremains further verification.\n\u2022Visual instruction tuning. After vision-language pre-\ntraining, the second-stage training, i.e., visual instruction\ntuning, aims to improve the instruction-following and task-\nsolving abilities of MLLMs. Generally, the input of vi-\nsual instruction tuning consists of an image and a task\ndescription, and the task is to generate a corresponding\n49. In existing work, large vision language models (LVLMs) [662] are\nalso used to term such bimodal models that are developed based on\nLLMs. We use the naming of MLLMs in this part due to its wide use in\nexisting literature.77\ntext output. To boost the performance, high-quality visual\ninstruction data is key to eliciting and enhancing the abil-\nities of MLLMs. Therefore, most studies are dedicated to\nconstructing various visual instruction datasets. As the basic\napproaches, early studies construct visual instructions by\ndistilling from GPT-4 [149] or reformulating vision-language\ntask datasets [151]. To enhance the quality of instruction\ndata, recent work further proposes improved strategies by\nincreasing the instruction diversity [834], incorporating fine-\ngrained information ( e.g., coordinate of objects) into the\ninstruction [833], or synthesizing complex visual reasoning\ninstructions [835].\nEvaluation of MLLM. After introducing the approaches to\ndeveloping MLLMs, we further discuss how to effectively\nassess the multimodal capabilities of MLLMs from the fol-\nlowing three aspects.\n\u2022Evaluation perspectives. The evaluation tasks for MLLMs\ncan be categorized into two main types: perception and\ncognition tasks. Specifically, perception tasks aim to assess the\nmodel\u2019s abilities in understanding the basic semantics of the\nimage content, while cognition tasks evaluate models with\nmore complex tasks that require reasoning based on per-\nception results. The perception ability is typically evaluated\nthrough classification tasks about attributes of image ( e.g.,\ntopic and style) and object ( e.g.,existence and color) or OCR-\nrelated tasks, based on existing datasets or new datasets\nderived from existing images with annotations by humans\nor LLMs [836\u2013839]. A notable perception issue is hallucina-\ntion [840], where the model\u2019s responses contain inconsistent\ncontent with the image. Among existing studies about hallu-\ncination in MLLMs [834, 841, 842], object hallucination [843]\nhas received much research attention. To conduct a stable,\nrobust evaluation of object hallucination, POPE [844] pro-\nposes a polling-based object probing approach for convert-\ning object recognition into a series of binary questions, and\nthe results indicate that current MLLMs often struggle with\nobject hallucination. Cognition tasks, on the other hand, re-\nquire MLLMs to perform reasoning based on image percep-\ntion. A common reasoning task is visual question answering\n(VQA), where models answer questions about images that\ndemand reasoning about spatial relationships [845], general\nknowledge [846], or scene text [847]. To fully explore the\ncapabilities of MLLMs, HallusionBench [848] collects 200\nsophisticated visual dependent or supplement questions, on\nwhich even the most advanced MLLMs like LLaVA-1.5 [831]\nand GPT-4V [133] fail to achieve good performance.\n\u2022Evaluation paradigms. The responses of MLLMs can\nbe evaluated either in a closed-ended or an open-ended\nmanner. Traditional multimodal tasks often rely on a closed-\nended evaluation framework, where the assessment is based\non the exact match between the model\u2019s response and the\nground-truth answer. Examples include the VQA score [849]\nfor visual question answering tasks and the CIDEr [850]\nscore for captioning tasks. However, MLLMs generate re-\nsponses in an open-ended way, which may contain the\ncorrect answer but not exactly match the ground-truth per-\nfectly. This discrepancy can lead to the underestimation of\nthe model\u2019s performance in previous evaluation paradigms.\nTo address this issue, recent approaches have incorporated\nhumans or LLMs as evaluators [829]. For instance, MM-Bench [838] employs ChatGPT to align the model responses\nwith the most relevant option in a set of multiple-choice\nquestions. Similarly, LLaVA [851] utilizes GPT-4 for eval-\nuating MLLMs\u2019 output, where GPT-4 takes the generated\nimage captions and object bounding boxes as visual inputs\nfor assessment. Such open-ended evaluation methods can\nimprove assessment accuracy while incurring higher costs\ndue to the involvement of humans or LLMs.\n\u2022Evaluation benchmarks. To facilitate a more thorough\nevaluation of MLLMs, various benchmarks have been devel-\noped. Part of them collect existing vision-language tasks for\ncomprehensive evaluation. For instance, LVLM-eHub [852]\naggregates 47 existing text-related visual tasks to assess\nsix distinct capabilities of MLLMs, and Reform-Eval [853]\ntakes this a step further by standardizing questions from\nexisting benchmarks into a uniform format and discusses\nhow the backbone models influence MLLMs\u2019 performance.\nIn addition to incorporating existing tasks, several work\nalso derives new questions annotated by humans or with\nthe help of LLMs. MME [839] creates a dataset by pair-\ning images from public sources with manually-collected\ntext instructions for perception and cognition evaluations.\nMMBench [838] transforms these instructions into multiple-\nchoice questions and introduces CircularEval to ensure\nevaluation consistency. SEED-Bench [854] further considers\ntemporal understanding tasks and enlarges the evaluation\nscale to 19K multiple-choice questions with the assistance of\nLLMs. MM-Vet [855] presents more complex tasks to assess\nthe integrated multimodal capabilities of MLLMs. It starts\nby defining six essential multimodal abilities and then cre-\nates intricate questions by combining multiple abilities. In\nsummary, the above benchmarks collectively contribute to\nthe comprehensive evaluation and improved development\nof MLLMs.\nKey Points for Improving MLLMs. To develop capable\nMLLMs, we continue to discuss three key points to improve\nthe model capacities, from the perspectives of instruction\ndata, training strategy, and safety and alignment.\n\u2022Visual instruction data . Extensive work [831, 856] has\nempirically found that both quantity and quality of visual\ninstructions have an important impact on model perfor-\nmance of MLLMs. One basic way to construct visual in-\nstructions is to leverage the exceptional capability of LLMs\nto synthesize instructions based on text descriptions of\nimages [851]. To further enhance the quality of instructions,\none can construct fine-grained visual instructions with the\nhelp of human annotation [833, 857] or synthesize more\ncomplex data through carefully-designed prompts [835].\nDespite the effectiveness of the above LLM-based ap-\nproaches, one primary question emerges as to whether a\nLLM ( i.e.,text generation model without training on any\nimages) possesses the ability to generate sufficiently good\nvisual instructions solely based on verbalized visual infor-\nmation ( e.g., captions and coordinates). Specially, existing\nwork has also revealed that visual instructions generated\nby LLMs sometimes contain misinterpretations about the\nvisual information, e.g.,object hallucination [844]. Therefore,\nit is crucial to design effective verification methods to con-\ntrol the quality of instruction data generated by LLMs [835].\nFurthermore, it still needs more investigation about what78\nmakes good visual instructions and how visual instructions\nelicit specific multimodal abilities in MLLMs.\n\u2022Model training. Different from LLMs, MLLMs are not\ntrained from scratch, but instead developed based on pre-\ntrained language and vision models. Existing work em-\nploys a typical two-stage approach for training MLLMs,\ni.e.,vision-language alignment pre-training and visual in-\nstruction tuning. In essence, existing MLLMs aim to (1) pre-\nserve the inherent capabilities and parametric knowledge\nof LLMs as possible, and meanwhile (2) effectively adapt\nto multimodal tasks by leveraging the pre-trained LLMs\nand visual encoders. To achieve the above two goals, two\ntypical training strategies are often employed for visual\ninstruction tuning, either only optimizing the connection\nmodule [151] or fine-tuning both the connector module\nand LLM component [851]. As we can see, the former\ncan reserve the original capacities of LLMs but likely have\na weak an adaptation performance, while the latter can\nfully adapt to multimodal tasks but suffer from the loss of\noriginal capacities of LLMs. More efforts should be made to\ninvestigate how to effectively balance the two aspects, so as\nto achieving improved multimodal capacities. In addition,\nexisting MLLMs are still overly dependent on the capacities\nof LLMs, which pose the limits on many multimodal tasks\n(e.g., space positioning). It will be meaningful to explore\nimproved training approaches of language models, so that\nmultimodal information can be also utilized in this process.\n\u2022Safety and alignment. Safety and alignment has been\nwidely discussed in LLMs, which aim to regulate the behav-\niors of models by technical approaches [66]. This topic is also\nimportant to MLLMs. Even a highly advanced MLLM ( e.g.,\nGPT-4V [133]) can be susceptible to safety issues. For exam-\nple, GPT-4V might occasionally exhibit factual inaccuracies\nand baseless inferences about images. In some cases, it may\neven generate harmful content targeting specific individuals\nor groups [133]. Furthermore, open-sourced MLLMs are\nalso prone to generate hallucinated response [844] and can\nbe easily manipulated to produce harmful content [858].\nTo address the aforementioned issues, some studies collect\nspecialized visual instructions to mitigate the problem of\nhallucination [834]. Another alternative approach is to train\na revision model to rectify hallucinated response generated\nby MLLMs in a post-hoc way [859]. Additionally, aligning\nMLLMs with RLHF can also assist MLLMs in generating\nresponses with improved factuality [860]. Despite these\nefforts, existing alignment techniques for MLLMs mainly\nconcentrate on several specific aspects ( e.g., hallucination),\nlacking a comprehensive consideration of alignment criteria.\nMore efforts should be made to promote the research of\nsafety and alignment for MLLMs.\n8.1.5 KG-Enhanced LLM\nDespite the excellent capacities, LLMs often suffer from\nchallenges on knowledge-intensive tasks, such as the po-\ntential to generate hallucinated content [602] and the lack of\ndomain-specific knowledge [861]. As a promising solution,\nknowledge graphs (KGs), which store enormous knowledge\nin the triple format, i.e.,\u27e8head entity, relation, tail entity\u27e9, can\nbe utilized to enhance the task performance of LLMs by pro-\nviding precise and necessary knowledge. Generally, knowl-\nedge enhanced approaches can be expanded into otherforms of structured data ( e.g., tables and databases) [862],\nwhile we limit our discussion to the integration of KG for\nimproving LLMs, which are detailed in two aspects, namely\nretrieval-augmented LLM and synergy-augmented LLM.\nRetrieval-Augmented LLM. Due to the huge amount of\nfact records in a KG, existing work typically adopts a\nretrieval model to first obtain a relatively small subgraph\nfrom KG, and then leverages it to enhance LLMs by en-\nriching the relevant knowledge. Before the advent of LLMs,\nthe retrieved subgraphs are often supplemented into train-\ning data, injecting knowledge information into PLMs via\nparameter learning [863\u2013865]. In contrast, to leverage the\nretrieved knowledge, LLMs mainly incorporate it as part of\nthe prompt, without parameter update. To implement this\napproach, there are two main technical problems, i.e.,how\nto retrieve relevant knowledge from KGs and how to make\nbetter use of the structured data by LLMs. For the first issue\n(i.e.,retrieving relevant knowledge), a typical approach is\nto train a small language model ( e.g., RoBERTa) to iden-\ntify question-related fact triples [866]. To further improve\nthe retrieval performance, several studies also propose an\niterative reading-then-reasoning framework, enabling the\nLLM to interact with the KG multiple times and acquire the\nrequired knowledge in a more accurate way [458]. For the\nsecond issue ( i.e.,utilizing retrieved knowledge), a straight-\nforward approach is to serialize the retrieved subgraph\nand craft specific prompts to include it as the input of\nLLMs [471, 651]. However, due to the loss of structured\ninformation in knowledge serialization, LLMs cannot fully\ncapture the structural semantics conveyed by original KGs.\nTo address this issue, several model-based approaches train\na specialized language model ( e.g., T5) to transform the\nsubgraph into the natural language text [867]. To guarantee\nthe transformation accuracy, it relies on sufficient training\npairs (often unsupervised constructed) [868] and excellent\nmodel capability [869].\nSynergy-Augmented LLM. To solve complex tasks ( e.g.,\nmulti-hop question answering [656]), it often requires LLMs\nto query a KG multiple times, following a systematic solu-\ntion plan. We call such a multi-turn interaction approach to\nenhancing LLM synergy-augmented LLM . To better synergize\nthe LLM and KG in a complementary manner, recent studies\npropose to decompose the complex task into multiple sub-\ngoals and iteratively solve each one by leveraging the nec-\nessary knowledge from KG [458, 870, 871]. In this process,\nthe LLM can be regarded as an autonomous agent (detailed\nin Section 8.1.6), which automatically generates the plan\nand executes it through interaction with the KG environ-\nment [870]. Specially, the mainstream approaches typically\nstart by enumerating the candidates using the available\nknowledge information at the current step, and then retrieve\nthe most appropriate candidates for the next step according\nto the question [870, 871]. By iterating the above two steps,\nLLMs can gradually collect relevant evidence [870, 871], and\nfinally approach the correct solution. Despite the effective-\nness, enumeration of the candidates over the KG would lead\nto a vast search space [872]. To address it, StructGPT [458]\nproposes a more efficient way to access knowledge infor-\nmation using the specialized interfaces for KGs. Specifically,79\nit carefully designs the specialized interfaces according to\nthe common data operations on KG ( e.g., relation extraction\nand triple extraction), to ensure efficient and accurate data\nextraction. In this way, LLMs can be instructed to better\nmanipulate and process the structural information of KGs,\nthus achieving improved task performance.\nFuture Directions. Besides the above approaches, there\nare several promising directions for KG-enhanced LLM\nremaining underexplored. First, due to the variety of struc-\ntured data, it is still difficult for LLMs to directly leverage\nvarious kinds of knowledge sources, e.g., domain-specific\nKGs. Therefore, it is essential to explore the unified way\nto manipulate and utilize different knowledge sources by\nLLMs. As a potential solution, it is promising to develop\neffective approaches to help LLMs comprehend and make\nuse of the access interfaces provided by specific knowledge\nsources to acquire precise knowledge [458], while more ef-\nforts should be made to investigate how to adapt to the data\nvariety in a cost-effective way. Second, with the evolution of\nreal-world information, the knowledge stored in LLMs may\nbecome outdated or incorrect. It is necessary to explore how\nto synchronize the updated knowledge into LLMs through\na cost-effective manner [873, 874]. Third, it is promising to\ninvestigate the use of factual information from KG to align\nLLMs in generating more faithful content [875, 876], which\ncan help reduce the hallucination of LLMs.\nIn addition to exploring KG-enhanced LLMs, it is also\nmeaningful to leverage LLMs to improve the tasks on the\nKG side ( i.e.,LLM4KG) [861, 877]. A typical example is that\nLLMs can help supplement or construct the KG. We omit\nthe discussion of this part, since it is beyond our scope.\n8.1.6 LLM-based Agent\nThe research on agents in AI aims to develop entities that\ncan perceive the environment, make decisions, and take\nactions to achieve specific goals [878]. However, traditional\nagents are often limited to heuristic rules or specific environ-\nments, which constrain their generalization to open-domain\nscenarios [879]. Given that LLMs possess excellent capacities\nin solving complex tasks, they have rapidly emerged as\npromising solutions for serving as the core computation\nunit of agents [818]. In this part, we will first introduce\nthe framework for LLM-based agents and then discuss their\napplications.\nOverall Framework. Next, we first detail the key compo-\nnents of an LLM-based agent and then present the typical\nworkflow.\n\u2022Components. Typically, there are three main com-\nponents in an LLM-based agent: memory ,planning50, and\nexecution . Specifically, the memory component aims to store\nthe information perceived from the environment and can\nbe utilized to support decision-making. In particular, LLM-\nbased agents usually maintain information in both short-\nterm memory and long-term memory with the operations\nof reading and writing. Short-term memory usually refers\nto the internal context window of LLMs ( i.e.,input), where\n50. Section 6.4 introduces planning as a utilization approach for\nLLMs, while in this section, we describe its utilization as a functional\ncomponent in LLM-based agents.LLMs can read and write through actions like reason-\ning [880]. While long-term memory can be mapped to the\nexternal storage like vector databases [537], where LLMs\ncan read through retrieval and write with reflection [686].\nSpecially, profiles are usually implemented with long-term\nmemory, which is an important feature for an agent that\nspecifies its role and function [818]. The planning component\nis responsible for generating the action plan based on the in-\nformation from the memory component. In data format, the\nplan usually takes the form of text-based instructions [441]\nor code-based programs [443]. To generate it, LLM-based\nagents will first propose several candidates and then select\na more suitable one among them [436]. The initial plan\ncan be further refined with execution feedback from the\nenvironment [528]. The execution component is in charge\nof carrying out the plan from the planning component,\nwhich can be fulfilled by the internal LLM [441] or external\ntools [880].\n\u2022Workflow. With the three components mentioned\nabove, a typical workflow of an LLM-based agent is as\nfollows. First, it receives information from the environment\nand writes it into short-term memory. Then, the agent\nprocesses the newly received information in the short-term\nmemory. Such a process can be enhanced with information\nretrieved from long-term memory. Subsequently, the plan-\nning component utilizes the processed information from\nshort-term memory to generate the next plan. Finally, the\nexecution component carries out the plan generated from\nthe planning component, which can be further assisted with\nexternal tools. By repeating the aforementioned process, the\nLLM-based agent can autonomously adjust its behavior in\nresponse to feedback from the environment and ultimately\nachieve its goal. Once LLM-based agents receive user re-\nquests or are assigned goals, they follow the above work-\nflow to accomplish tasks through multi-turn interactions\nwith the environment.\nTo summarize, in an LLM-based agent, the LLM serves\nas the core computation unit and is equipped with compo-\nnents including memory ,planning , and execution . These com-\nponents are integrated in a systematic way under the control\nof the LLM during interactions with the environment. For\nmore details, the readers might refer to the comprehensive\nsurvey for LLM-based AI agents [818].\nApplications. Recently, LLM-based agents have shown\ngreat potential in autonomously solving complex tasks,\nmaking it feasible to rapidly develop capable applications\nfor specific domains or tasks. In this section, we will discuss\nthe applications in single-agent and multi-agent scenarios.\n\u2022Single-agent based applications. Applications based on\na single-agent mode mainly aim to develop capable task\nsolvers that can autonomously complete user requests. A\nlarge number of single-agent projects have been developed,\nwhich focus on general-purpose task solving. As a rep-\nresentative project, AutoGPT [534] empowers LLMs with\nlong/short-term memory management and external tools\nlike search engines. In order to autonomously address a\nuser request, AutoGPT understands the request with knowl-\nedge from its memory and actions like reasoning, decom-\nposes it into a detailed plan, executes the plan step-by-\nstep with the assistance of tools, and refines the rest plan80\nbased on feedback from the environment. Such an iterative\nprocess continues until the user request is successfully re-\nsolved. Other similar projects include GPT-Engineer [881]\nand XAgent [882]. In addition, there is also some work that\naims to develop autonomous agents for specific domains,\nsuch as WebGPT [81] for the web-browsing environment,\nProgPrompt [530] for the real-life environment, and Voy-\nager [697] for the Minecraft environment.\n\u2022Multi-agent based applications. Different from single-\nagent systems where agents work independently, multi-\nagent systems work in collaboration to unleash collective\nintelligence. Typically, multiple agents can be instantiated\nfrom the same or different LLMs, each with their respective\nroles and functions. According to the coordinating strategies\namong these agents, multi-agent systems can be divided\ninto two categories: cooperation-based and competition-\nbased. In the cooperation-based mode, to share information\nand seek collaborative actions among agents, various com-\nmunication protocols have been proposed, including free-\nform dialogue [883], structured document [884], and data\nembedding [885]. Based on the communication protocol,\nagents can be effectively organized for downstream appli-\ncations, such as software engineering [884], user behavior\nanalysis [819, 821], and society simulation [533]. In the\ncompetition-based mode, debate serves as one of the pop-\nular communication protocols to foster divergent thinking\nand elicit valuable external feedback among agents. Such a\nway is beneficial for domains that demand precise decision-\nmaking and accurate responses, such as mathematical rea-\nsoning [886] and evaluation [732].\nRemaining Issues. Despite the huge success, there are still\nseveral issues that limit the development and applications\nof LLM-based agents. First, with the explosive growth of the\nmodel scale, the efficiency of LLM-based agents, including\nboth the time and memory overhead, becomes an important\nissue for large-scale deployment, especially for multi-agent\nsystems with numerous instances of LLMs. Second, with the\nscaling of the number of LLM-based agents, more effective\nand efficient communication protocols and architectures are\nrequired to support the increased complexity of coordina-\ntion among agents. Furthermore, building capable agents\nposes technical challenges for the capacities of LLMs like\ninstruction following and long text modeling. Since existing\nLLMs are not specially optimized for instantiating agents,\nmost public-sourced LLMs like LLaMA cannot effectively\nfacilitate the development of agents. Therefore, it is crucial\nto develop capable, specialized models to serve as the core\ncomputation unit of agents.\n8.1.7 LLM for Evaluation\nWhile human evaluation can generally offer reliable quality\nassessment, it is also often hindered by high annotation\ncosts, significant time requirements, and annotation incon-\nsistencies [887]. In contrast, automatic evaluation can be\nemployed as a scalable alternative to human evaluation.\nTraditional automatic evaluations have relied on reference-\nbased metrics ( e.g., BLEU and ROUGE). Recently, with\nthe emergence of LLMs as general task solvers highlights\ntheir potential as automatic evaluators [647, 727], making it\npromising to conduct LLM based evaluation. In the follow-ing part, we will introduce the recent progress on LLM for\nevaluation, including evaluation formats, methods, meta-\nevaluation, and the remaining issues.\nEvaluation Formats. Depending on the type of evaluation\noutcome, the evaluation format can be categorized into\nscore-based evaluation and language-based evaluation . Score-\nbased evaluation employs measurable metrics to assign\nquality scores ( e.g., ratings or rankings) for evaluated texts.\nA prevalent way is to conduct pairwise comparison, where\nLLMs are used to determine the partial order relation of\ncandidate texts following specific guidelines [354, 647, 727],\nwhich greatly simplifies the evaluation task. However, it\nmay face the inefficiency issue when scaling up the number\nof candidates [727]. When high-quality reference texts are\navailable during evaluation, LLMs can be instructed to score\ntexts under the guidance provided by references [716, 727,\n728]. On the other hand, language-based evaluation focuses\non generating critiques and suggestions, offering qualitative\nexplanation beyond simple quantitative scoring [371, 888\u2013\n890]. It is particularly useful for gathering language feed-\nback signals for human alignment tuning [371, 888]. Fur-\nthermore, it can evolve into a multi-turn interaction frame-\nwork, where LLM-based evaluators provide natural lan-\nguage feedback to existing solutions from task solvers [891].\nThis framework evaluates the ability of LLMs to leverage\nlanguage feedback for refining self-generated solutions.\nEvaluation Methods. A common method for LLM-based\nevaluation involves prompting LLMs with specific instruc-\ntions. To further improve the quality of LLM-based eval-\nuation, recent work proposes to prompt LLMs with varied\ncontexts to generate diverse evaluation feedback. These con-\ntexts vary in aspects such as the candidate order [647, 727],\nevaluation perspectives [892, 893] ( e.g., relevance, clarity,\noriginality), and evaluation explanation [647]. The gener-\nated multiple evaluation feedbacks are then aggregated to\nproduce a final evaluation result, which makes the evalua-\ntion process less prone to biases from individual feedback\nand allows for a more thorough evaluation by covering\na wider range of evaluation aspects. To further improve\nthe quality of the single-model evaluation, recent studies\nalso develop multi-agent collaboration frameworks [893\u2013\n895] or fine-tune LLMs as specified evaluators [371, 888\u2013\n890, 896]. In a multi-model collaboration mode, different\nLLMs evaluate the candidates by engaging in discussions\nto align preferences and reach a consensus [894, 895]. This\nmethod helps reduce the potential biases in individual\nmodels through the consensus reached by multiple agents.\nAnother approach to improving single-model evaluation\nis to specialize LLMs as scores or critics through fine-\ntuning [371, 888\u2013890, 896]. This process involves creating\ndatasets annotated with preferences and feedback from\nhumans or proficient LLMs. These datasets are then used to\ntrain evaluation-oriented models, enabling them to generate\npairwise preference or language feedback. The specialized\nLLM evaluators demonstrate competitive performance with\nfewer parameters [889, 890, 896].\nMeta-Evaluation. To effectively assess the quality of\nLLM-based evaluators, meta-evaluation benchmarks have\nbeen introduced, for gauging the agreement with human81\npreferences and the fairness of the evaluations made by\nLLMs [647, 727, 893, 897, 898]. As a representative bench-\nmark, MT-Bench [727] evaluates the agreement between\nLLMs and human judgments, demonstrating that GPT-4\naligns closely with human preferences in no-tie compar-\nisons on 80 multi-turn questions. In addition, to address\npotential biases arising from subjective human evaluations,\nLLMBar [897] manually designs outputs that are objectively\nworse but superficially appealing, which could mislead\nevaluators. The evaluation results reveal that even the most\nadvanced LLMs still fall short of human-level evaluation in\nthe challenging setting.\nRemaining Issues. As discussed in Section 7.1.1, recent\nstudies demonstrate that LLM-based evaluators expose\nmultiple types of bias, such as order bias, self-preference\nbias, and length bias [647, 727]. Although some biases can\nbe mitigated through methods like multi-path ensemble or\nmulti-agent collaboration, they remain inherent to LLM-\nbased evaluators. Consequently, addressing these biases\nintrinsically within the models continues to be an a chal-\nlenging issue. In addition, recent work has revealed that\nLLMs may be incapable of understanding the self-generated\ncontent, exhibiting a weaker understanding capacity com-\npared to their generation capabilities [899]. Even the most\nadvanced LLMs still struggle identifying their reasoning or\nfactual errors without external feedback [900, 901]. Conse-\nquently, current LLM-based evaluators might not be ade-\nquate for evaluating top-tier LLMs or complex tasks. This\nunderscores the importance of improvement approaches\nfor LLM-based evaluators, especially for evaluating capable\nLLMs and complex tasks demanding sophisticated reason-\ning, planning, and domain-specific knowledge.\n8.2 LLM for Specific Domains\nIn this part, we discuss the applications of LLMs on several\nrepresentative domains, including healthcare, education,\nlaw, finance, and scientific research assistance.\nHealthcare is a vital application field closely related to\nhuman life. Ever since the advent of ChatGPT, a number of\nstudies have applied ChatGPT or other LLMs to the medical\ndomain. It has been shown that LLMs are capable of han-\ndling a variety of healthcare tasks, e.g., biology information\nextraction [763], medical advice consultation [902], mental\nhealth analysis [903], and report simplification [904]. As\nthe major technical approach, researchers typically design\nspecific prompts or instructions to guide LLMs to perform a\nwide range of medical tasks. To further harness the power\nof LLMs in the healthcare domain, researchers propose to\ndevelop healthcare-related LLMs [356, 905, 906]. Specifically,\nthe Med-PaLM models [356, 905] achieves expert-level per-\nformance on the United States Medical Licensing Exami-\nnation (USMLE), and earns greater approval from physi-\ncians in answering consumer\u2019s medical questions. However,\nLLMs may fabricate medical misinformation [904, 907],\ne.g., misinterpreting medical terms and suggesting advice\ninconsistent with medical guidelines. In addition, it would\nalso raise privacy concerns to upload the health information\nof patients [763] into a commercial server that support the\nLLM.Education is also an important application domain where\nLLMs potentially exert significant influence. Existing work\nhas found that LLMs can achieve student-level performance\non standardized tests [46] in a variety of subjects of math-\nematics ( e.g., physics, computer science) on both multiple-\nchoice and free-response problems. In addition, empirical\nstudies have shown that LLMs can serve as writing or read-\ning assistant for education [908, 909]. A recent study [909]\nreveals that ChatGPT is capable of generating logically\nconsistent answers across disciplines, balancing both depth\nand breadth. Another quantitative analysis [908] shows that\nstudents utilizing ChatGPT (either keeping or refining the\nresults from LLMs as their own answers) perform better\nthan average students in some courses from the computer\nsecurity field. Recently, several perspective papers [910, 911]\nalso explore various application scenarios of LLMs in class-\nroom teaching, such as teacher-student collaboration, per-\nsonalized learning, and assessment automation. However,\nthe application of LLMs in education may lead to a series\nof practical issues, e.g., plagiarism, potential bias in AI-\ngenerated content, overreliance on LLMs, and inequitable\naccess for non-English speaking individuals [912].\nLaw is a specialized domain that is built on professional\ndomain knowledge. Recently, a number of studies have ap-\nplied LLMs to solve various legal tasks, e.g., legal document\nanalysis [913], legal judgment prediction [914], and legal\ndocument writing [915]. A recent study [916] has found\nthat LLMs exhibit powerful abilities of legal interpretation\nand reasoning. Moreover, the latest GPT-4 model achieves\na top 10% score in a simulated bar exam compared with\nhuman test-takers [46]. To further improve the performance\nof LLMs in the law domain, specially designed legal prompt\nengineering are employed to yield advanced performance\nin long legal document comprehension and complex legal\nreasoning [917, 918]. To summarize the progress, LLMs can\nact as helpful assistants to legal profession. Despite the\nprogress, the use of LLMs in law raises concerns about\nlegal challenges, including copyright issues [919], personal\ninformation leakage [920], or bias and discrimination [921].\nFinance is an important field where LLMs have promis-\ning application prospects. LLMs have been employed on\nvarious finance related tasks, such as numerical claim\ndetection [922], financial sentiment analysis [923], finan-\ncial named entity recognition [924], and financial reason-\ning [925]. Despite the competitive zero-shot performance\nexhibited by general-purpose LLMs in the finance tasks,\nthey still underperform domain-specific PLMs containing\nmillion-scale parameters [922]. To leverage the scaling effect\nof LLMs, researchers collect large-scale finance corpora for\ncontinually pre-training LLMs ( e.g., BloombergGPT [360],\nXuanYuan 2.0 [926], and FinGPT [927]). BloombergGPT\nhas demonstrated remarkable performance across a diverse\nrange of financial tasks while maintaining competitive per-\nformance in general-purpose tasks [360]. Nevertheless, it is\nimperative to consider the potential risks in the application\nof LLMs in finance, as the generation of inaccurate or\nharmful content by LLMs could have significant adverse\nimplications for financial markets [360]. Therefore, it needs\nmore strict reviewing and monitoring on the use of LLMs in82\nthe financial field.\nScientific research is another promising field that LLMs\ncan empower the development progress. Prior research\ndemonstrates the effectiveness of LLMs in handling\nknowledge-intensive scientific tasks ( e.g., PubMedQA [928],\nBioASQ [929]), especially for LLMs that are trained on\nscientific-related corpora [35, 203, 930]. Given the excel-\nlent general abilities and broad scientific knowledge, LLMs\nhold significant potential as helpful assistants across var-\nious stages of the scientific research pipeline [931]. First,\nduring the literature survey stage, LLMs can help conduct\na comprehensive overview of the progress in a specific\nresearch field [932, 933]. Second, during the research idea\ngeneration stage, LLMs demonstrate the ability to generate\nintriguing scientific hypotheses [934]. Third, during the data\nanalysis stage, LLMs can be employed to conduct automatic\napproaches to analyzing the data characteristics, includ-\ning data exploration, visualization, and deriving analytical\nconclusions [935, 936]. Fourth, during the paper writing\nstage, researchers can also benefit from the assistance of\nLLMs in scientific writing [937, 938], in which LLMs can\noffer valuable support for scientific writing through diverse\nmeans, such as summarizing the existing content and pol-\nishing the writing [939]. In addition, LLMs can aid in\nthe automated paper review process, encompassing tasks\nsuch as error detection, checklist verification, and candidate\nranking [940]. Despite these advances, there is much room\nfor improving the capacities of LLMs to serve as helpful,\ntrustworthy scientific assistants, to both increase the quality\nof the generated scientific content and reduce the harmful\nhallucinations.\nSummary . In addition to the aforementioned work, the\napplications of LLMs have been also discussed in several\nother domains. For instance, in the psychologic domain,\nsome recent work has studied the human-like characteristics\nof LLMs, such as self-awareness, theory of mind (ToM), and\naffective computing [941, 942]. In particular, an empirical\nevaluation of ToM conducted on two classic false-belief\ntasks speculates that LLMs may have ToM-like abilities\nsince the model in the GPT-3.5 series achieves comparable\nperformance with nine-year-old children in ToM task [941].\nIn addition, another line of work has investigated applying\nLLMs into the software development domain, e.g., code\nsuggestion [943], code summarization [944], and automated\nprogram repair [945]. To summarize, to assist humans by\nLLMs in real-world tasks has become a significant area of\nresearch. However, it also presents challenges. Ensuring the\naccuracy of LLM-generated content, addressing biases, and\nmaintaining user privacy and data security are crucial con-\nsiderations when applying LLMs to real-world scenarios.\n9 C ONCLUSION AND FUTURE DIRECTIONS\nIn this survey, we have reviewed the recent progress of large\nlanguage models (LLMs), and introduced the key concepts,\nfindings, and techniques for understanding and utilizing\nLLMs. We focus on the large-sized models ( i.e.,having a size\nlarger than 10B) while excluding the contents of early pre-\ntrained language models ( e.g., BERT and GPT-2) that have\nbeen well covered in the existing literature. In particular,our survey has discussed four important aspects of LLMs,\ni.e.,pre-training, adaptation, utilization, and evaluation. For\neach aspect, we highlight the techniques or findings that are\nkey to the success of LLMs. Furthermore, we also summa-\nrize the available resources for developing LLMs and dis-\ncuss important implementation guidelines for reproducing\nLLMs. This survey tries to cover the most recent literature\nabout LLMs and provides a good reference resource on this\ntopic for both researchers and engineers.\nNext, we summarize the discussions of this survey, and\nintroduce the challenges and future directions for LLMs, in\nthe following aspects.\nBasics and Principles. Instead of training on specific task\ngoals, LLMs learn from unsupervised pre-training on large-\nscale text data. This is quite different from previous multi-\ntask learning approaches, which aim to extend the training\ntasks as possible to achieve sufficient generalization. Thus,\nit is essential to reveal the basic principles or elements that\nestablish the foundation of the abilities of LLMs. Although\nthe basic idea of language models is intuitive, it is still chal-\nlenging to formally explain why LLMs trained by simple\nlanguage modeling objectives ( e.g., next token prediction)\ncan become capable of solving various real-world tasks.\nTo investigate this problem, a promising approach is to\nstudy the capacity learning (or selection) mechanism based\non unsupervised pre-training, since the model capacity of\nLLMs strongly depends on pre-training data. In addition,\nscaling plays an important role in improving the capacity\nof LLMs [31, 55, 64], and it is very useful to conduct more\ntheoretical analysis about how the behaviors of large models\nrelate to those of small models, e.g., what behaviors of large\nmodels can be inferred from small models and what can\u2019t be\npredicted indeed. Another research direction is to explore\nmore deep analysis on model generalization for LLMs,\nsince increasing concerns have been raised about whether\nLLMs can generalize beyond the knowledge encoded by\npre-training data. Furthermore, data contamination has be-\ncome a severe issue for fairly assessing the performance of\nLLMs [738], and thus setting appropriate evaluation proto-\ncol will be the basis to investigate and analyze the model\ncapacity of LLMs.\nModel Architecture. Due to the scalability and effective-\nness, Transformer has become the de facto architecture\nfor building LLMs. Various strategies have been proposed\nto improve the performance of this architecture, such as\nneural network configuration and scalable parallel training\n(see discussions in Section 4.2.2). However, Transformer\nstill suffers from high training costs and slow inference\nrates. More efforts [251, 252] are still in need to develop\nimproved model architectures for large-scale pre-training.\nSpecially, system-level or hardware-level optimization ( e.g.,\nFlashAttention [284]) is worth more exploration to improve\nthe efficiency of Transformer architectures. In addition, as an\nimportant basic capacity, existing LLMs typically maintain\na long context window. For example, the most recent GPT-4\nTurbo enables a long context of 128K tokens, and Claude\n2.1 also supports the input up to 200K tokens. Although\nmany efforts have been made to enhance the long context\nmodeling ability of LLMs [264, 291], the resulting mod-83\nels still can\u2019t well process the information in the context\nwindow [299]. To address this issue, specific architecture\nadaptations or algorithms might be needed to enhance the\nmodeling and utilization of long context information. An-\nother worrying concern is that existing work mostly focuses\non training LLMs with decoder-only Transformers. Despite\nthe effectiveness, it severely limits the more wide, diverse\nexplorations on alternative model architectures.\nModel Training. For pre-training, it is essential to establish\na data-centric infrastructure and training procedure for LLM\noptimization, which can effectively support a systematic\nprocess of data collection, data cleaning, data mixture, and\ndata curriculum. Furthermore, it also calls for more flexible\nmechanisms of hardware support or resource schedule, so\nas to better organize and utilize the resources in a computing\ncluster. In practice, it is very challenging to pre-train capable\nLLMs, due to the huge compute consumption and the\nsensitivity to data quality and training tricks [78, 93]. Thus,\nit becomes particularly important to develop systemic, eco-\nnomical pre-training approaches for optimizing LLMs, e.g.,\npredictable scaling [46] and proxy model training [59]. More\ntraining recipes or principles should be investigated and\nshared to reduce the potential risk of degradation or failure\nin large-scale model optimization. Although increasingly\nmore model checkpoints and cleaned datasets have been\nreleased, there still lacks reproducible work on pre-training\ndata preparation ( e.g., detailed cleaning strategies) and data\nscheduling ( e.g., data mixture and curriculum). Since it is\nvery costly to pre-train a LLM from scratch, it is important\nto design suitable mechanisms for continually pre-training\nor fine-tuning the LLM based on publicly available model\ncheckpoints ( e.g., LLaMA [57] and Flan-T5 [69]). For this\npurpose, a number of technical issues have to be resolved,\ne.g., catastrophic forgetting and task specialization. Further-\nmore, it is also useful to develop effective tuning strategies\nthat effectively inject or edit specific knowledge [672], e.g.,\ncorrecting the outdated facts.\nModel Utilization. Based on the natural language inter-\nface, prompting has become the prominent approach for\nusing LLMs to solving various tasks. By combining task\ndescriptions and demonstration examples into prompts, in-\ncontext learning (ICL) endows LLMs with the ability to\nperform well on new tasks, even outperforming full-data\nfine-tuned models in some cases. To enhance the ability of\ncomplex reasoning, advanced prompting techniques have\nbeen proposed, exemplified by the chain-of-thought (CoT)\nstrategy, which includes the intermediate reasoning steps\ninto prompts. Furthermore, planning is a promising ap-\nproach for solving complex tasks, which iteratively invokes\nLLMs by leveraging tool use capacities. Despite these ef-\nforts, several basic problems related to prompting are still\nunder-explored: why a good prompt can elicit the correct\nanswer but a bad prompt cannot, how to reveal the working\nprinciples of advanced prompting methods ( e.g., ICL and\nCoT) and further improve these existing approaches, and\nhow to efficiently find the effective prompts for LLMs on\nspecific tasks. Furthermore, from a practical perspective, it\nhas become a fundamental challenge to reduce the inference\ncost of LLMs, especially in large-scale deployment. Anotherpopular research direction is retrieval-augmented gener-\nation, where retrieved contexts from supporting sources\nare included into prompts for task solving. It has been\nshown that retrieval augmentation can extend the knowl-\nedge boundary and improve the question answering ca-\npacity [461], but may suffer from the effectiveness of long\ncontext utilization by LLMs [299].\nSafety and Alignment. Despite the capacities, LLMs are\nfaced with great safety challenges in practical use. As a\nfundamental issue of probabilistic modeling nature, LLMs\nexhibit a tendency to generate hallucinations [638], refer-\nring to texts that seem plausible but may be factually\nincorrect [46]. What is worse, LLMs might be elicited by\nintentional instructions to produce harmful, biased, or toxic\ntexts for malicious systems, leading to the potential risks\nof misuse [55, 66]. To have a detailed discussion of the\nsafety issues of LLMs ( e.g., privacy, overreliance, disinfor-\nmation, and influence operations), the readers can refer to\nthe GPT-3/4 technical reports [46, 55]. As the major tech-\nnical approach to averting these issues, alignment methods\n(e.g., RLHF) [66, 116] have been widely used by leveraging\nhuman feedback for developing well-aligned LLMs. How-\never, RLHF heavily relies on high-quality human feedback\ndata from professional labelers, which is costly and time-\nconsuming to recruit qualified human annotators. There-\nfore, it is necessary to improve the RLHF framework for\nreducing the efforts of human labelers and seek a more\nefficient annotation approach with guaranteed data quality,\ne.g., LLMs can be employed to assist the labeling work.\nFurthermore, it is also suggested to develop simplified\noptimization algorithms for alignment [386, 389], to reduce\nthe training difficulty and unstability of RLHF. As another\npractical approach, red teaming [132, 369] has been adopted\nfor improving the model safety of LLMs, which utilizes\nthe collected adversarial prompts to refine the LLMs ( i.e.,\navoiding the attacks from red teaming). In addition, privacy\nconcerns are also important to consider when fine-tuning\nLLMs with domain-specific data, and thus federated based\nlearning [946] can be useful in privacy-restricted scenarios.\nApplication and Ecosystem. As LLMs have shown strong\ncapacities in solving various tasks, they can be applied\nin a broad range of real-world applications ( i.e.,following\ntask-specific natural language instructions). As a remarkable\nprogress, ChatGPT has potentially changed the way how\nhumans access information, which has been additionally\nintegrated in the release of New Bing . Generally, in the\nnear future, it can be foreseen that LLMs would have a\nsignificant impact on information-seeking techniques, in-\ncluding both search engines and recommender systems.\nFurthermore, LLMs make it possible to develop more intel-\nligent systems ( e.g.,autonomous AI agents) to tackle various\ncomplex tasks in real-world scenarios. Specially, Assistants\nAPI has been launched by OpenAI (featured by instructions,\nknowledge and tool use), enabling rapid development of\nagent-like assistants within the applications. This wave of\ntechnical innovation would lead to an ecosystem of LLM-\nempowered applications ( e.g., OpenAI\u2019s GPT Store), which\nhas a close connection with human life. Lastly, the rise of\nLLMs sheds light on the exploration of artificial general84\nintelligence (AGI). It is promising to develop more smart AI\nsystems than ever. However, in this development process,\nAI safety should be one of the primary concerns, i.e.,making\nAI lead to good for humanity but not bad [40].\nCODA\nIt is not an easy job to write this long survey and update\nits content with timely work. First of all, we would like to\nsincerely thank the support from the readers and our team\nmembers. We work very hard on this survey, and hope that\nit can present a comprehensive, timely reference for LLMs.\nSurvey Writing . This survey was planned during a discus-\nsion meeting held by our research team, and we aimed to\nsummarize the recent advances of large language models\nas a highly readable report for our team members. The\nfirst draft was finished on March 13, 2023, in which our\nteam members tried their best to include the related stud-\nies about LLMs in a relatively objective, comprehensive\nway. Then, we have extensively revised the writing and\ncontents in several passes. Due to the space limit, we can\nonly include a fraction of existing LLMs in Figure 3 and\nTable 1, by setting the selection criterion. However, we set\na more relaxed criterion for model selection on our GitHub\npage (https://github.com/RUCAIBox/LLMSurvey), which\nwill be regularly maintained. We release the initial version\non March 31, 2023, the major revision on June 29, 2023,\nand second version on September 10, 2023, and this latest\nversion (major revision) on November 23, 2023.\nSeeking for Advice . Despite all our efforts, this survey\nis still far from perfect: we are likely to miss important\nreferences or topics, and might also have non-rigorous\nexpressions or discussions. We will continuously update\nthis survey, and improve the quality as much as we can.\nFor us, survey writing is also a learning process for LLMs\nby ourselves. For readers with constructive suggestions to\nimprove this survey, you are welcome to leave comments on\nthe GitHub page of our survey or directly email our authors.\nWe will make revisions following the received comments\nor suggestions in a future version, and acknowledge the\nreaders who have contributed constructive suggestions in\nour survey.\nUpdate log . In this part, we regularly maintain an update\nlog for the submissions of this survey to arXiv:\n\u2022First release on March 31, 2023: the initial version.\n\u2022Update on April 9, 2023: add the affiliation information,\nrevise Figure 3 and Table 1 and clarify the correspond-\ning selection criterion for LLMs, improve the writing,\nand correct some minor errors.\n\u2022Update on April 11, 2023: correct the errors for library\nresources.\n\u2022Update on April 12, 2023: revise Figure 3 and Table 1,\nand clarify the release date of LLMs.\n\u2022Update on April 16, 2023: add a new Section 2.2 about\nthe technical evolution of GPT-series models.\n\u2022Update on April 24, 2023: add the discussion about\nscaling laws and add some explanations about the\nmodel sizes for emergent abilities (Section 2.1); add an\nillustrative figure for the attention patterns for differentarchitectures in Figure 9, and add the detailed formulas\nin Table 6.\n\u2022Update on April 25, 2023: revise some copy errors in\nfigures and tables.\n\u2022Update on April 27, 2023: add efficient tuning in Sec-\ntion 5.3.\n\u2022Update on April 28, 2023: revise Section 5.3.\n\u2022Update on May 7, 2023: revise Table 1, Table 2, and\nsome minor points.\n\u2022Update on June 29, 2023 (major revision):\n\u2013Section 1: add Figure 1 for the trends of published\nLLM papers in arXiv;\n\u2013Section 2: add Figure 4 for GPT\u2019s evolution and the\ncorresponding discussion;\n\u2013Section 3: add Figure 5 for LLaMA family and the\ncorresponding discussion;\n\u2013Section 5: add latest discussion about the synthetic\ndata formatting of instruction tuning in Section 5.1.1,\nthe empirical analysis for instruction tuning in Sec-\ntion 5.1.4, parameter-efficient model adaptation in\nSection 5.3 and memory-efficient adaptation in Sec-\ntion 5.4;\n\u2013Section 6: add latest discussion about the underlying\nmechanism of ICL 6.2.3, planning for complex task\nsolving in Section 6.4;\n\u2013Section 7: update Table 14 for representative datasets\nfor evaluating advanced abilities of LLMs, and em-\npirical ability evaluation in Section 7.4;\n\u2013Section 6.1.1: add prompt design;\n\u2013Section 8: add the discussions on applications of\nLLMs in finance and scientific research domains;\n\u2022Update on September 10, 2023 (major revision):\n\u2013Claim the copyrights of the figures and tables in this\npaper.\n\u2013Add latest LLMs, techniques and their descriptions in\nSection 3, Section 4, Section 5, Section 6 and Section 7;\n\u2013Section 4: add latest discussion about the decoding\nstrategy in Section 4.2.5;\n\u2013Section 5: add latest discussion about the practical\ntricks for instruction tuning in Section 5.1.2, the\nempirical analysis on LLaMA (13B) for instruction\ntuning in Section 5.1.4, practical strategies for RLHF\nin Section 5.2.3, alignment without RLHF in Sec-\ntion 5.2.4 and remarks on SFT and RLHF in Sec-\ntion 5.2.5;\n\u2013Section 6: update the content about the planning for\ncomplex task solving in Section 6.4;\n\u2013Section 7: add discussions about evaluation ap-\nproaches in Section 7.3.2, Table 15 for the category\nof existing evaluation work, and update empirical\nability evaluation in Section 7.4 and the results on\nTable 16;\n\u2013Section 6.1.1: add new prompt examples in Table 12;\n\u2022Update on November 23, 2023 (this version):\n\u2013Section 1: add Figure 2 for the evolution process of\nfour generations of language models;\n\u2013Section 2: add more discussion about scaling laws\nand how emergent abilities relate to scaling laws;\n\u2013Section 3: add latest LLMs in Figure 3 and Table 1,\nlatest APIs in Section 3.1, commonly used datasets85\nfor instruction tuning and alignment tuning in Sec-\ntion 3.3, and several libraries in Section 3.4;\n\u2013Section 4: add latest discussion about the data\nscheduling, including data mixtures and data cur-\nriculum in Section 4.1.3; add summary of data prepa-\nration in Section 4.1.4; add discussion about model-\ning long context in Section 4.2.4; add discussion about\ndecoding efficiency issues and add latest decoding\nstrategies in Section 4.2.5;\n\u2013Section 5: add latest discussion about instance con-\nstruction and tuning strategies in Section 5.1; add\nlatest discussion about process-supervised RLHF in\nSection 5.2.3, and the empirical study on quantized\nLLaMA models (7B and 13B) in Section 5.4.3;\n\u2013Section 6: add latest discussion about prompt op-\ntimization in Section 6.1.2, and update the content\nabout chain-of-thought prompting in Section 6.3;\n\u2013Section 8: add latest discussion about LLM for re-\nsearch directions in Section 8.1;\n\u2013Section 9: revise the content in the several aspects.\nPlanning Content . We will regularly include new content\ninto this survey, to make it more self-contained and up-\nto-date. Here, we list several potential topics that might\nappear in the next major version(s): (1) more experiments\nwith larger language models for both instruction tuning and\nability evaluation; (2) more detailed prompting practice; (3)\ntraining recipe; (4) more theoretical analysis and discussion;\n(5) more discussions on applications.\nClarifications on Experiments . In this version, we have\nincluded a number experiments on instruction-tuning (Ta-\nble 9), overall ability evaluation (Table 16), and prompt\nengineering (Table 17). Due to the limit of computational\nresources, our experiments are not complete, limited to\nsmall-sized models or a few comparisons. Despite that, we\nfeel that it might be meaningful to share the partial results to\nthe public. We will try to include the missing results of larger\nmodels or more comparisons in the future versions. We also\ncall for support of computing power for conducting more\ncomprehensive experiments.\nChinese Version . We also provide a translated Chinese ver-\nsion (corresponding to the first release) of this survey paper\nat the link: https://github.com/RUCAIBox/LLMSurvey/\nblob/main/assets/LLM Survey Chinese.pdf. Four volun-\nteers contribute to check and revise the content, and they\nare Yiwen Hu, Xin Deng, Xinming Hou, Yanbin Yin, and\nZhanshuo Cao (in order of contribution). We will also con-\ntinuously update the Chinese version, but it may not be as\ntimely as the latest English version.\nACKNOWLEDGMENTS\nThe authors would like to thank Yankai Lin and Yutao Zhu\nfor proofreading this paper. Since the first release of this\npaper, we have received a number of valuable comments\nfrom the readers. We sincerely thank the readers who have\nwritten to us with constructive suggestions and comments:\nTyler Suard, Damai Dai, Liang Ding, Stella Biderman,\nKevin Gray, Jay Alammar, Yubo Feng, Mark Holmstrom,\nXingdong Liu, Il-Seok Oh, Yiting Liu, Shaojun Wang,Gaoyan Ou, Todd Morrill, Hao Liu, Zhenyu Zhang, and\nXinlin Zhuang.\nSince the v11 version (June 29, 2023), we have been\nadding a large number of experiments and prompt prac-\ntices. These new contents are completed by a number of\nvolunteers in our team. Here, we add a special part to thank\nall the students who have worked very hard on this part\n(also including the ones on our author list).\nContribution on Experiments. We would like to sincerely\nthank the following people for their hard work involved in\nexperiments shown in Table 16.\n\u2022Xiaoxue Cheng: implement the experiments for evalu-\nation on Language Generation and HaluEval tasks.\n\u2022Yuhao Wang: implement the experiments for evalua-\ntion on interaction with environment tasks.\n\u2022Bowen Zheng: implement the experiments for evalua-\ntion on tool manipulation tasks.\nContribution on Tips. We list the following guys for their\ncontributions on the corresponding numbers of provided\ntips for designing prompts in Table 12.\n\u2022Xiaolei Wang: T3, O3\n\u2022Beichen Zhang: D2, D5\n\u2022Zhipeng Chen: D3, D4\n\u2022Junjie Zhang: D6\n\u2022Bowen Zheng: D7\n\u2022Zican Dong: D8\n\u2022Xinyu Tang: C2\n\u2022Yifan Du: T4\n\u2022Tianyi Tang: O6, O7, D9\n\u2022Yupeng Hou: O8, C3\n\u2022Salvatore Raieli: C4\nREFERENCES\n[1] Y. Bengio, R. Ducharme, P . Vincent, and C. Janvin, \u201cA\nneural probabilistic language model,\u201d J. Mach. Learn.\nRes., vol. 3, pp. 1137\u20131155, 2003.\n[2] R. Collobert, J. Weston, L. Bottou, M. Karlen,\nK. Kavukcuoglu, and P . P . Kuksa, \u201cNatural language\nprocessing (almost) from scratch,\u201d J. Mach. Learn. Res. ,\nvol. 12, pp. 2493\u20132537, 2011.\n[3] S. Pinker, The Language Instinct: How the Mind Creates\nLanguage . Brilliance Audio; Unabridged edition,\n2014.\n[4] M. D. Hauser, N. Chomsky, and W. T. Fitch, \u201cThe\nfaculty of language: what is it, who has it, and how\ndid it evolve?\u201d science , vol. 298, no. 5598, pp. 1569\u2013\n1579, 2002.\n[5] A. M. Turing, \u201cComputing machinery and intelli-\ngence,\u201d Mind , vol. LIX, no. 236, pp. 433\u2013460, 1950.\n[6] F. Jelinek, Statistical Methods for Speech Recognition .\nMIT Press, 1998.\n[7] J. Gao and C. Lin, \u201cIntroduction to the special issue\non statistical language modeling,\u201d ACM Trans. Asian\nLang. Inf. Process. , vol. 3, no. 2, pp. 87\u201393, 2004.\n[8] R. Rosenfeld, \u201cTwo decades of statistical language\nmodeling: Where do we go from here?\u201d Proceedings\nof the IEEE , vol. 88, no. 8, pp. 1270\u20131278, 2000.86\n[9] A. Stolcke, \u201cSrilm-an extensible language modeling\ntoolkit,\u201d in Seventh international conference on spoken\nlanguage processing , 2002.\n[10] X. Liu and W. B. Croft, \u201cStatistical language modeling\nfor information retrieval,\u201d Annu. Rev. Inf. Sci. Technol. ,\nvol. 39, no. 1, pp. 1\u201331, 2005.\n[11] C. Zhai, Statistical Language Models for Information Re-\ntrieval , ser. Synthesis Lectures on Human Language\nTechnologies. Morgan & Claypool Publishers, 2008.\n[12] S. M. Thede and M. P . Harper, \u201cA second-order hid-\nden markov model for part-of-speech tagging,\u201d in\n27th Annual Meeting of the Association for Computational\nLinguistics, University of Maryland, College Park, Mary-\nland, USA, 20-26 June 1999 , R. Dale and K. W. Church,\nEds. ACL, 1999, pp. 175\u2013182.\n[13] L. R. Bahl, P . F. Brown, P . V . de Souza, and R. L. Mercer,\n\u201cA tree-based statistical language model for natural\nlanguage speech recognition,\u201d IEEE Transactions on\nAcoustics, Speech, and Signal Processing , vol. 37, no. 7,\npp. 1001\u20131008, 1989.\n[14] T. Brants, A. C. Popat, P . Xu, F. J. Och, and J. Dean,\n\u201cLarge language models in machine translation,\u201d in\nEMNLP-CoNLL 2007, Proceedings of the 2007 Joint Con-\nference on Empirical Methods in Natural Language Pro-\ncessing and Computational Natural Language Learning,\nJune 28-30, 2007, Prague, Czech Republic , J. Eisner, Ed.\nACL, 2007, pp. 858\u2013867.\n[15] S. M. Katz, \u201cEstimation of probabilities from sparse\ndata for the language model component of a speech\nrecognizer,\u201d IEEE Trans. Acoust. Speech Signal Process. ,\nvol. 35, no. 3, pp. 400\u2013401, 1987.\n[16] W. A. Gale and G. Sampson, \u201cGood-turing frequency\nestimation without tears,\u201d J. Quant. Linguistics , vol. 2,\nno. 3, pp. 217\u2013237, 1995.\n[17] T. Mikolov, M. Karafi \u00b4at, L. Burget, J. Cernock \u00b4y, and\nS. Khudanpur, \u201cRecurrent neural network based lan-\nguage model,\u201d in INTERSPEECH 2010, 11th Annual\nConference of the International Speech Communication\nAssociation, Makuhari, Chiba, Japan, September 26-30,\n2010 , T. Kobayashi, K. Hirose, and S. Nakamura, Eds.\nISCA, 2010, pp. 1045\u20131048.\n[18] S. Kombrink, T. Mikolov, M. Karafi \u00b4at, and L. Burget,\n\u201cRecurrent neural network based language modeling\nin meeting recognition,\u201d in INTERSPEECH 2011, 12th\nAnnual Conference of the International Speech Commu-\nnication Association, Florence, Italy, August 27-31, 2011 .\nISCA, 2011, pp. 2877\u20132880.\n[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean, \u201cDistributed representations of words and\nphrases and their compositionality,\u201d in Advances in\nNeural Information Processing Systems 26: 27th Annual\nConference on Neural Information Processing Systems\n2013. Proceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States , C. J. C. Burges, L. Bot-\ntou, Z. Ghahramani, and K. Q. Weinberger, Eds., 2013,\npp. 3111\u20133119.\n[20] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEf-\nficient estimation of word representations in vector\nspace,\u201d in 1st International Conference on Learning Rep-\nresentations, ICLR 2013, Scottsdale, Arizona, USA, May\n2-4, 2013, Workshop Track Proceedings , Y. Bengio andY. LeCun, Eds., 2013.\n[21] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner,\nC. Clark, K. Lee, and L. Zettlemoyer, \u201cDeep contex-\ntualized word representations,\u201d in Proceedings of the\n2018 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2018, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 1 (Long Papers) , M. A.\nWalker, H. Ji, and A. Stent, Eds. Association for\nComputational Linguistics, 2018, pp. 2227\u20132237.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n\u201cAttention is all you need,\u201d in Advances in Neural\nInformation Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA , 2017, pp. 5998\u20136008.\n[23] J. Devlin, M. Chang, K. Lee, and K. Toutanova, \u201cBERT:\npre-training of deep bidirectional transformers for\nlanguage understanding,\u201d in Proceedings of the 2019\nConference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers) ,\nJ. Burstein, C. Doran, and T. Solorio, Eds. Association\nfor Computational Linguistics, 2019, pp. 4171\u20134186.\n[24] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V . Stoyanov, and L. Zettlemoyer,\n\u201cBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension,\u201d in Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020 , 2020, pp. 7871\u20137880.\n[25] W. Fedus, B. Zoph, and N. Shazeer, \u201cSwitch trans-\nformers: Scaling to trillion parameter models with\nsimple and efficient sparsity,\u201d J. Mach. Learn. Res , pp.\n1\u201340, 2021.\n[26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei,\nI. Sutskever et al. , \u201cLanguage models are unsuper-\nvised multitask learners,\u201d OpenAI blog , p. 9, 2019.\n[27] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen,\nO. Levy, M. Lewis, L. Zettlemoyer, and V . Stoyanov,\n\u201cRoberta: A robustly optimized BERT pretraining ap-\nproach,\u201d CoRR , vol. abs/1907.11692, 2019.\n[28] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika,\nZ. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey,\nM. S. Bari, C. Xu, U. Thakker, S. S. Sharma,\nE. Szczechla, T. Kim, G. Chhablani, N. V . Nayak,\nD. Datta, J. Chang, M. T. Jiang, H. Wang, M. Man-\nica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden,\nT. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli,\nT. F\u00b4evry, J. A. Fries, R. Teehan, T. L. Scao, S. Bider-\nman, L. Gao, T. Wolf, and A. M. Rush, \u201cMultitask\nprompted training enables zero-shot task generaliza-\ntion,\u201d in The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29,\n2022 . OpenReview.net, 2022.\n[29] T. Wang, A. Roberts, D. Hesslow, T. L. Scao, H. W.\nChung, I. Beltagy, J. Launay, and C. Raffel, \u201cWhat\nlanguage model architecture and pretraining objective\nworks best for zero-shot generalization?\u201d in Interna-\ntional Conference on Machine Learning, ICML 2022, 17-2387\nJuly 2022, Baltimore, Maryland, USA , ser. Proceedings\nof Machine Learning Research, vol. 162, 2022, pp.\n22 964\u201322 984.\n[30] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown,\nB. Chess, R. Child, S. Gray, A. Radford, J. Wu, and\nD. Amodei, \u201cScaling laws for neural language mod-\nels,\u201d CoRR , vol. abs/2001.08361, 2020.\n[31] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph,\nS. Borgeaud, D. Yogatama, M. Bosma, D. Zhou,\nD. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals,\nP . Liang, J. Dean, and W. Fedus, \u201cEmergent abilities of\nlarge language models,\u201d CoRR , vol. abs/2206.07682,\n2022.\n[32] M. Shanahan, \u201cTalking about large language models,\u201d\nCoRR , vol. abs/2212.03551, 2022.\n[33] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi,\nQ. Le, and D. Zhou, \u201cChain of thought prompting\nelicits reasoning in large language models,\u201d CoRR , vol.\nabs/2201.11903, 2022.\n[34] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya,\nT. Cai, E. Rutherford, D. de Las Casas, L. A. Hen-\ndricks, J. Welbl, A. Clark, T. Hennigan, E. Noland,\nK. Millican, G. van den Driessche, B. Damoc, A. Guy,\nS. Osindero, K. Simonyan, E. Elsen, J. W. Rae,\nO. Vinyals, and L. Sifre, \u201cTraining compute-optimal\nlarge language models,\u201d vol. abs/2203.15556, 2022.\n[35] R. Taylor, M. Kardas, G. Cucurull, T. Scialom,\nA. Hartshorn, E. Saravia, A. Poulton, V . Kerkez, and\nR. Stojnic, \u201cGalactica: A large language model for\nscience,\u201d CoRR , vol. abs/2211.09085, 2022.\n[36] P . Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and\nG. Neubig, \u201cPre-train, prompt, and predict: A system-\natic survey of prompting methods in natural language\nprocessing,\u201d ACM Comput. Surv. , pp. 195:1\u2013195:35,\n2023.\n[37] C. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang,\nC. Ji, Q. Yan, L. He, H. Peng, J. Li, J. Wu, Z. Liu, P . Xie,\nC. Xiong, J. Pei, P . S. Yu, and L. Sun, \u201cA comprehensive\nsurvey on pretrained foundation models: A history\nfrom BERT to chatgpt,\u201d CoRR , vol. abs/2302.09419,\n2023.\n[38] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo,\nJ. Qiu, Y. Yao, A. Zhang, L. Zhang, W. Han, M. Huang,\nQ. Jin, Y. Lan, Y. Liu, Z. Liu, Z. Lu, X. Qiu, R. Song,\nJ. Tang, J. Wen, J. Yuan, W. X. Zhao, and J. Zhu, \u201cPre-\ntrained models: Past, present and future,\u201d AI Open ,\nvol. 2, pp. 225\u2013250, 2021.\n[39] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang,\n\u201cPre-trained models for natural language processing:\nA survey,\u201d CoRR , vol. abs/2003.08271, 2020.\n[40] S. Altman, \u201cPlanning for agi and beyond,\u201d OpenAI\nBlog, February 2023.\n[41] S. Bubeck, V . Chandrasekaran, R. Eldan, J. Gehrke,\nE. Horvitz, E. Kamar, P . Lee, Y. T. Lee, Y. Li, S. Lund-\nberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang,\n\u201cSparks of artificial general intelligence: Early experi-\nments with gpt-4,\u201d vol. abs/2303.12712, 2023.\n[42] S. Huang, L. Dong, W. Wang, Y. Hao, S. Singhal, S. Ma,\nT. Lv, L. Cui, O. K. Mohammed, B. Patra, Q. Liu,\nK. Aggarwal, Z. Chi, J. Bjorck, V . Chaudhary, S. Som,\nX. Song, and F. Wei, \u201cLanguage is not all you need:Aligning perception with language models,\u201d CoRR ,\nvol. abs/2302.14045, 2023.\n[43] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P . S. Yu, and\nL. Sun, \u201cA comprehensive survey of ai-generated\ncontent (aigc): A history of generative ai from gan to\nchatgpt,\u201d arXiv preprint arXiv:2303.04226 , 2023.\n[44] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdh-\nery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu\net al. , \u201cPalm-e: An embodied multimodal language\nmodel,\u201d arXiv preprint arXiv:2303.03378 , 2023.\n[45] C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and\nN. Duan, \u201cVisual chatgpt: Talking, drawing and edit-\ning with visual foundation models,\u201d arXiv preprint\narXiv:2303.04671 , 2023.\n[46] OpenAI, \u201cGpt-4 technical report,\u201d OpenAI , 2023.\n[47] Y. Fu, H. Peng, and T. Khot, \u201cHow does gpt obtain its\nability? tracing emergent abilities of language models\nto their sources,\u201d Yao Fu\u2019s Notion , Dec 2022.\n[48] J. Li, T. Tang, W. X. Zhao, and J. Wen, \u201cPretrained\nlanguage model for text generation: A survey,\u201d in\nProceedings of the Thirtieth International Joint Conference\non Artificial Intelligence, IJCAI 2021, Virtual Event /\nMontreal, Canada, 19-27 August 2021 , Z. Zhou, Ed.\nijcai.org, 2021, pp. 4492\u20134499.\n[49] P . Lu, L. Qiu, W. Yu, S. Welleck, and K. Chang, \u201cA\nsurvey of deep learning for mathematical reasoning,\u201d\nCoRR , vol. abs/2212.10535, 2022.\n[50] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang,\nX. Sun, J. Xu, L. Li, and Z. Sui, \u201cA survey for in-context\nlearning,\u201d CoRR , vol. abs/2301.00234, 2023.\n[51] J. Huang and K. C. Chang, \u201cTowards reasoning\nin large language models: A survey,\u201d CoRR , vol.\nabs/2212.10403, 2022.\n[52] S. Qiao, Y. Ou, N. Zhang, X. Chen, Y. Yao, S. Deng,\nC. Tan, F. Huang, and H. Chen, \u201cReasoning with\nlanguage model prompting: A survey,\u201d CoRR , vol.\nabs/2212.09597, 2022.\n[53] J. Zhou, P . Ke, X. Qiu, M. Huang, and J. Zhang, \u201cChat-\ngpt: potential, prospects, and limitations,\u201d in Frontiers\nof Information Technology & Electronic Engineering , 2023,\npp. 1\u20136.\n[54] W. X. Zhao, J. Liu, R. Ren, and J. Wen, \u201cDense text\nretrieval based on pretrained language models: A\nsurvey,\u201d CoRR , vol. abs/2211.14876, 2022.\n[55] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan,\nP . Dhariwal, A. Neelakantan, P . Shyam, G. Sastry,\nA. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,\nT. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. Mc-\nCandlish, A. Radford, I. Sutskever, and D. Amodei,\n\u201cLanguage models are few-shot learners,\u201d in Ad-\nvances in Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing Sys-\ntems 2020, NeurIPS 2020, December 6-12, 2020, virtual ,\nH. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and\nH. Lin, Eds., 2020.\n[56] A. Chowdhery, S. Narang, J. Devlin, M. Bosma,\nG. Mishra, A. Roberts, P . Barham, H. W. Chung,\nC. Sutton, S. Gehrmann, P . Schuh, K. Shi,\nS. Tsvyashchenko, J. Maynez, A. Rao, P . Barnes,88\nY. Tay, N. Shazeer, V . Prabhakaran, E. Reif, N. Du,\nB. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Is-\nard, G. Gur-Ari, P . Yin, T. Duke, A. Levskaya, S. Ghe-\nmawat, S. Dev, H. Michalewski, X. Garcia, V . Misra,\nK. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan,\nH. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Do-\nhan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pil-\nlai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child,\nO. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta,\nM. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-\nHellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel,\n\u201cPalm: Scaling language modeling with pathways,\u201d\nCoRR , vol. abs/2204.02311, 2022.\n[57] H. Touvron, T. Lavril, G. Izacard, X. Martinet,\nM. Lachaux, T. Lacroix, B. Rozi `ere, N. Goyal, E. Ham-\nbro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and\nG. Lample, \u201cLlama: Open and efficient foundation\nlanguage models,\u201d CoRR , 2023.\n[58] T. Henighan, J. Kaplan, M. Katz, M. Chen, C. Hesse,\nJ. Jackson, H. Jun, T. B. Brown, P . Dhariwal, S. Gray\net al. , \u201cScaling laws for autoregressive generative\nmodeling,\u201d arXiv preprint arXiv:2010.14701 , 2020.\n[59] S. M. Xie, H. Pham, X. Dong, N. Du, H. Liu, Y. Lu,\nP . Liang, Q. V . Le, T. Ma, and A. W. Yu, \u201cDoremi:\nOptimizing data mixtures speeds up language model\npretraining,\u201d arXiv preprint arXiv:2305.10429 , 2023.\n[60] P . Villalobos, J. Sevilla, L. Heim, T. Besiroglu,\nM. Hobbhahn, and A. Ho, \u201cWill we run out of data?\nan analysis of the limits of scaling datasets in machine\nlearning,\u201d CoRR , vol. abs/2211.04325, 2022. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2211.04325\n[61] N. Muennighoff, A. M. Rush, B. Barak, T. L. Scao,\nA. Piktus, N. Tazi, S. Pyysalo, T. Wolf, and C. Raffel,\n\u201cScaling data-constrained language models,\u201d arXiv\npreprint arXiv:2305.16264 , 2023.\n[62] I. McKenzie, A. Lyzhov, A. Parrish, A. Prabhu,\nA. Mueller, N. Kim, S. Bowman, and E. Perez,\n\u201cThe inverse scaling prize,\u201d 2022. [Online]. Available:\nhttps://github.com/inverse-scaling/prize\n[63] B. A. Huberman and T. Hogg, \u201cPhase transitions in\nartificial intelligence systems,\u201d Artificial Intelligence ,\nvol. 33, no. 2, pp. 155\u2013171, 1987.\n[64] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoff-\nmann, H. F. Song, J. Aslanides, S. Henderson, R. Ring,\nS. Young, E. Rutherford, T. Hennigan, J. Menick,\nA. Cassirer, R. Powell, G. van den Driessche, L. A.\nHendricks, M. Rauh, P . Huang, A. Glaese, J. Welbl,\nS. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins,\nA. Creswell, N. McAleese, A. Wu, E. Elsen, S. M.\nJayakumar, E. Buchatskaya, D. Budden, E. Suther-\nland, K. Simonyan, M. Paganini, L. Sifre, L. Martens,\nX. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya,\nD. Donato, A. Lazaridou, A. Mensch, J. Lespiau,\nM. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sotti-\naux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama,\nC. de Masson d\u2019Autume, Y. Li, T. Terzi, V . Mikulik,\nI. Babuschkin, A. Clark, D. de Las Casas, A. Guy,\nC. Jones, J. Bradbury, M. J. Johnson, B. A. Hechtman,\nL. Weidinger, I. Gabriel, W. S. Isaac, E. Lockhart,\nS. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub,\nJ. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu,and G. Irving, \u201cScaling language models: Methods,\nanalysis & insights from training gopher,\u201d CoRR , vol.\nabs/2112.11446, 2021.\n[65] D. Dai, Y. Sun, L. Dong, Y. Hao, Z. Sui, and F. Wei,\n\u201cWhy can GPT learn in-context? language models se-\ncretly perform gradient descent as meta-optimizers,\u201d\nCoRR , vol. abs/2212.10559, 2022.\n[66] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wain-\nwright, P . Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller,\nM. Simens, A. Askell, P . Welinder, P . F. Christiano,\nJ. Leike, and R. Lowe, \u201cTraining language models to\nfollow instructions with human feedback,\u201d CoRR , vol.\nabs/2203.02155, 2022.\n[67] J. Wei, M. Bosma, V . Y. Zhao, K. Guu, A. W. Yu,\nB. Lester, N. Du, A. M. Dai, and Q. V . Le, \u201cFine-\ntuned language models are zero-shot learners,\u201d in\nThe Tenth International Conference on Learning Repre-\nsentations, ICLR 2022, Virtual Event, April 25-29, 2022 .\nOpenReview.net, 2022.\n[68] R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer,\nA. Kulshreshtha, H. Cheng, A. Jin, T. Bos, L. Baker,\nY. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri,\nM. Menegali, Y. Huang, M. Krikun, D. Lepikhin,\nJ. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma,\nY. Zhou, C. Chang, I. Krivokon, W. Rusch, M. Pick-\nett, K. S. Meier-Hellstern, M. R. Morris, T. Doshi,\nR. D. Santos, T. Duke, J. Soraker, B. Zevenbergen,\nV . Prabhakaran, M. Diaz, B. Hutchinson, K. Olson,\nA. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Ra-\njakumar, A. Butryna, M. Lamm, V . Kuzmina, J. Fenton,\nA. Cohen, R. Bernstein, R. Kurzweil, B. Aguera-Arcas,\nC. Cui, M. Croak, E. H. Chi, and Q. Le, \u201cLamda:\nLanguage models for dialog applications,\u201d CoRR , vol.\nabs/2201.08239, 2022.\n[69] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay,\nW. Fedus, E. Li, X. Wang, M. Dehghani, S. Brahma,\nA. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen,\nA. Chowdhery, S. Narang, G. Mishra, A. Yu, V . Y.\nZhao, Y. Huang, A. M. Dai, H. Yu, S. Petrov, E. H.\nChi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V . Le,\nand J. Wei, \u201cScaling instruction-finetuned language\nmodels,\u201d CoRR , vol. abs/2210.11416, 2022.\n[70] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb,\nA. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta,\nA. Garriga-Alonso, A. Kluska, A. Lewkowycz,\nA. Agarwal, A. Power, A. Ray, A. Warstadt, A. W.\nKocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish,\nA. Nie, A. Hussain, A. Askell, A. Dsouza, A. Rahane,\nA. S. Iyer, A. Andreassen, A. Santilli, A. Stuhlm \u00a8uller,\nA. M. Dai, A. La, A. K. Lampinen, A. Zou, A. Jiang,\nA. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli,\nA. Venkatesh, A. Gholamidavoodi, A. Tabassum,\nA. Menezes, A. Kirubarajan, A. Mullokandov, A. Sab-\nharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakas,\nand et al., \u201cBeyond the imitation game: Quantifying\nand extrapolating the capabilities of language mod-\nels,\u201d CoRR , vol. abs/2206.04615, 2022.\n[71] R. Schaeffer, B. Miranda, and S. Koyejo, \u201cAre emer-\ngent abilities of large language models a mirage?\u201d\narXiv preprint arXiv:2304.15004 , 2023.89\n[72] S. Hu, X. Liu, X. Han, X. Zhang, C. He, W. Zhao, Y. Lin,\nN. Ding, Z. Ou, G. Zeng, Z. Liu, and M. Sun, \u201cUnlock\npredictable scaling from emergent abilities,\u201d 2023.\n[73] A. Power, Y. Burda, H. Edwards, I. Babuschkin, and\nV . Misra, \u201cGrokking: Generalization beyond overfit-\nting on small algorithmic datasets,\u201d arXiv preprint\narXiv:2201.02177 , 2022.\n[74] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He,\n\u201cDeepspeed: System optimizations enable training\ndeep learning models with over 100 billion parame-\nters,\u201d in KDD , 2020, pp. 3505\u20133506.\n[75] M. Shoeybi, M. Patwary, R. Puri, P . LeGresley,\nJ. Casper, and B. Catanzaro, \u201cMegatron-lm: Training\nmulti-billion parameter language models using model\nparallelism,\u201d CoRR , vol. abs/1909.08053, 2019.\n[76] D. Narayanan, M. Shoeybi, J. Casper, P . LeGres-\nley, M. Patwary, V . Korthikanti, D. Vainbrand,\nP . Kashinkunti, J. Bernauer, B. Catanzaro, A. Phan-\nishayee, and M. Zaharia, \u201cEfficient large-scale lan-\nguage model training on GPU clusters using\nmegatron-lm,\u201d in International Conference for High Per-\nformance Computing, Networking, Storage and Analysis,\nSC 2021, St. Louis, Missouri, USA, November 14-19,\n2021 . ACM, 2021, p. 58.\n[77] V . Korthikanti, J. Casper, S. Lym, L. McAfee, M. An-\ndersch, M. Shoeybi, and B. Catanzaro, \u201cReducing ac-\ntivation recomputation in large transformer models,\u201d\nCoRR , vol. abs/2205.05198, 2022.\n[78] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hess-\nlow, R. Castagn \u00b4e, A. S. Luccioni, F. Yvon, M. Gall \u00b4e,\nJ. Tow, A. M. Rush, S. Biderman, A. Webson, P . S.\nAmmanamanchi, T. Wang, B. Sagot, N. Muennighoff,\nA. V . del Moral, O. Ruwase, R. Bawden, S. Bekman,\nA. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier,\nS. Tan, P . O. Suarez, V . Sanh, H. Laurenc \u00b8on, Y. Jer-\nnite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan,\nA. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers,\nA. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm,\nC. Leong, D. van Strien, D. I. Adelani, and et al.,\n\u201cBLOOM: A 176b-parameter open-access multilingual\nlanguage model,\u201d CoRR , vol. abs/2211.05100, 2022.\n[79] P . F. Christiano, J. Leike, T. B. Brown, M. Martic,\nS. Legg, and D. Amodei, \u201cDeep reinforcement learn-\ning from human preferences,\u201d in Advances in Neural\nInformation Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December\n4-9, 2017, Long Beach, CA, USA , I. Guyon, U. von\nLuxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V . N.\nVishwanathan, and R. Garnett, Eds., 2017, pp. 4299\u2013\n4307.\n[80] T. Schick, J. Dwivedi-Yu, R. Dess `\u0131, R. Raileanu,\nM. Lomeli, L. Zettlemoyer, N. Cancedda, and\nT. Scialom, \u201cToolformer: Language models can teach\nthemselves to use tools,\u201d CoRR , vol. abs/2302.04761,\n2023.\n[81] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang,\nC. Kim, C. Hesse, S. Jain, V . Kosaraju, W. Saun-\nders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger,\nK. Button, M. Knight, B. Chess, and J. Schulman,\n\u201cWebgpt: Browser-assisted question-answering with\nhuman feedback,\u201d CoRR , vol. abs/2112.09332, 2021.[82] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li, and P . J. Liu, \u201cExploring\nthe limits of transfer learning with a unified text-\nto-text transformer,\u201d J. Mach. Learn. Res. , pp. 140:1\u2013\n140:67, 2020.\n[83] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-\nRfou, A. Siddhant, A. Barua, and C. Raffel, \u201cmt5: A\nmassively multilingual pre-trained text-to-text trans-\nformer,\u201d in Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021 , 2021, pp.\n483\u2013498.\n[84] W. Zeng, X. Ren, T. Su, H. Wang, Y. Liao, Z. Wang,\nX. Jiang, Z. Yang, K. Wang, X. Zhang, C. Li,\nZ. Gong, Y. Yao, X. Huang, J. Wang, J. Yu, Q. Guo,\nY. Yu, Y. Zhang, J. Wang, H. Tao, D. Yan, Z. Yi,\nF. Peng, F. Jiang, H. Zhang, L. Deng, Y. Zhang,\nZ. Lin, C. Zhang, S. Zhang, M. Guo, S. Gu, G. Fan,\nY. Wang, X. Jin, Q. Liu, and Y. Tian, \u201cPangu- \u03b1:\nLarge-scale autoregressive pretrained chinese lan-\nguage models with auto-parallel computation,\u201d CoRR ,\nvol. abs/2104.12369, 2021.\n[85] Z. Zhang, Y. Gu, X. Han, S. Chen, C. Xiao, Z. Sun,\nY. Yao, F. Qi, J. Guan, P . Ke, Y. Cai, G. Zeng, Z. Tan,\nZ. Liu, M. Huang, W. Han, Y. Liu, X. Zhu, and\nM. Sun, \u201cCPM-2: large-scale cost-effective pre-trained\nlanguage models,\u201d CoRR , vol. abs/2106.10715, 2021.\n[86] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang,\nY. Zhou, S. Savarese, and C. Xiong, \u201cCodegen: An\nopen large language model for code with mtulti-turn\nprogram synthesis,\u201d arXiv preprint arXiv:2203.13474 ,\n2022.\n[87] S. Black, S. Biderman, E. Hallahan, Q. Anthony,\nL. Gao, L. Golding, H. He, C. Leahy, K. McDonell,\nJ. Phang, M. Pieler, U. S. Prashanth, S. Purohit,\nL. Reynolds, J. Tow, B. Wang, and S. Weinbach, \u201cGpt-\nneox-20b: An open-source autoregressive language\nmodel,\u201d CoRR , vol. abs/2204.06745, 2022.\n[88] Y. Wang, S. Mishra, P . Alipoormolabashi, Y. Kordi,\nA. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran,\nA. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis,\nH. G. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuz-\nnia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi,\nM. Parmar, M. Purohit, N. Varshney, P . R. Kaza,\nP . Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat,\nS. Mishra, S. R. A, S. Patro, T. Dixit, and X. Shen,\n\u201cSuper-naturalinstructions: Generalization via declar-\native instructions on 1600+ NLP tasks,\u201d in Proceedings\nof the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022 , 2022, pp. 5085\u2013\n5109.\n[89] Y. Tay, M. Dehghani, V . Q. Tran, X. Garc \u00b4\u0131a, J. Wei,\nX. Wang, H. W. Chung, D. Bahri, T. Schuster,\nH. Zheng, D. Zhou, N. Houlsby, and D. Metzler, \u201cUl2:\nUnifying language learning paradigms,\u201d 2022.\n[90] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen,\nS. Chen, C. Dewan, M. T. Diab, X. Li, X. V . Lin,\nT. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig,\nP . S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer,90\n\u201cOPT: open pre-trained transformer language mod-\nels,\u201d CoRR , vol. abs/2205.01068, 2022.\n[91] M. R. Costa-juss `a, J. Cross, O. C \u00b8 elebi, M. Elbayad,\nK. Heafield, K. Heffernan, E. Kalbassi, J. Lam,\nD. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek,\nA. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez,\nP . Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan,\nD. Rowe, S. Spruit, C. Tran, P . Andrews, N. F. Ayan,\nS. Bhosale, S. Edunov, A. Fan, C. Gao, V . Goswami,\nF. Guzm \u00b4an, P . Koehn, A. Mourachko, C. Ropers,\nS. Saleem, H. Schwenk, and J. Wang, \u201cNo language\nleft behind: Scaling human-centered machine transla-\ntion,\u201d CoRR , vol. abs/2207.04672, 2022.\n[92] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue,\nZ. Wang, L. Shen, A. Wang, Y. Li et al. , \u201cCodegeex:\nA pre-trained model for code generation with mul-\ntilingual evaluations on humaneval-x,\u201d arXiv preprint\narXiv:2303.17568 , 2023.\n[93] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding,\nZ. Yang, Y. Xu, W. Zheng, X. Xia, W. L. Tam, Z. Ma,\nY. Xue, J. Zhai, W. Chen, P . Zhang, Y. Dong, and\nJ. Tang, \u201cGLM-130B: an open bilingual pre-trained\nmodel,\u201d vol. abs/2210.02414, 2022.\n[94] N. Muennighoff, T. Wang, L. Sutawika, A. Roberts,\nS. Biderman, T. L. Scao, M. S. Bari, S. Shen, Z. X. Yong,\nH. Schoelkopf, X. Tang, D. Radev, A. F. Aji, K. Al-\nmubarak, S. Albanie, Z. Alyafeai, A. Webson, E. Raff,\nand C. Raffel, \u201cCrosslingual generalization through\nmultitask finetuning,\u201d CoRR , vol. abs/2211.01786,\n2022.\n[95] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig,\nP . Yu, K. Shuster, T. Wang, Q. Liu, P . S. Koura, X. Li,\nB. O\u2019Horo, G. Pereyra, J. Wang, C. Dewan, A. Celikyil-\nmaz, L. Zettlemoyer, and V . Stoyanov, \u201cOPT-IML: scal-\ning language model instruction meta learning through\nthe lens of generalization,\u201d CoRR , vol. abs/2212.12017,\n2022.\n[96] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley,\nK. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S.\nPrashanth, E. Raff et al. , \u201cPythia: A suite for analyzing\nlarge language models across training and scaling,\u201d\narXiv preprint arXiv:2304.01373 , 2023.\n[97] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and\nY. Zhou, \u201cCodegen2: Lessons for training llms on\nprogramming and natural languages,\u201d CoRR , vol.\nabs/2305.02309, 2023.\n[98] R. Li, L. B. Allal, Y. Zi, N. Muennighoff,\nD. Kocetkov, C. Mou, M. Marone, C. Akiki,\nJ. Li, J. Chim, Q. Liu, E. Zheltonozhskii, T. Y. Zhuo,\nT. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier,\nJ. Monteiro, O. Shliazhko, N. Gontier, N. Meade,\nA. Zebaze, M. Yee, L. K. Umapathi, J. Zhu, B. Lipkin,\nM. Oblokulov, Z. Wang, R. M. V , J. Stillerman,\nS. S. Patel, D. Abulkhanov, M. Zocca, M. Dey,\nZ. Zhang, N. Fahmy, U. Bhattacharyya, W. Yu,\nS. Singh, S. Luccioni, P . Villegas, M. Kunakov,\nF. Zhdanov, M. Romero, T. Lee, N. Timor,\nJ. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert,\nT. Dao, M. Mishra, A. Gu, J. Robinson, C. J.\nAnderson, B. Dolan-Gavitt, D. Contractor, S. Reddy,\nD. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis,S. Hughes, T. Wolf, A. Guha, L. von Werra, and\nH. de Vries, \u201cStarcoder: may the source be with you!\u201d\nCoRR , vol. abs/2305.06161, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2305.06161\n[99] H. Touvron, L. Martin, K. Stone, P . Albert, A. Alma-\nhairi, Y. Babaei, N. Bashlykov, S. Batra, P . Bhargava,\nS. Bhosale et al. , \u201cLlama 2: Open foundation and fine-\ntuned chat models,\u201d arXiv preprint arXiv:2307.09288 ,\n2023.\n[100] A. Yang, B. Xiao, B. Wang, B. Zhang, C. Yin, C. Lv,\nD. Pan, D. Wang, D. Yan, F. Yang et al. , \u201cBaichuan\n2: Open large-scale language models,\u201d arXiv preprint\narXiv:2309.10305 , 2023.\n[101] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng,\nY. Fan, W. Ge, Y. Han, F. Huang et al. , \u201cQwen technical\nreport,\u201d arXiv preprint arXiv:2309.16609 , 2023.\n[102] X. Li, Y. Yao, X. Jiang, X. Fang, X. Meng, S. Fan, P . Han,\nJ. Li, L. Du, B. Qin et al. , \u201cFlm-101b: An open llm and\nhow to train it with $100 k budget,\u201d arXiv preprint\narXiv:2309.03852 , 2023.\n[103] T. Wei, L. Zhao, L. Zhang, B. Zhu, L. Wang, H. Yang,\nB. Li, C. Cheng, W. L \u00a8u, R. Hu et al. , \u201cSkywork: A\nmore open bilingual foundation model,\u201d arXiv preprint\narXiv:2310.19341 , 2023.\n[104] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat,\nY. Huang, M. Krikun, N. Shazeer, and Z. Chen,\n\u201cGshard: Scaling giant models with conditional com-\nputation and automatic sharding,\u201d in 9th International\nConference on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021 , 2021.\n[105] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P .\nde Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger,\nM. Petrov, H. Khlaaf, G. Sastry, P . Mishkin, B. Chan,\nS. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser,\nM. Bavarian, C. Winter, P . Tillet, F. P . Such, D. Cum-\nmings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-\nVoss, W. H. Guss, A. Nichol, A. Paino, N. Tezak,\nJ. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saun-\nders, C. Hesse, A. N. Carr, J. Leike, J. Achiam,\nV . Misra, E. Morikawa, A. Radford, M. Knight,\nM. Brundage, M. Murati, K. Mayer, P . Welinder,\nB. McGrew, D. Amodei, S. McCandlish, I. Sutskever,\nand W. Zaremba, \u201cEvaluating large language models\ntrained on code,\u201d CoRR , vol. abs/2107.03374, 2021.\n[106] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang,\nJ. Liu, X. Chen, Y. Zhao, Y. Lu, W. Liu, Z. Wu,\nW. Gong, J. Liang, Z. Shang, P . Sun, W. Liu, X. Ouyang,\nD. Yu, H. Tian, H. Wu, and H. Wang, \u201cERNIE 3.0:\nLarge-scale knowledge enhanced pre-training for lan-\nguage understanding and generation,\u201d CoRR , vol.\nabs/2107.02137, 2021.\n[107] O. Lieber, O. Sharir, B. Lenz, and Y. Shoham, \u201cJurassic-\n1: Technical details and evaluation,\u201d White Paper. AI21\nLabs, vol. 1, 2021.\n[108] B. Kim, H. Kim, S. Lee, G. Lee, D. Kwak, D. H. Jeon,\nS. Park, S. Kim, S. Kim, D. Seo, H. Lee, M. Jeong,\nS. Lee, M. Kim, S. Ko, S. Kim, T. Park, J. Kim, S. Kang,\nN. Ryu, K. M. Yoo, M. Chang, S. Suh, S. In, J. Park,\nK. Kim, H. Kim, J. Jeong, Y. G. Yeo, D. Ham, D. Park,\nM. Y. Lee, J. Kang, I. Kang, J. Ha, W. Park, and91\nN. Sung, \u201cWhat changes can large-scale language\nmodels bring? intensive study on hyperclova: Billions-\nscale korean generative pretrained transformers,\u201d in\nProceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2021, Virtual\nEvent / Punta Cana, Dominican Republic, 7-11 November,\n2021 . Association for Computational Linguistics,\n2021.\n[109] S. Wu, X. Zhao, T. Yu, R. Zhang, C. Shen, H. Liu, F. Li,\nH. Zhu, J. Luo, L. Xu et al. , \u201cYuan 1.0: Large-scale\npre-trained language model in zero-shot and few-shot\nlearning,\u201d arXiv preprint arXiv:2110.04725 , 2021.\n[110] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli,\nT. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-\nSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez,\nJ. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. B.\nBrown, J. Clark, S. McCandlish, C. Olah, and J. Ka-\nplan, \u201cA general language assistant as a laboratory\nfor alignment,\u201d CoRR , vol. abs/2112.00861, 2021.\n[111] S. Wang, Y. Sun, Y. Xiang, Z. Wu, S. Ding, W. Gong,\nS. Feng, J. Shang, Y. Zhao, C. Pang, J. Liu, X. Chen,\nY. Lu, W. Liu, X. Wang, Y. Bai, Q. Chen, L. Zhao,\nS. Li, P . Sun, D. Yu, Y. Ma, H. Tian, H. Wu, T. Wu,\nW. Zeng, G. Li, W. Gao, and H. Wang, \u201cERNIE 3.0\ntitan: Exploring larger-scale knowledge enhanced pre-\ntraining for language understanding and generation,\u201d\nCoRR , vol. abs/2112.12731, 2021.\n[112] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin,\nY. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph,\nL. Fedus, M. P . Bosma, Z. Zhou, T. Wang, Y. E.\nWang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-\nHellstern, T. Duke, L. Dixon, K. Zhang, Q. V . Le,\nY. Wu, Z. Chen, and C. Cui, \u201cGlam: Efficient scaling\nof language models with mixture-of-experts,\u201d in In-\nternational Conference on Machine Learning, ICML 2022,\n17-23 July 2022, Baltimore, Maryland, USA , 2022, pp.\n5547\u20135569.\n[113] S. Smith, M. Patwary, B. Norick, P . LeGresley, S. Rajb-\nhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas,\nV . Korthikanti, E. Zheng, R. Child, R. Y. Aminabadi,\nJ. Bernauer, X. Song, M. Shoeybi, Y. He, M. Hous-\nton, S. Tiwary, and B. Catanzaro, \u201cUsing deepspeed\nand megatron to train megatron-turing NLG 530b,\nA large-scale generative language model,\u201d CoRR , vol.\nabs/2201.11990, 2022.\n[114] Y. Li, D. H. Choi, J. Chung, N. Kushman, J. Schrit-\ntwieser, R. Leblond, T. Eccles, J. Keeling, F. Gi-\nmeno, A. D. Lago, T. Hubert, P . Choy, C. de Mas-\nson d\u2019Autume, I. Babuschkin, X. Chen, P . Huang,\nJ. Welbl, S. Gowal, A. Cherepanov, J. Molloy, D. J.\nMankowitz, E. S. Robson, P . Kohli, N. de Freitas,\nK. Kavukcuoglu, and O. Vinyals, \u201cCompetition-level\ncode generation with alphacode,\u201d Science , 2022.\n[115] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta,\nW. Hamza, H. Khan, C. Peris, S. Rawls, A. Rosen-\nbaum, A. Rumshisky, C. S. Prakash, M. Sridhar,\nF. Triefenbach, A. Verma, G. T \u00a8ur, and P . Natara-\njan, \u201cAlexatm 20b: Few-shot learning using a\nlarge-scale multilingual seq2seq model,\u201d CoRR , vol.\nabs/2208.01448, 2022.\n[116] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides,V . Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chad-\nwick, P . Thacker, L. Campbell-Gillingham, J. Ue-\nsato, P . Huang, R. Comanescu, F. Yang, A. See,\nS. Dathathri, R. Greig, C. Chen, D. Fritz, J. S. Elias,\nR. Green, S. Mokr \u00b4a, N. Fernando, B. Wu, R. Foley,\nS. Young, I. Gabriel, W. Isaac, J. Mellor, D. Hassabis,\nK. Kavukcuoglu, L. A. Hendricks, and G. Irving,\n\u201cImproving alignment of dialogue agents via targeted\nhuman judgements,\u201d CoRR , vol. abs/2209.14375, 2022.\n[117] H. Su, X. Zhou, H. Yu, Y. Chen, Z. Zhu, Y. Yu, and\nJ. Zhou, \u201cWelm: A well-read pre-trained language\nmodel for chinese,\u201d CoRR , vol. abs/2209.10372, 2022.\n[118] Y. Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So,\nS. Shakeri, X. Garcia, H. S. Zheng, J. Rao, A. Chowdh-\nery, D. Zhou, D. Metzler, S. Petrov, N. Houlsby, Q. V .\nLe, and M. Dehghani, \u201cTranscending scaling laws\nwith 0.1% extra compute,\u201d CoRR , vol. abs/2210.11399,\n2022.\n[119] X. Ren, P . Zhou, X. Meng, X. Huang, Y. Wang,\nW. Wang, P . Li, X. Zhang, A. Podolskiy, G. Arshinov,\nA. Bout, I. Piontkovskaya, J. Wei, X. Jiang, T. Su,\nQ. Liu, and J. Yao, \u201cPangu- \u03a3: Towards trillion pa-\nrameter language model with sparse heterogeneous\ncomputing,\u201d CoRR , vol. abs/2303.10845, 2023.\n[120] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lep-\nikhin, A. Passos, S. Shakeri, E. Taropa, P . Bailey,\nZ. Chen et al. , \u201cPalm 2 technical report,\u201d arXiv preprint\narXiv:2305.10403 , 2023.\n[121] A. Radford, R. J \u00b4ozefowicz, and I. Sutskever, \u201cLearn-\ning to generate reviews and discovering sentiment,\u201d\nCoRR , vol. abs/1704.01444, 2017.\n[122] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever\net al. , \u201cImproving language understanding by genera-\ntive pre-training,\u201d 2018.\n[123] B. McCann, N. S. Keskar, C. Xiong, and R. Socher,\n\u201cThe natural language decathlon: Multitask learning\nas question answering,\u201d CoRR , vol. abs/1806.08730,\n2018.\n[124] Y. Zhang, S. Sun, M. Galley, Y. Chen, C. Brockett,\nX. Gao, J. Gao, J. Liu, and B. Dolan, \u201cDIALOGPT :\nLarge-scale generative pre-training for conversational\nresponse generation,\u201d in Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics:\nSystem Demonstrations, ACL 2020, Online, July 5-10,\n2020 , A. Celikyilmaz and T. Wen, Eds. Association\nfor Computational Linguistics, 2020, pp. 270\u2013278.\n[125] D. Ham, J. Lee, Y. Jang, and K. Kim, \u201cEnd-to-end neu-\nral pipeline for goal-oriented dialogue systems using\nGPT-2,\u201d in Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020 . Association for Computational\nLinguistics, 2020, pp. 583\u2013592.\n[126] I. Drori, S. Tran, R. Wang, N. Cheng, K. Liu, L. Tang,\nE. Ke, N. Singh, T. L. Patti, J. Lynch, A. Shporer,\nN. Verma, E. Wu, and G. Strang, \u201cA neural network\nsolves and generates mathematics problems by pro-\ngram synthesis: Calculus, differential equations, linear\nalgebra, and more,\u201d CoRR , vol. abs/2112.15594, 2021.\n[127] A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han,\nJ. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hal-\nlacy, J. Heidecke, P . Shyam, B. Power, T. E. Nekoul,92\nG. Sastry, G. Krueger, D. Schnurr, F. P . Such, K. Hsu,\nM. Thompson, T. Khan, T. Sherbakov, J. Jang, P . Welin-\nder, and L. Weng, \u201cText and code embeddings by\ncontrastive pre-training,\u201d CoRR , vol. abs/2201.10005,\n2022.\n[128] J. Schulman, F. Wolski, P . Dhariwal, A. Radford,\nand O. Klimov, \u201cProximal policy optimization algo-\nrithms,\u201d arXiv preprint arXiv:1707.06347 , 2017.\n[129] N. Stiennon, L. Ouyang, J. Wu, D. M. Ziegler, R. Lowe,\nC. Voss, A. Radford, D. Amodei, and P . F. Chris-\ntiano, \u201cLearning to summarize from human feed-\nback,\u201d CoRR , vol. abs/2009.01325, 2020.\n[130] OpenAI, \u201cOur approach to alignment research,\u201d Ope-\nnAI Blog , August 2022.\n[131] \u2014\u2014, \u201cIntroducing chatgpt,\u201d OpenAI Blog , November\n2022.\n[132] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai,\nS. Kadavath, B. Mann, E. Perez, N. Schiefer,\nK. Ndousse, A. Jones, S. Bowman, A. Chen, T. Con-\nerly, N. DasSarma, D. Drain, N. Elhage, S. E. Showk,\nS. Fort, Z. Hatfield-Dodds, T. Henighan, D. Hernan-\ndez, T. Hume, J. Jacobson, S. Johnston, S. Kravec,\nC. Olsson, S. Ringer, E. Tran-Johnson, D. Amodei,\nT. Brown, N. Joseph, S. McCandlish, C. Olah, J. Ka-\nplan, and J. Clark, \u201cRed teaming language models\nto reduce harms: Methods, scaling behaviors, and\nlessons learned,\u201d CoRR , vol. abs/2209.07858, 2022.\n[133] OpenAI, \u201cGpt-4v(ision) system card,\u201d OpenAI , 2023.\n[134] \u2014\u2014, \u201cLessons learned on language model safety and\nmisuse,\u201d OpenAI blog , 2022.\n[135] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\npelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Hes-\nlow, J. Launay, Q. Malartic, B. Noune, B. Pannier,\nand G. Penedo, \u201cFalcon-40B: an open large language\nmodel with state-of-the-art performance,\u201d 2023.\n[136] L. Huawei Technologies Co., \u201cHuawei mindspore\nai development framework,\u201d in Artificial Intelligence\nTechnology . Springer, 2022, pp. 137\u2013162.\n[137] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li,\nC. Guestrin, P . Liang, and T. B. Hashimoto, \u201cStan-\nford alpaca: An instruction-following llama model,\u201d\nhttps://github.com/tatsu-lab/stanford alpaca, 2023.\n[138] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,\nL. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez,\nI. Stoica, and E. P . Xing, \u201cVicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality,\u201d\n2023. [Online]. Available: https://vicuna.lmsys.org\n[139] 2023. [Online]. Available: https://github.com/\nnebuly-ai/nebullvm/tree/main/apps/accelerate/\nchatllama\n[140] Y. You, \u201cColossalchat: An open-source\nsolution for cloning chatgpt with a complete\nrlhf pipeline,\u201d 2023. [Online]. Available:\nhttps://medium.com/@yangyou berkeley/\ncolossalchat-an-open-source-solution-for-cloning-\nchatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b\n[141] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru,\nA. Cappelli, H. Alobeidli, B. Pannier, E. Almazrouei,\nand J. Launay, \u201cThe RefinedWeb dataset for Falcon\nLLM: outperforming curated corpora with web data,\nand web data only,\u201d arXiv preprint arXiv:2306.01116 ,2023.\n[142] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li,\nC. Guestrin, P . Liang, and T. B. Hashimoto, \u201cStan-\nford alpaca: An instruction-following llama model,\u201d\nhttps://github.com/tatsu-lab/stanford alpaca, 2023.\n[143] Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith,\nD. Khashabi, and H. Hajishirzi, \u201cSelf-instruct: Align-\ning language model with self generated instructions,\u201d\nCoRR , vol. abs/2212.10560, 2022.\n[144] Alpaca-LoRA, \u201cInstruct-tune llama on consumer\nhardware,\u201d https://github.com/tloen/alpaca-lora,\n2023.\n[145] E. J. Hu, Y. Shen, P . Wallis, Z. Allen-Zhu, Y. Li, S. Wang,\nL. Wang, and W. Chen, \u201cLora: Low-rank adaptation of\nlarge language models,\u201d in The Tenth International Con-\nference on Learning Representations, ICLR 2022, Virtual\nEvent, April 25-29, 2022 . OpenReview.net, 2022.\n[146] X. Geng, A. Gudibande, H. Liu, E. Wallace, P . Abbeel,\nS. Levine, and D. Song, \u201cKoala: A dialogue model for\nacademic research,\u201d Blog post, April 2023.\n[147] Y. Ji, Y. Deng, Y. Gong, Y. Peng, Q. Niu, B. Ma, and\nX. Li, \u201cBelle: Be everyone\u2019s large language model en-\ngine,\u201d https://github.com/LianjiaTech/BELLE, 2023.\n[148] D. Eccleston, \u201cSharegpt,\u201d https://sharegpt.com/,\n2023.\n[149] H. Liu, C. Li, Q. Wu, and Y. J. Lee, \u201cVisual instruction\ntuning,\u201d CoRR , vol. abs/2304.08485, 2023.\n[150] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny,\n\u201cMinigpt-4: Enhancing vision-language understand-\ning with advanced large language models,\u201d CoRR , vol.\nabs/2304.10592, 2023.\n[151] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang,\nB. Li, P . Fung, and S. C. H. Hoi, \u201cInstructblip: Towards\ngeneral-purpose vision-language models with instruc-\ntion tuning,\u201d CoRR , vol. abs/2305.06500, 2023.\n[152] Y. Su, T. Lan, H. Li, J. Xu, Y. Wang, and D. Cai,\n\u201cPandagpt: One model to instruction-follow them\nall,\u201d 2023.\n[153] Y. Zhu, R. Kiros, R. S. Zemel, R. Salakhutdinov, R. Ur-\ntasun, A. Torralba, and S. Fidler, \u201cAligning books\nand movies: Towards story-like visual explanations\nby watching movies and reading books,\u201d in 2015 IEEE\nInternational Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015 . IEEE Computer\nSociety, 2015, pp. 19\u201327.\n[154] \u201cProject gutenberg.\u201d [Online]. Available: https://\nwww.gutenberg.org/\n[155] T. H. Trinh and Q. V . Le, \u201cA simple method for\ncommonsense reasoning,\u201d CoRR , vol. abs/1806.02847,\n2018.\n[156] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk,\nA. Farhadi, F. Roesner, and Y. Choi, \u201cDefending\nagainst neural fake news,\u201d in Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada , H. M.\nWallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch \u00b4e-\nBuc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 9051\u2013\n9062.\n[157] A. Gokaslan, V . C. E. Pavlick, and S. Tellex,\n\u201cOpenwebtext corpus,\u201d http://Skylion007.github.io/93\nOpenWebTextCorpus, 2019.\n[158] J. Baumgartner, S. Zannettou, B. Keegan, M. Squire,\nand J. Blackburn, \u201cThe pushshift reddit dataset,\u201d in\nProceedings of the Fourteenth International AAAI Con-\nference on Web and Social Media, ICWSM 2020, Held\nVirtually, Original Venue: Atlanta, Georgia, USA, June\n8-11, 2020 . AAAI Press, 2020, pp. 830\u2013839.\n[159] \u201cWikipedia.\u201d [Online]. Available: https://en.\nwikipedia.org/wiki/Main Page\n[160] \u201cBigquery dataset.\u201d [Online]. Available: https://\ncloud.google.com/bigquery?hl=zh-cn\n[161] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe,\nC. Foster, J. Phang, H. He, A. Thite, N. Nabeshima,\nS. Presser, and C. Leahy, \u201cThe pile: An 800gb dataset\nof diverse text for language modeling,\u201d CoRR , vol.\nabs/2101.00027, 2021.\n[162] H. Laurenc \u00b8on, L. Saulnier, T. Wang, C. Akiki, A. V .\ndel Moral, T. Le Scao, L. Von Werra, C. Mou, E. G.\nPonferrada, H. Nguyen et al. , \u201cThe bigscience roots\ncorpus: A 1.6 tb composite multilingual dataset,\u201d in\nThirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track , 2022.\n[163] \u201cCommon crawl.\u201d [Online]. Available: https://\ncommoncrawl.org/\n[164] \u201cA reproduction version of cc-stories on hugging\nface.\u201d [Online]. Available: https://huggingface.co/\ndatasets/spacemanidol/cc-stories\n[165] B. Wang and A. Komatsuzaki, \u201cGPT-J-6B: A 6 Billion\nParameter Autoregressive Language Model,\u201d https://\ngithub.com/kingoflolz/mesh-transformer-jax, 2021.\n[166] S. Mishra, D. Khashabi, C. Baral, and H. Ha-\njishirzi, \u201cCross-task generalization via natural lan-\nguage crowdsourcing instructions,\u201d in Proceedings of\nthe 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022 , S. Muresan, P . Nakov,\nand A. Villavicencio, Eds., 2022, pp. 3470\u20133487.\n[167] S. H. Bach, V . Sanh, Z. X. Yong, A. Webson, C. Raffel,\nN. V . Nayak, A. Sharma, T. Kim, M. S. Bari, T. F \u00b4evry,\nZ. Alyafeai, M. Dey, A. Santilli, Z. Sun, S. Ben-David,\nC. Xu, G. Chhablani, H. Wang, J. A. Fries, M. S.\nAlShaibani, S. Sharma, U. Thakker, K. Almubarak,\nX. Tang, D. R. Radev, M. T. Jiang, and A. M. Rush,\n\u201cPromptsource: An integrated development environ-\nment and repository for natural language prompts,\u201d\ninACL (demo) . Association for Computational Lin-\nguistics, 2022, pp. 93\u2013104.\n[168] T. Tang, J. Li, W. X. Zhao, and J. Wen, \u201cMVP: multi-\ntask supervised pre-training for natural language gen-\neration,\u201d CoRR , vol. abs/2206.12131, 2022.\n[169] H. Nguyen, S. Suri, K. Tsui, Shahules786, T. team, and\nC. Schuhmann, \u201cThe oig dataset,\u201d https://laion.ai/\nblog/oig-dataset/, 2023.\n[170] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen,\nN. DasSarma, D. Drain, S. Fort, D. Ganguli,\nT. Henighan, N. Joseph, S. Kadavath, J. Kernion,\nT. Conerly, S. E. Showk, N. Elhage, Z. Hatfield-Dodds,\nD. Hernandez, T. Hume, S. Johnston, S. Kravec,\nL. Lovitt, N. Nanda, C. Olsson, D. Amodei, T. B.\nBrown, J. Clark, S. McCandlish, C. Olah, B. Mann, and\nJ. Kaplan, \u201cTraining a helpful and harmless assistantwith reinforcement learning from human feedback,\u201d\nCoRR , vol. abs/2204.05862, 2022. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2204.05862\n[171] B. Guo, X. Zhang, Z. Wang, M. Jiang, J. Nie, Y. Ding,\nJ. Yue, and Y. Wu, \u201cHow close is chatgpt to human ex-\nperts? comparison corpus, evaluation, and detection,\u201d\narXiv preprint arXiv:2301.07597 , 2023.\n[172] M. Conover, M. Hayes, A. Mathur, J. Xie, J. Wan,\nS. Shah, A. Ghodsi, P . Wendell, M. Zaharia, and R. Xin.\n(2023) Free dolly: Introducing the world\u2019s first truly\nopen instruction-tuned llm.\n[173] A. K \u00a8opf, Y. Kilcher, D. von R \u00a8utte, S. Anagnos-\ntidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M.\nDuc, O. Stanley, R. Nagyfi et al. , \u201cOpenassistant\nconversations\u2013democratizing large language model\nalignment,\u201d arXiv preprint arXiv:2304.07327 , 2023.\n[174] J. Cheung, \u201cGuanaco - generative universal assis-\ntant for natural-language adaptive context-aware om-\nnilingual outputs,\u201d https://guanaco-model.github.\nio/, 2023.\n[175] C. Xu, D. Guo, N. Duan, and J. McAuley, \u201cBaize: An\nopen-source chat model with parameter-efficient tun-\ning on self-chat data,\u201d arXiv preprint arXiv:2304.01196 ,\n2023.\n[176] Y. Ji, Y. Gong, Y. Deng, Y. Peng, Q. Niu, B. Ma,\nand X. Li, \u201cTowards better instruction following\nlanguage models for chinese: Investigating the im-\npact of training data and evaluation,\u201d arXiv preprint\narXiv:2304.07854 , 2023.\n[177] K. Ethayarajh, Y. Choi, and S. Swayamdipta, \u201cUn-\nderstanding dataset difficulty with V-usable informa-\ntion,\u201d in Proceedings of the 39th International Conference\non Machine Learning , 2022, pp. 5988\u20136008.\n[178] N. Lambert, L. Tunstall, N. Rajani,\nand T. Thrush. (2023) Huggingface h4\nstack exchange preference dataset. [On-\nline]. Available: https://huggingface.co/datasets/\nHuggingFaceH4/stack-exchange-preferences\n[179] R. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai,\nD. Yang, and S. Vosoughi, \u201cTraining socially aligned\nlanguage models in simulated human society,\u201d CoRR ,\nvol. abs/2305.16960, 2023.\n[180] G. Xu, J. Liu, M. Yan, H. Xu, J. Si, Z. Zhou, P . Yi, X. Gao,\nJ. Sang, R. Zhang, J. Zhang, C. Peng, F. Huang, and\nJ. Zhou, \u201cCvalues: Measuring the values of chinese\nlarge language models from safety to responsibility,\u201d\n2023.\n[181] J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y. Wang, and\nY. Yang, \u201cSafe rlhf: Safe reinforcement learning from\nhuman feedback,\u201d arXiv preprint arXiv:2310.12773 ,\n2023.\n[182] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika,\nZ. Alyafeai, A. Chaffin, A. Stiegler, A. Raja, M. Dey,\nM. S. Bari, C. Xu, U. Thakker, S. S. Sharma,\nE. Szczechla, T. Kim, G. Chhablani, N. V . Nayak,\nD. Datta, J. Chang, M. T. Jiang, H. Wang, M. Man-\nica, S. Shen, Z. X. Yong, H. Pandey, R. Bawden,\nT. Wang, T. Neeraj, J. Rozen, A. Sharma, A. Santilli,\nT. F\u00b4evry, J. A. Fries, R. Teehan, T. L. Scao, S. Bider-\nman, L. Gao, T. Wolf, and A. M. Rush, \u201cMultitask\nprompted training enables zero-shot task generaliza-94\ntion,\u201d in The Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29,\n2022 . OpenReview.net, 2022.\n[183] S. Longpre, L. Hou, T. Vu, A. Webson, H. W.\nChung, Y. Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei\net al. , \u201cThe flan collection: Designing data and meth-\nods for effective instruction tuning,\u201d arXiv preprint\narXiv:2301.13688 , 2023.\n[184] K. Cobbe, V . Kosaraju, M. Bavarian, J. Hilton,\nR. Nakano, C. Hesse, and J. Schulman, \u201cTraining\nverifiers to solve math word problems,\u201d CoRR , vol.\nabs/2110.14168, 2021.\n[185] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth,\nand J. Berant, \u201cDid aristotle use a laptop? A question\nanswering benchmark with implicit reasoning strate-\ngies,\u201d Trans. Assoc. Comput. Linguistics , vol. 9, pp. 346\u2013\n361, 2021.\n[186] O. Camburu, B. Shillingford, P . Minervini,\nT. Lukasiewicz, and P . Blunsom, \u201cMake up your\nmind! adversarial generation of inconsistent natural\nlanguage explanations,\u201d in Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\nD. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault,\nEds. Association for Computational Linguistics,\n2020, pp. 4157\u20134165.\n[187] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue,\nA. Moi, P . Cistac, T. Rault, R. Louf, M. Funtowicz,\nJ. Davison, S. Shleifer, P . von Platen, C. Ma, Y. Jer-\nnite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. M. Rush, \u201cTransformers: State-of-\nthe-art natural language processing,\u201d in Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, EMNLP 2020\n- Demos, Online, November 16-20, 2020 . Association\nfor Computational Linguistics, 2020, pp. 38\u201345.\n[188] J. Bradbury, R. Frostig, P . Hawkins, M. J. Johnson,\nC. Leary, D. Maclaurin, G. Necula, A. Paszke,\nJ. VanderPlas, S. Wanderman-Milne, and Q. Zhang,\n\u201cJAX: composable transformations of Python+NumPy\nprograms,\u201d 2018. [Online]. Available: http://github.\ncom/google/jax\n[189] Z. Bian, H. Liu, B. Wang, H. Huang, Y. Li, C. Wang,\nF. Cui, and Y. You, \u201cColossal-ai: A unified deep learn-\ning system for large-scale parallel training,\u201d CoRR ,\nvol. abs/2110.14883, 2021.\n[190] J. Fang, Y. Yu, S. Li, Y. You, and J. Zhou, \u201cPatrick-\nstar: Parallel training of pre-trained models via\na chunk-based memory management,\u201d CoRR , vol.\nabs/2108.05818, 2021.\n[191] \u201cBmtrain: Effient training for big models.\u201d [Online].\nAvailable: https://github.com/OpenBMB/BMTrain\n[192] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang,\n\u201cFastmoe: A fast mixture-of-expert training system,\u201d\nCoRR , vol. abs/2103.13262, 2021.\n[193] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng,\nC. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica,\n\u201cEfficient memory management for large language\nmodel serving with pagedattention,\u201d in Proceedings of\nthe ACM SIGOPS 29th Symposium on Operating Systems\nPrinciples , 2023.[194] (2023) Deepspeed-mii. [Online]. Available: https:\n//github.com/microsoft/DeepSpeed-MII\n[195] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford,\nD. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel,\nG. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux,\nP . Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and\nW. E. Sayed, \u201cMistral 7b,\u201d 2023.\n[196] Z. Yao, R. Y. Aminabadi, O. Ruwase, S. Rajbhandari,\nX. Wu, A. A. Awan, J. Rasley, M. Zhang, C. Li,\nC. Holmes, Z. Zhou, M. Wyatt, M. Smith, L. Kurilenko,\nH. Qin, M. Tanaka, S. Che, S. L. Song, and Y. He,\n\u201cDeepSpeed-Chat: Easy, Fast and Affordable RLHF\nTraining of ChatGPT-like Models at All Scales,\u201d arXiv\npreprint arXiv:2308.01320 , 2023.\n[197] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad-\nbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. K \u00a8opf, E. Z. Yang, Z. De-\nVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, \u201cPytorch: An imper-\native style, high-performance deep learning library,\u201d\ninAdvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information Process-\ning Systems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada , H. M. Wallach, H. Larochelle,\nA. Beygelzimer, F. d\u2019Alch \u00b4e-Buc, E. B. Fox, and R. Gar-\nnett, Eds., 2019, pp. 8024\u20138035.\n[198] M. Abadi, P . Barham, J. Chen, Z. Chen, A. Davis,\nJ. Dean, M. Devin, S. Ghemawat, G. Irving, M. Is-\nard, M. Kudlur, J. Levenberg, R. Monga, S. Moore,\nD. G. Murray, B. Steiner, P . A. Tucker, V . Vasudevan,\nP . Warden, M. Wicke, Y. Yu, and X. Zheng, \u201cTensor-\nflow: A system for large-scale machine learning,\u201d in\n12th USENIX Symposium on Operating Systems Design\nand Implementation, OSDI 2016, Savannah, GA, USA,\nNovember 2-4, 2016 , K. Keeton and T. Roscoe, Eds.\nUSENIX Association, 2016, pp. 265\u2013283.\n[199] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang,\nT. Xiao, B. Xu, C. Zhang, and Z. Zhang, \u201cMxnet:\nA flexible and efficient machine learning library\nfor heterogeneous distributed systems,\u201d CoRR , vol.\nabs/1512.01274, 2015.\n[200] Y. Ma, D. Yu, T. Wu, and H. Wang, \u201cPaddlepaddle: An\nopen-source deep learning platform from industrial\npractice,\u201d Frontiers of Data and Domputing , vol. 1, no. 1,\np. 105, 2019.\n[201] J. Yuan, X. Li, C. Cheng, J. Liu, R. Guo, S. Cai, C. Yao,\nF. Yang, X. Yi, C. Wu, H. Zhang, and J. Zhao, \u201cOne-\nflow: Redesign the distributed deep learning frame-\nwork from scratch,\u201d CoRR , vol. abs/2110.15032, 2021.\n[202] S. Roller, E. Dinan, N. Goyal, D. Ju, M. Williamson,\nY. Liu, J. Xu, M. Ott, E. M. Smith, Y. Boureau, and\nJ. Weston, \u201cRecipes for building an open-domain chat-\nbot,\u201d in Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics:\nMain Volume, EACL 2021, Online, April 19 - 23, 2021 ,\n2021, pp. 300\u2013325.\n[203] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer,\nH. Michalewski, V . V . Ramasesh, A. Slone, C. Anil,\nI. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur,\nG. Gur-Ari, and V . Misra, \u201cSolving quantitative rea-\nsoning problems with language models,\u201d CoRR , vol.95\nabs/2206.14858, 2022.\n[204] T. Saier, J. Krause, and M. F \u00a8arber, \u201cunarxive 2022:\nAll arxiv publications pre-processed for nlp, includ-\ning structured full-text and citation network,\u201d arXiv\npreprint arXiv:2303.14957 , 2023.\n[205] H. A. Simon, \u201cExperiments with a heuristic compiler,\u201d\nJ. ACM , vol. 10, no. 4, pp. 493\u2013506, 1963.\n[206] Z. Manna and R. J. Waldinger, \u201cToward automatic\nprogram synthesis,\u201d Commun. ACM , vol. 14, no. 3, pp.\n151\u2013165, 1971.\n[207] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong,\nL. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou,\n\u201cCodebert: A pre-trained model for programming and\nnatural languages,\u201d in Findings of EMNLP , 2020.\n[208] J. Austin, A. Odena, M. I. Nye, M. Bosma,\nH. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry,\nQ. V . Le, and C. Sutton, \u201cProgram synthesis with large\nlanguage models,\u201d CoRR , vol. abs/2108.07732, 2021.\n[209] S. Black, L. Gao, P . Wang, C. Leahy, and S. Bi-\nderman, \u201cGPT-Neo: Large Scale Autoregressive Lan-\nguage Modeling with Mesh-Tensorflow,\u201d 2021.\n[210] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn,\n\u201cA systematic evaluation of large language models of\ncode,\u201d in MAPS@PLDI , 2022.\n[211] A. Madaan, S. Zhou, U. Alon, Y. Yang, and G. Neubig,\n\u201cLanguage models of code are few-shot commonsense\nlearners,\u201d in Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December 7-11,\n2022 , Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds.\nAssociation for Computational Linguistics, 2022, pp.\n1384\u20131403.\n[212] S. Longpre, G. Yauney, E. Reif, K. Lee, A. Roberts,\nB. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno et al. ,\n\u201cA pretrainer\u2019s guide to training data: Measuring\nthe effects of data age, domain coverage, quality, &\ntoxicity,\u201d arXiv preprint arXiv:2305.13169 , 2023.\n[213] D. Chen, Y. Huang, Z. Ma, H. Chen, X. Pan, C. Ge,\nD. Gao, Y. Xie, Z. Liu, J. Gao, Y. Li, B. Ding, and\nJ. Zhou, \u201cData-juicer: A one-stop data processing sys-\ntem for large language models,\u201d 2023.\n[214] D. Hernandez, T. B. Brown, T. Conerly, N. DasSarma,\nD. Drain, S. E. Showk, N. Elhage, Z. Hatfield-Dodds,\nT. Henighan, T. Hume, S. Johnston, B. Mann, C. Olah,\nC. Olsson, D. Amodei, N. Joseph, J. Kaplan, and S. Mc-\nCandlish, \u201cScaling laws and interpretability of learn-\ning from repeated data,\u201d CoRR , vol. abs/2205.10487,\n2022.\n[215] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi,\n\u201cThe curious case of neural text degeneration,\u201d in\n8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .\nOpenReview.net, 2020.\n[216] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck,\nC. Callison-Burch, and N. Carlini, \u201cDeduplicating\ntraining data makes language models better,\u201d in Pro-\nceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022 , 2022, pp. 8424\u2013\n8445.\n[217] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tram `er,and C. Zhang, \u201cQuantifying memorization across\nneural language models,\u201d CoRR , 2022.\n[218] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski,\nA. Herbert-Voss, K. Lee, A. Roberts, T. B. Brown,\nD. Song, \u00b4U. Erlingsson, A. Oprea, and C. Raffel, \u201cEx-\ntracting training data from large language models,\u201d\nin30th USENIX Security Symposium, USENIX Security\n2021, August 11-13, 2021 , 2021, pp. 2633\u20132650.\n[219] N. Kandpal, E. Wallace, and C. Raffel, \u201cDeduplicating\ntraining data mitigates privacy risks in language mod-\nels,\u201d in International Conference on Machine Learning,\nICML 2022, 17-23 July 2022, Baltimore, Maryland, USA .\nPMLR, 2022, pp. 10 697\u201310 707.\n[220] J. D. Lafferty, A. McCallum, and F. C. N. Pereira,\n\u201cConditional random fields: Probabilistic models for\nsegmenting and labeling sequence data,\u201d in Proceed-\nings of the Eighteenth International Conference on Machine\nLearning (ICML 2001), Williams College, Williamstown,\nMA, USA, June 28 - July 1, 2001 , C. E. Brodley and\nA. P . Danyluk, Eds. Morgan Kaufmann, 2001, pp.\n282\u2013289.\n[221] P . Gage, \u201cA new algorithm for data compression,\u201d C\nUsers Journal , vol. 12, no. 2, pp. 23\u201338, 1994.\n[222] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural ma-\nchine translation of rare words with subword units,\u201d\ninProceedings of the 54th Annual Meeting of the Asso-\nciation for Computational Linguistics, ACL 2016, August\n7-12, 2016, Berlin, Germany, Volume 1: Long Papers . The\nAssociation for Computer Linguistics, 2016.\n[223] M. Schuster and K. Nakajima, \u201cJapanese and korean\nvoice search,\u201d in 2012 IEEE international conference on\nacoustics, speech and signal processing (ICASSP) . IEEE,\n2012, pp. 5149\u20135152.\n[224] Y. Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi,\nW. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,\nJ. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser,\nS. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens,\nG. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. Corrado,\nM. Hughes, and J. Dean, \u201cGoogle\u2019s neural machine\ntranslation system: Bridging the gap between human\nand machine translation,\u201d CoRR , vol. abs/1609.08144,\n2016.\n[225] T. Kudo, \u201cSubword regularization: Improving neural\nnetwork translation models with multiple subword\ncandidates,\u201d in Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics, ACL\n2018, Melbourne, Australia, July 15-20, 2018, Volume 1:\nLong Papers , I. Gurevych and Y. Miyao, Eds. Associ-\nation for Computational Linguistics, 2018, pp. 66\u201375.\n[226] T. Kudo and J. Richardson, \u201cSentencepiece: A simple\nand language independent subword tokenizer and\ndetokenizer for neural text processing,\u201d in Proceedings\nof the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2018: System Demonstra-\ntions, Brussels, Belgium, October 31 - November 4, 2018 ,\nE. Blanco and W. Lu, Eds. Association for Computa-\ntional Linguistics, 2018.\n[227] M. Davis and M. D \u00a8urst, \u201cUnicode normalization\nforms,\u201d 2001.\n[228] P . Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak,96\nand I. Sutskever, \u201cDeep double descent: Where bigger\nmodels and more data hurt,\u201d in 8th International Con-\nference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020 . OpenReview.net,\n2020.\n[229] K. Tirumala, D. Simig, A. Aghajanyan, and A. S.\nMorcos, \u201cD4: Improving llm pretraining via document\nde-duplication and diversification,\u201d arXiv preprint\narXiv:2308.12284 , 2023.\n[230] Z. Shen, T. Tao, L. Ma, W. Neiswanger, J. Hestness,\nN. Vassilieva, D. Soboleva, and E. Xing, \u201cSlimpajama-\ndc: Understanding data combinations for llm train-\ning,\u201d arXiv preprint arXiv:2309.10818 , 2023.\n[231] S. M. Xie, S. Santurkar, T. Ma, and P . Liang, \u201cData\nselection for language models via importance resam-\npling,\u201d arXiv preprint arXiv:2302.03169 , 2023.\n[232] X. Wang, W. Zhou, Q. Zhang, J. Zhou, S. Gao,\nJ. Wang, M. Zhang, X. Gao, Y. Chen, and T. Gui,\n\u201cFarewell to aimless large-scale pretraining: Influ-\nential subset selection for language model,\u201d arXiv\npreprint arXiv:2305.12816 , 2023.\n[233] D. Paperno, G. Kruszewski, A. Lazaridou, Q. N.\nPham, R. Bernardi, S. Pezzelle, M. Baroni, G. Boleda,\nand R. Fern \u00b4andez, \u201cThe LAMBADA dataset: Word\nprediction requiring a broad discourse context,\u201d in\nACL (1) . The Association for Computer Linguistics,\n2016.\n[234] M. F. Chen, N. Roberts, K. Bhatia, J. Wang, C. Zhang,\nF. Sala, and C. R \u00b4e, \u201cSkill-it! a data-driven skills frame-\nwork for understanding and training language mod-\nels,\u201d arXiv preprint arXiv:2307.14430 , 2023.\n[235] B. Rozi `ere, J. Gehring, F. Gloeckle, S. Sootla,\nI. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez,\nJ. Rapin, A. Kozhevnikov, I. Evtimov, J. Bitton,\nM. Bhatt, C. Canton-Ferrer, A. Grattafiori, W. Xiong,\nA. D \u00b4efossez, J. Copet, F. Azhar, H. Touvron, L. Mar-\ntin, N. Usunier, T. Scialom, and G. Synnaeve, \u201cCode\nllama: Open foundation models for code,\u201d CoRR , vol.\nabs/2308.12950, 2023.\n[236] Y. Bengio, J. Louradour, R. Collobert, and J. Weston,\n\u201cCurriculum learning,\u201d in ICML , 2009, pp. 41\u201348.\n[237] C. Xu, C. Rosset, L. Del Corro, S. Mahajan, J. McAuley,\nJ. Neville, A. H. Awadallah, and N. Rao, \u201cContrastive\npost-training large language models on data curricu-\nlum,\u201d arXiv preprint arXiv:2310.02263 , 2023.\n[238] S. Tworkowski, K. Staniszewski, M. Pacek, Y. Wu,\nH. Michalewski, and P . Milos, \u201cFocused transformer:\nContrastive training for context scaling,\u201d CoRR , vol.\nabs/2307.03170, 2023.\n[239] Z. Azerbayev, H. Schoelkopf, K. Paster, M. D. Santos,\nS. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and\nS. Welleck, \u201cLlemma: An open language model for\nmathematics,\u201d arXiv preprint arXiv:2310.10631 , 2023.\n[240] S. Chen, S. Wong, L. Chen, and Y. Tian, \u201cExtending\ncontext window of large language models via posi-\ntional interpolation,\u201d CoRR , vol. abs/2306.15595, 2023.\n[241] G. Wenzek, M.-A. Lachaux, A. Conneau, V . Chaud-\nhary, F. Guzm \u00b4an, A. Joulin, and \u00b4E. Grave, \u201cCcnet:\nExtracting high quality monolingual datasets from\nweb crawl data,\u201d in Proceedings of the Twelfth Language\nResources and Evaluation Conference , 2020, pp. 4003\u20134012.\n[242] A. Joulin, E. Grave, P . Bojanowski, and T. Mikolov,\n\u201cBag of tricks for efficient text classification,\u201d in EACL ,\n2017, pp. 427\u2013431.\n[243] D. Chen, Y. Huang, Z. Ma, H. Chen, X. Pan, C. Ge,\nD. Gao, Y. Xie, Z. Liu, J. Gao et al. , \u201cData-juicer: A\none-stop data processing system for large language\nmodels,\u201d arXiv preprint arXiv:2309.02033 , 2023.\n[244] B. Zhang, B. Ghorbani, A. Bapna, Y. Cheng, X. Garcia,\nJ. Shen, and O. Firat, \u201cExamining scaling and transfer\nof language model architectures for machine transla-\ntion,\u201d in International Conference on Machine Learning,\nICML 2022, 17-23 July 2022, Baltimore, Maryland, USA ,\n2022, pp. 26 176\u201326 192.\n[245] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang,\nJ. Gao, M. Zhou, and H. Hon, \u201cUnified language\nmodel pre-training for natural language understand-\ning and generation,\u201d in Advances in Neural Informa-\ntion Processing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS 2019,\nDecember 8-14, 2019, Vancouver, BC, Canada , 2019, pp.\n13 042\u201313 054.\n[246] A. Clark, D. de Las Casas, A. Guy, A. Mensch,\nM. Paganini, J. Hoffmann, B. Damoc, B. A. Hecht-\nman, T. Cai, S. Borgeaud, G. van den Driessche,\nE. Rutherford, T. Hennigan, M. J. Johnson, A. Cassirer,\nC. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osin-\ndero, O. Vinyals, M. Ranzato, J. W. Rae, E. Elsen,\nK. Kavukcuoglu, and K. Simonyan, \u201cUnified scaling\nlaws for routed language models,\u201d in International\nConference on Machine Learning, ICML 2022, 17-23 July\n2022, Baltimore, Maryland, USA , 2022, pp. 4057\u20134086.\n[247] A. Gu, K. Goel, and C. R \u00b4e, \u201cEfficiently modeling\nlong sequences with structured state spaces,\u201d\ninThe Tenth International Conference on Learning\nRepresentations, ICLR 2022, Virtual Event, April 25-29,\n2022 . OpenReview.net, 2022. [Online]. Available:\nhttps://openreview.net/forum?id=uYLFoz1vlAC\n[248] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur,\n\u201cLong range language modeling via gated state\nspaces,\u201d CoRR , vol. abs/2206.13947, 2022. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2206.13947\n[249] T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra,\nand C. R \u00b4e, \u201cHungry hungry hippos: Towards\nlanguage modeling with state space models,\u201d\nCoRR , vol. abs/2212.14052, 2022. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2212.14052\n[250] M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao,\nS. Baccus, Y. Bengio, S. Ermon, and C. R \u00b4e, \u201cHyena hi-\nerarchy: Towards larger convolutional language mod-\nels,\u201d in ICML , 2023.\n[251] B. Peng, E. Alcaide, Q. Anthony, A. Albalak,\nS. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella,\nK. K. G. V ., X. He, H. Hou, P . Kazienko, J. Kocon,\nJ. Kong, B. Koptyra, H. Lau, K. S. I. Mantri, F. Mom,\nA. Saito, X. Tang, B. Wang, J. S. Wind, S. Wozniak,\nR. Zhang, Z. Zhang, Q. Zhao, P . Zhou, J. Zhu, and\nR. Zhu, \u201cRWKV: reinventing rnns for the transformer\nera,\u201d CoRR , vol. abs/2305.13048, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2305.13048\n[252] Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue,97\nJ. Wang, and F. Wei, \u201cRetentive network: A succes-\nsor to transformer for large language models,\u201d arXiv\npreprint arXiv:2307.08621 , 2023.\n[253] J. T. Smith, A. Warrington, and S. Linderman, \u201cSim-\nplified state space layers for sequence modeling,\u201d in\nICLR , 2023.\n[254] A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gul-\ncehre, R. Pascanu, and S. De, \u201cResurrecting recurrent\nneural networks for long sequences,\u201d in ICML , 2023.\n[255] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou,\nD. Yin, J. Lin, X. Zou, Z. Shao, H. Yang, and J. Tang,\n\u201cCogview: Mastering text-to-image generation via\ntransformers,\u201d in Advances in Neural Information Pro-\ncessing Systems 34: Annual Conference on Neural Infor-\nmation Processing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual , 2021, pp. 19 822\u201319 835.\n[256] L. J. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer normal-\nization,\u201d vol. abs/1607.06450, 2016.\n[257] B. Zhang and R. Sennrich, \u201cRoot mean square layer\nnormalization,\u201d in Advances in Neural Information Pro-\ncessing Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada , 2019, pp. 12 360\u2013\n12 371.\n[258] H. Wang, S. Ma, L. Dong, S. Huang, D. Zhang, and\nF. Wei, \u201cDeepnet: Scaling transformers to 1, 000 lay-\ners,\u201d vol. abs/2203.00555, 2022.\n[259] V . Nair and G. E. Hinton, \u201cRectified linear units im-\nprove restricted boltzmann machines,\u201d in Proceedings\nof the 27th international conference on machine learning\n(ICML-10) , 2010, pp. 807\u2013814.\n[260] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy,\nand S. R. Bowman, \u201cGLUE: A multi-task bench-\nmark and analysis platform for natural language un-\nderstanding,\u201d in Proceedings of the Workshop: Analyz-\ning and Interpreting Neural Networks for NLP , Black-\nboxNLP@EMNLP 2018, Brussels, Belgium, November 1,\n2018 , T. Linzen, G. Chrupala, and A. Alishahi, Eds.\nAssociation for Computational Linguistics, 2018, pp.\n353\u2013355.\n[261] P . Ramachandran, B. Zoph, and Q. V . Le,\n\u201cSearching for activation functions,\u201d arXiv preprint\narXiv:1710.05941 , 2017.\n[262] N. Shazeer, \u201cGLU variants improve transformer,\u201d vol.\nabs/2002.05202, 2020.\n[263] J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, \u201cRoformer: En-\nhanced transformer with rotary position embedding,\u201d\nvol. abs/2104.09864, 2021.\n[264] O. Press, N. A. Smith, and M. Lewis, \u201cTrain short, test\nlong: Attention with linear biases enables input length\nextrapolation,\u201d in The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April\n25-29, 2022 , 2022.\n[265] S. Ioffe and C. Szegedy, \u201cBatch normalization:\nAccelerating deep network training by reducing\ninternal covariate shift,\u201d in Proceedings of the 32nd\nInternational Conference on Machine Learning, ICML\n2015, Lille, France, 6-11 July 2015 , ser. JMLR Workshop\nand Conference Proceedings, F. R. Bach and D. M.\nBlei, Eds., vol. 37. JMLR.org, 2015, pp. 448\u2013456.\n[Online]. Available: http://proceedings.mlr.press/v37/ioffe15.html\n[266] S. Narang, H. W. Chung, Y. Tay, L. Fedus, T. F \u00b4evry,\nM. Matena, K. Malkan, N. Fiedel, N. Shazeer, Z. Lan,\nY. Zhou, W. Li, N. Ding, J. Marcus, A. Roberts,\nand C. Raffel, \u201cDo transformer modifications transfer\nacross implementations and applications?\u201d in Proceed-\nings of the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 7-11 November, 2021 ,\n2021, pp. 5758\u20135773.\n[267] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing,\nH. Zhang, Y. Lan, L. Wang, and T. Liu, \u201cOn layer nor-\nmalization in the transformer architecture,\u201d in ICML ,\n2020.\n[268] A. Baevski and M. Auli, \u201cAdaptive input represen-\ntations for neural language modeling,\u201d in 7th Inter-\nnational Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019 . Open-\nReview.net, 2019.\n[269] L. Liu, X. Liu, J. Gao, W. Chen, and J. Han, \u201cUnder-\nstanding the difficulty of training transformers,\u201d in\nProceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020 . Association for Computational\nLinguistics, 2020, pp. 5747\u20135763.\n[270] D. Hendrycks and K. Gimpel, \u201cGaussian error linear\nunits (gelus),\u201d arXiv preprint arXiv:1606.08415 , 2016.\n[271] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier,\n\u201cLanguage modeling with gated convolutional net-\nworks,\u201d in Proceedings of the 34th International Confer-\nence on Machine Learning, ICML 2017, Sydney, NSW,\nAustralia, 6-11 August 2017 , 2017, pp. 933\u2013941.\n[272] T. L. Scao, T. Wang, D. Hesslow, S. Bekman, M. S. Bari,\nS. Biderman, H. Elsahar, N. Muennighoff, J. Phang,\nO. Press, C. Raffel, V . Sanh, S. Shen, L. Sutawika, J. Tae,\nZ. X. Yong, J. Launay, and I. Beltagy, \u201cWhat language\nmodel to train if you have one million GPU hours?\u201d in\nFindings of the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates, Decem-\nber 7-11, 2022 , 2022, pp. 765\u2013782.\n[273] P . Shaw, J. Uszkoreit, and A. Vaswani, \u201cSelf-\nattention with relative position representations,\u201d\ninProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-\nHLT, New Orleans, Louisiana, USA, June 1-6, 2018,\nVolume 2 (Short Papers) , M. A. Walker, H. Ji,\nand A. Stent, Eds. Association for Computational\nLinguistics, 2018, pp. 464\u2013468. [Online]. Available:\nhttps://doi.org/10.18653/v1/n18-2074\n[274] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell,\nQ. V . Le, and R. Salakhutdinov, \u201cTransformer-xl:\nAttentive language models beyond a fixed-length\ncontext,\u201d in Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Volume\n1: Long Papers , A. Korhonen, D. R. Traum, and\nL. M `arquez, Eds. Association for Computational\nLinguistics, 2019, pp. 2978\u20132988. [Online]. Available:\nhttps://doi.org/10.18653/v1/p19-1285\n[275] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdi-98\nnov, and Q. V . Le, \u201cXlnet: Generalized autoregressive\npretraining for language understanding,\u201d Advances in\nneural information processing systems , vol. 32, 2019.\n[276] B. Peng, J. Quesnelle, H. Fan, and E. Shippole, \u201cYarn:\nEfficient context window extension of large language\nmodels,\u201d CoRR , vol. abs/2309.00071, 2023.\n[277] Y. Sun, L. Dong, B. Patra, S. Ma, S. Huang,\nA. Benhaim, V . Chaudhary, X. Song, and F. Wei,\n\u201cA length-extrapolatable transformer,\u201d CoRR , vol.\nabs/2212.10554, 2022. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2212.10554\n[278] H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A.\nSmith, and L. Kong, \u201cRandom feature attention,\u201d in\n9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\n[279] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie,\nC. Alberti, S. Onta \u02dcn\u00b4on, P . Pham, A. Ravula, Q. Wang,\nL. Yang, and A. Ahmed, \u201cBig bird: Transformers for\nlonger sequences,\u201d in Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual , 2020.\n[280] R. Child, S. Gray, A. Radford, and I. Sutskever, \u201cGen-\nerating long sequences with sparse transformers,\u201d\nCoRR , vol. abs/1904.10509, 2019.\n[281] N. Shazeer, \u201cFast transformer decoding: One write-\nhead is all you need,\u201d CoRR , vol. abs/1911.02150,\n2019. [Online]. Available: http://arxiv.org/abs/1911.\n02150\n[282] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy,\nF. Lebr \u00b4on, and S. Sanghai, \u201cGqa: Training general-\nized multi-query transformer models from multi-head\ncheckpoints,\u201d arXiv preprint arXiv:2305.13245 , 2023.\n[283] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. Re,\n\u201cFlashattention: Fast and memory-efficient exact at-\ntention with IO-awareness,\u201d in NeurIPS , 2022.\n[284] T. Dao, \u201cFlashattention-2: Faster attention with bet-\nter parallelism and work partitioning,\u201d arXiv preprint\narXiv:2307.08691 , 2023.\n[285] \u201cvllm: Easy, fast, and cheap llm serving with\npagedattention.\u201d [Online]. Available: https://vllm.ai/\n[286] A. Yuan, A. Coenen, E. Reif, and D. Ippolito, \u201cWord-\ncraft: story writing with large language models,\u201d in\n27th International Conference on Intelligent User Inter-\nfaces , 2022, pp. 841\u2013852.\n[287] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P . Das,\nand S. Reddy, \u201cThe impact of positional encoding\non length generalization in transformers,\u201d CoRR , vol.\nabs/2305.19466, 2023.\n[288] W. Xiong, J. Liu, I. Molybog, H. Zhang, P . Bhargava,\nR. Hou, L. Martin, R. Rungta, K. A. Sankararaman,\nB. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang,\nK. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis,\nS. Wang, and H. Ma, \u201cEffective long-context scaling of\nfoundation models,\u201d CoRR , vol. abs/2309.16039, 2023.\n[289] kaiokendev, \u201cThings I\u2019m learning while training su-\nperhot.\u201d 2023.\n[290] Z. Dong, T. Tang, J. Li, W. X. Zhao, and J. Wen,\n\u201cBAMBOO: A comprehensive benchmark for evalu-\nating long text modeling capacities of large language\nmodels,\u201d CoRR , vol. abs/2309.13345, 2023.[291] J. Su. (2023) Transformer upgrade path: 12, infinite\nextrapolation of rerope?\n[292] X. Liu, H. Yan, S. Zhang, C. An, X. Qiu, and D. Lin,\n\u201cScaling laws of rope-based extrapolation,\u201d CoRR , vol.\nabs/2310.05209, 2023.\n[293] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sun-\ndararajan, and S. Naidu, \u201cGiraffe: Adventures in\nexpanding context lengths in llms,\u201d CoRR , vol.\nabs/2308.10882, 2023.\n[294] G. Izacard and E. Grave, \u201cLeveraging passage re-\ntrieval with generative models for open domain ques-\ntion answering,\u201d in Proceedings of the 16th Conference of\nthe European Chapter of the Association for Computational\nLinguistics: Main Volume, EACL 2021, Online, April 19 -\n23, 2021 . Association for Computational Linguistics,\n2021, pp. 874\u2013880.\n[295] N. Ratner, Y. Levine, Y. Belinkov, O. Ram, I. Magar,\nO. Abend, E. Karpas, A. Shashua, K. Leyton-Brown,\nand Y. Shoham, \u201cParallel context windows for large\nlanguage models,\u201d in Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2023, Toronto, Canada, July\n9-14, 2023 . Association for Computational Linguis-\ntics, 2023, pp. 6383\u20136402.\n[296] Y. Hao, Y. Sun, L. Dong, Z. Han, Y. Gu, and F. Wei,\n\u201cStructured prompting: Scaling in-context learning to\n1, 000 examples,\u201d CoRR , 2022.\n[297] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLong-\nformer: The long-document transformer,\u201d CoRR , vol.\nabs/2004.05150, 2020.\n[298] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis,\n\u201cEfficient streaming language models with attention\nsinks,\u201d CoRR , vol. abs/2309.17453, 2023.\n[299] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilac-\nqua, F. Petroni, and P . Liang, \u201cLost in the middle:\nHow language models use long contexts,\u201d CoRR , vol.\nabs/2307.03172, 2023.\n[300] C. Han, Q. Wang, W. Xiong, Y. Chen, H. Ji, and\nS. Wang, \u201cLm-infinite: Simple on-the-fly length gen-\neralization for large language models,\u201d CoRR , vol.\nabs/2308.16137, 2023.\n[301] A. Bertsch, U. Alon, G. Neubig, and M. R. Gormley,\n\u201cUnlimiformer: Long-range transformers with unlim-\nited length input,\u201d CoRR , vol. abs/2305.01625, 2023.\n[302] Y. Wu, M. N. Rabe, D. Hutchins, and C. Szegedy,\n\u201cMemorizing transformers,\u201d in The Tenth International\nConference on Learning Representations, ICLR 2022, Vir-\ntual Event, April 25-29, 2022 . OpenReview.net, 2022.\n[303] H. Chen, R. Pasunuru, J. Weston, and A. Celiky-\nilmaz, \u201cWalking down the memory maze: Beyond\ncontext limit through interactive reading,\u201d CoRR , vol.\nabs/2310.05029, 2023.\n[304] W. Zhou, Y. E. Jiang, P . Cui, T. Wang, Z. Xiao, Y. Hou,\nR. Cotterell, and M. Sachan, \u201cRecurrentgpt: Interac-\ntive generation of (arbitrarily) long text,\u201d CoRR , vol.\nabs/2305.13304, 2023.\n[305] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and\nJ. E. Gonzalez, \u201cMemgpt: Towards llms as operating\nsystems,\u201d CoRR , vol. abs/2310.08560, 2023.\n[306] P . Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu,\nS. Subramanian, E. Bakhturina, M. Shoeybi, and99\nB. Catanzaro, \u201cRetrieval meets long context large lan-\nguage models,\u201d CoRR , vol. abs/2310.03025, 2023.\n[307] K. Murray and D. Chiang, \u201cCorrecting length bias in\nneural machine translation,\u201d in WMT . Association\nfor Computational Linguistics, 2018, pp. 212\u2013223.\n[308] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi,\n\u201cThe curious case of neural text degeneration,\u201d in\nICLR , 2020.\n[309] C.-M. U. P . P . D. O. C. SCIENCE, Speech Understanding\nSystems. Summary of Results of the Five-Year Research\nEffort at Carnegie-Mellon University , 1977.\n[310] P . Koehn and R. Knowles, \u201cSix challenges for neural\nmachine translation,\u201d in NMT@ACL . Association for\nComputational Linguistics, 2017, pp. 28\u201339.\n[311] Y. Wu, M. Schuster, Z. Chen, Q. V . Le, M. Norouzi,\nW. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,\nJ. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser,\nS. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens,\nG. Kurian, N. Patil, W. Wang, C. Young, J. Smith,\nJ. Riesa, A. Rudnick, O. Vinyals, G. Corrado,\nM. Hughes, and J. Dean, \u201cGoogle\u2019s neural machine\ntranslation system: Bridging the gap between human\nand machine translation,\u201d CoRR , vol. abs/1609.08144,\n2016.\n[312] R. Paulus, C. Xiong, and R. Socher, \u201cA deep rein-\nforced model for abstractive summarization,\u201d in ICLR\n(Poster) . OpenReview.net, 2018.\n[313] A. K. Vijayakumar, M. Cogswell, R. R. Selvaraju,\nQ. Sun, S. Lee, D. J. Crandall, and D. Batra, \u201cDiverse\nbeam search: Decoding diverse solutions from neural\nsequence models,\u201d CoRR , vol. abs/1610.02424, 2016.\n[314] A. Fan, M. Lewis, and Y. N. Dauphin, \u201cHierarchical\nneural story generation,\u201d in ACL (1) . Association for\nComputational Linguistics, 2018, pp. 889\u2013898.\n[315] J. Hewitt, C. D. Manning, and P . Liang, \u201cTrunca-\ntion sampling as language model desmoothing,\u201d in\nEMNLP (Findings) . Association for Computational\nLinguistics, 2022, pp. 3414\u20133427.\n[316] Y. Su, T. Lan, Y. Wang, D. Yogatama, L. Kong, and\nN. Collier, \u201cA contrastive framework for neural text\ngeneration,\u201d in NeurIPS , 2022.\n[317] C. Meister, T. Pimentel, G. Wiher, and R. Cotterell,\n\u201cLocally typical sampling,\u201d Trans. Assoc. Comput. Lin-\nguistics , 2023.\n[318] X. L. Li, A. Holtzman, D. Fried, P . Liang, J. Eisner,\nT. Hashimoto, L. Zettlemoyer, and M. Lewis, \u201cCon-\ntrastive decoding: Open-ended text generation as op-\ntimization,\u201d in ACL (1) . Association for Computa-\ntional Linguistics, 2023, pp. 12 286\u201312 312.\n[319] Y. Chuang, Y. Xie, H. Luo, Y. Kim, J. R. Glass, and\nP . He, \u201cDola: Decoding by contrasting layers im-\nproves factuality in large language models,\u201d CoRR ,\nvol. abs/2309.03883, 2023.\n[320] L. Chen, \u201cDissecting batching effects in gpt inference,\u201d\n2023. [Online]. Available: https://le.qun.ch/en/blog/\n2023/05/13/transformer-batching/\n[321] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin,\nB. Chen, P . Liang, C. R \u00b4e, I. Stoica, and C. Zhang,\n\u201cFlexgen: High-throughput generative inference of\nlarge language models with a single GPU,\u201d in ICML ,\nser. Proceedings of Machine Learning Research, vol.202. PMLR, 2023, pp. 31 094\u201331 116.\n[322] T. Dao, D. Haziza, F. Massa, and G. Sizov, \u201cFlash-\ndecoding for long-context inference,\u201d https://crfm.\nstanford.edu/2023/10/12/flashdecoding.html, 2023.\n[323] Y. Leviathan, M. Kalman, and Y. Matias, \u201cFast infer-\nence from transformers via speculative decoding,\u201d in\nInternational Conference on Machine Learning , 2023.\n[324] C. Chen, S. Borgeaud, G. Irving, J. Lespiau, L. Sifre,\nand J. Jumper, \u201cAccelerating large language model\ndecoding with speculative sampling,\u201d CoRR , vol.\nabs/2302.01318, 2023.\n[325] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang,\nR. Y. Y. Wong, Z. Chen, D. Arfeen, R. Abhyankar,\nand Z. Jia, \u201cSpecinfer: Accelerating generative LLM\nserving with speculative inference and token tree ver-\nification,\u201d CoRR , vol. abs/2305.09781, 2023.\n[326] B. Spector and C. R \u00b4e, \u201cAccelerating LLM infer-\nence with staged speculative decoding,\u201d CoRR , vol.\nabs/2308.04623, 2023.\n[327] L. D. Corro, A. D. Giorno, S. Agarwal, B. Yu, A. H.\nAwadallah, and S. Mukherjee, \u201cSkipdecode: Autore-\ngressive skip decoding with batching and caching for\nefficient LLM inference,\u201d CoRR , vol. abs/2307.02628,\n2023.\n[328] D. P . Kingma and J. Ba, \u201cAdam: A method for\nstochastic optimization,\u201d in 3rd International Confer-\nence on Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Conference Track Proceedings ,\nY. Bengio and Y. LeCun, Eds., 2015.\n[329] I. Loshchilov and F. Hutter, \u201cFixing weight decay\nregularization in adam,\u201d CoRR , vol. abs/1711.05101,\n2017.\n[330] N. Shazeer and M. Stern, \u201cAdafactor: Adaptive learn-\ning rates with sublinear memory cost,\u201d in Proceedings\nof the 35th International Conference on Machine Learning,\nICML 2018, Stockholmsm\u00a8 assan, Stockholm, Sweden, July\n10-15, 2018 , ser. Proceedings of Machine Learning\nResearch, J. G. Dy and A. Krause, Eds., vol. 80. PMLR,\n2018, pp. 4603\u20134611.\n[331] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen,\nM. X. Chen, H. Lee, J. Ngiam, Q. V . Le, Y. Wu, and\nZ. Chen, \u201cGpipe: Efficient training of giant neural\nnetworks using pipeline parallelism,\u201d in Advances in\nNeural Information Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada , H. M. Wallach, H. Larochelle, A. Beygelzimer,\nF. d\u2019Alch \u00b4e-Buc, E. B. Fox, and R. Garnett, Eds., 2019,\npp. 103\u2013112.\n[332] A. Harlap, D. Narayanan, A. Phanishayee, V . Seshadri,\nN. R. Devanur, G. R. Ganger, and P . B. Gibbons,\n\u201cPipedream: Fast and efficient pipeline parallel DNN\ntraining,\u201d CoRR , vol. abs/1806.03377, 2018.\n[333] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He,\n\u201cZero: memory optimizations toward training trillion\nparameter models,\u201d in Proceedings of the International\nConference for High Performance Computing, Networking,\nStorage and Analysis, SC 2020, Virtual Event / Atlanta,\nGeorgia, USA, November 9-19, 2020 , C. Cuicchi, I. Qual-\nters, and W. T. Kramer, Eds. IEEE/ACM, 2020, p. 20.\n[334] P . Micikevicius, S. Narang, J. Alben, G. F. Di-100\namos, E. Elsen, D. Garc \u00b4\u0131a, B. Ginsburg, M. Houston,\nO. Kuchaiev, G. Venkatesh, and H. Wu, \u201cMixed preci-\nsion training,\u201d CoRR , vol. abs/1710.03740, 2017.\n[335] Q. Xu, S. Li, C. Gong, and Y. You, \u201cAn efficient 2d\nmethod for training super-large deep learning mod-\nels,\u201d CoRR , vol. abs/2104.05343, 2021.\n[336] B. Wang, Q. Xu, Z. Bian, and Y. You, \u201cTesseract:\nParallelize the tensor parallelism efficiently,\u201d in Pro-\nceedings of the 51st International Conference on Parallel\nProcessing, ICPP 2022, Bordeaux, France, 29 August 2022\n- 1 September 2022 . ACM, 2022.\n[337] Z. Bian, Q. Xu, B. Wang, and Y. You, \u201cMaximizing\nparallelism in distributed training for huge neural\nnetworks,\u201d CoRR , vol. abs/2105.14450, 2021.\n[338] S. Li, F. Xue, C. Baranwal, Y. Li, and Y. You, \u201cSequence\nparallelism: Long sequence training from system per-\nspective,\u201d arXiv e-prints , pp. arXiv\u20132105, 2021.\n[339] FairScale authors, \u201cFairscale: A general purpose\nmodular pytorch library for high performance\nand large scale training,\u201d https://github.com/\nfacebookresearch/fairscale, 2021.\n[340] L. Zheng, Z. Li, H. Zhang, Y. Zhuang, Z. Chen,\nY. Huang, Y. Wang, Y. Xu, D. Zhuo, E. P . Xing et al. ,\n\u201cAlpa: Automating inter-and {Intra-Operator }paral-\nlelism for distributed deep learning,\u201d in OSDI , 2022,\npp. 559\u2013578.\n[341] T. Chen, B. Xu, C. Zhang, and C. Guestrin, \u201cTraining\ndeep nets with sublinear memory cost,\u201d CoRR , vol.\nabs/1604.06174, 2016.\n[342] R. Lou, K. Zhang, and W. Yin, \u201cIs prompt all you\nneed? no. A comprehensive and broader view of in-\nstruction learning,\u201d CoRR , vol. abs/2303.10475, 2023.\n[343] X. Liu, P . He, W. Chen, and J. Gao, \u201cMulti-task deep\nneural networks for natural language understand-\ning,\u201d in ACL (1) . Association for Computational\nLinguistics, 2019, pp. 4487\u20134496.\n[344] A. Aghajanyan, A. Gupta, A. Shrivastava, X. Chen,\nL. Zettlemoyer, and S. Gupta, \u201cMuppet: Massive\nmulti-task representations with pre-finetuning,\u201d in\nEMNLP (1) . Association for Computational Linguis-\ntics, 2021, pp. 5799\u20135811.\n[345] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung,\nY. Tay, D. Zhou, Q. V . Le, B. Zoph, J. Wei, and\nA. Roberts, \u201cThe flan collection: Designing data and\nmethods for effective instruction tuning,\u201d CoRR , vol.\nabs/2301.13688, 2023.\n[346] C. Xu, Q. Sun, K. Zheng, X. Geng, P . Zhao, J. Feng,\nC. Tao, and D. Jiang, \u201cWizardlm: Empowering large\nlanguage models to follow complex instructions,\u201d\nCoRR , vol. abs/2304.12244, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2304.12244\n[347] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox,\nY. Yang, and C. Gan, \u201cPrinciple-driven self-alignment\nof language models from scratch with minimal human\nsupervision,\u201d arXiv preprint arXiv:2305.03047 , 2023.\n[348] X. Li, P . Yu, C. Zhou, T. Schick, L. Zettle-\nmoyer, O. Levy, J. Weston, and M. Lewis, \u201cSelf-\nalignment with instruction backtranslation,\u201d CoRR ,\nvol. abs/2308.06259, 2023.\n[349] C. Zhou, P . Liu, P . Xu, S. Iyer, J. Sun, Y. Mao, X. Ma,\nA. Efrat, P . Yu, L. Yu et al. , \u201cLima: Less is more foralignment,\u201d arXiv preprint arXiv:2305.11206 , 2023.\n[350] L. Chen, S. Li, J. Yan, H. Wang, K. Gunaratna, V . Ya-\ndav, Z. Tang, V . Srinivasan, T. Zhou, H. Huang, and\nH. Jin, \u201cAlpagasus: Training A better alpaca with\nfewer data,\u201d CoRR , vol. abs/2307.08701, 2023.\n[351] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal,\nH. Palangi, and A. H. Awadallah, \u201cOrca: Progressive\nlearning from complex explanation traces of GPT-4,\u201d\nCoRR , vol. abs/2306.02707, 2023.\n[352] YuLan-Chat-Team, \u201cYulan-chat: An open-source\nbilingual chatbot,\u201d https://github.com/RUC-GSAI/\nYuLan-Chat, 2023.\n[353] Y. Wang, H. Ivison, P . Dasigi, J. Hessel, T. Khot, K. R.\nChandu, D. Wadden, K. MacMillan, N. A. Smith,\nI. Beltagy, and H. Hajishirzi, \u201cHow far can camels\ngo? exploring the state of instruction tuning on open\nresources,\u201d CoRR , vol. abs/2306.04751, 2023.\n[354] B. Peng, C. Li, P . He, M. Galley, and J. Gao, \u201cInstruc-\ntion tuning with GPT-4,\u201d CoRR , vol. abs/2304.03277,\n2023.\n[355] M. M. Krell, M. Kosec, S. P . Perez, and A. Fitzgib-\nbon, \u201cEfficient sequence packing without cross-\ncontamination: Accelerating large language mod-\nels without impacting performance,\u201d arXiv preprint\narXiv:2107.02027 , 2021.\n[356] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei,\nH. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis,\nS. Pfohl et al. , \u201cLarge language models encode clinical\nknowledge,\u201d arXiv preprint arXiv:2212.13138 , 2022.\n[357] J. Zhang, R. Xie, Y. Hou, W. X. Zhao, L. Lin, and\nJ. Wen, \u201cRecommendation as instruction following:\nA large language model empowered recommendation\napproach,\u201d CoRR , vol. abs/2305.07001, 2023.\n[358] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin,\nand T. Liu, \u201cHuatuo: Tuning llama model with chinese\nmedical knowledge,\u201d arXiv preprint arXiv:2304.06975 ,\n2023.\n[359] Q. Huang, M. Tao, Z. An, C. Zhang, C. Jiang, Z. Chen,\nZ. Wu, and Y. Feng, \u201cLawyer llama technical report,\u201d\narXiv preprint arXiv:2305.15062 , 2023.\n[360] S. Wu, O. Irsoy, S. Lu, V . Dabravolski, M. Dredze,\nS. Gehrmann, P . Kambadur, D. Rosenberg, and\nG. Mann, \u201cBloomberggpt: A large language model for\nfinance,\u201d arXiv preprint arXiv:2303.17564 , 2023.\n[361] T. Liu and B. K. H. Low, \u201cGoat: Fine-tuned llama\noutperforms gpt-4 on arithmetic tasks,\u201d arXiv preprint\narXiv:2305.14201 , 2023.\n[362] T. Sun, X. Zhang, Z. He, P . Li, Q. Cheng, H. Yan, X. Liu,\nY. Shao, Q. Tang, X. Zhao, K. Chen, Y. Zheng, Z. Zhou,\nR. Li, J. Zhan, Y. Zhou, L. Li, X. Yang, L. Wu, Z. Yin,\nX. Huang, and X. Qiu, \u201cMoss: Training conversational\nlanguage models from synthetic data,\u201d 2023.\n[363] Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani,\nJ. Ba, C. Guestrin, P . Liang, and T. B. Hashimoto,\n\u201cAlpacafarm: A simulation framework for methods\nthat learn from human feedback,\u201d CoRR , vol.\nabs/2305.14387, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2305.14387\n[364] D. Hendrycks, C. Burns, S. Basart, A. Zou,\nM. Mazeika, D. Song, and J. Steinhardt, \u201cMeasuring\nmassive multitask language understanding,\u201d in ICLR .101\nOpenReview.net, 2021.\n[365] M. Suzgun, N. Scales, N. Sch \u00a8arli, S. Gehrmann, Y. Tay,\nH. W. Chung, A. Chowdhery, Q. V . Le, E. H. Chi,\nD. Zhou, and J. Wei, \u201cChallenging big-bench tasks and\nwhether chain-of-thought can solve them,\u201d CoRR , vol.\nabs/2210.09261, 2022.\n[366] Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V . Miku-\nlik, and G. Irving, \u201cAlignment of language agents,\u201d\nCoRR , vol. abs/2103.14659, 2021.\n[367] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Rad-\nford, D. Amodei, P . F. Christiano, and G. Irving, \u201cFine-\ntuning language models from human preferences,\u201d\nCoRR , vol. abs/1909.08593, 2019.\n[368] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli,\nT. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-\nSarma, N. Elhage, Z. Hatfield-Dodds, D. Hernandez,\nJ. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. B.\nBrown, J. Clark, S. McCandlish, C. Olah, and J. Ka-\nplan, \u201cA general language assistant as a laboratory\nfor alignment,\u201d CoRR , vol. abs/2112.00861, 2021.\n[369] E. Perez, S. Huang, H. F. Song, T. Cai, R. Ring,\nJ. Aslanides, A. Glaese, N. McAleese, and G. Irving,\n\u201cRed teaming language models with language mod-\nels,\u201d in Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2022,\nAbu Dhabi, United Arab Emirates, December 7-11, 2022 ,\nY. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Asso-\nciation for Computational Linguistics, 2022, pp. 3419\u2013\n3448.\n[370] J. Menick, M. Trebacz, V . Mikulik, J. Aslanides,\nH. F. Song, M. Chadwick, M. Glaese, S. Young,\nL. Campbell-Gillingham, G. Irving, and N. McAleese,\n\u201cTeaching language models to support answers with\nverified quotes,\u201d CoRR , vol. abs/2203.11147, 2022.\n[371] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion,\nA. Jones, A. Chen, A. Goldie, A. Mirhoseini,\nC. McKinnon, C. Chen, C. Olsson, C. Olah,\nD. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-\nJohnson, E. Perez, J. Kerr, J. Mueller, J. Ladish,\nJ. Landau, K. Ndousse, K. Lukosiute, L. Lovitt,\nM. Sellitto, N. Elhage, N. Schiefer, N. Mercado,\nN. DasSarma, R. Lasenby, R. Larson, S. Ringer,\nS. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham,\nT. Telleen-Lawton, T. Conerly, T. Henighan, T. Hume,\nS. R. Bowman, Z. Hatfield-Dodds, B. Mann,\nD. Amodei, N. Joseph, S. McCandlish, T. Brown, and\nJ. Kaplan, \u201cConstitutional AI: harmlessness from AI\nfeedback,\u201d CoRR , vol. abs/2212.08073, 2022. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2212.08073\n[372] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard,\nC. Bishop, V . Carbune, and A. Rastogi, \u201cRLAIF: scal-\ning reinforcement learning from human feedback with\nAI feedback,\u201d CoRR , vol. abs/2309.00267, 2023.\n[373] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang,\nK. Shum, and T. Zhang, \u201cRAFT: reward ranked fine-\ntuning for generative foundation model alignment,\u201d\nCoRR , vol. abs/2304.06767, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2304.06767\n[374] A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli,\nT. Henighan, A. Jones, N. Joseph, B. Mann, N. Das-\nSarma et al. , \u201cA general language assistant as a labo-ratory for alignment,\u201d arXiv preprint arXiv:2112.00861 ,\n2021.\n[375] R. Zheng, S. Dou, S. Gao, W. Shen, B. Wang, Y. Liu,\nS. Jin, Q. Liu, L. Xiong, L. Chen et al. , \u201cSecrets of rlhf\nin large language models part i: Ppo,\u201d arXiv preprint\narXiv:2307.04964 , 2023.\n[376] J. Uesato, N. Kushman, R. Kumar, H. F. Song, N. Y.\nSiegel, L. Wang, A. Creswell, G. Irving, and I. Hig-\ngins, \u201cSolving math word problems with process- and\noutcome-based feedback,\u201d CoRR , vol. abs/2211.14275,\n2022.\n[377] H. Lightman, V . Kosaraju, Y. Burda, H. Edwards,\nB. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever,\nand K. Cobbe, \u201cLet\u2019s verify step by step,\u201d CoRR , vol.\nabs/2305.20050, 2023.\n[378] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika,\nA. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song,\nand J. Steinhardt, \u201cMeasuring coding challenge com-\npetence with APPS,\u201d in NeurIPS Datasets and Bench-\nmarks , 2021.\n[379] Q. Ma, H. Zhou, T. Liu, J. Yuan, P . Liu, Y. You, and\nH. Yang, \u201cLet\u2019s reward step by step: Step-level reward\nmodel as the navigators for reasoning,\u201d CoRR , vol.\nabs/2310.10080, 2023.\n[380] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,\nA. Huang, A. Guez, T. Hubert, L. Baker, M. Lai,\nA. Bolton, Y. Chen, T. P . Lillicrap, F. Hui, L. Sifre,\nG. van den Driessche, T. Graepel, and D. Hassabis,\n\u201cMastering the game of go without human knowl-\nedge,\u201d Nat., pp. 354\u2013359, 2017.\n[381] T. Anthony, Z. Tian, and D. Barber, \u201cThinking fast\nand slow with deep learning and tree search,\u201d in\nAdvances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Processing Sys-\ntems 2017, December 4-9, 2017, Long Beach, CA, USA ,\n2017, pp. 5360\u20135370.\n[382] H. Luo, Q. Sun, C. Xu, P . Zhao, J. Lou, C. Tao,\nX. Geng, Q. Lin, S. Chen, and D. Zhang, \u201cWizard-\nmath: Empowering mathematical reasoning for large\nlanguage models via reinforced evol-instruct,\u201d CoRR ,\nvol. abs/2308.09583, 2023.\n[383] R. Liu, C. Jia, G. Zhang, Z. Zhuang, T. X. Liu, and\nS. Vosoughi, \u201cSecond thoughts are best: Learning\nto re-align with human values from text edits,\u201d in\nNeurIPS , 2022.\n[384] X. Lu, S. Welleck, J. Hessel, L. Jiang, L. Qin, P . West,\nP . Ammanabrolu, and Y. Choi, \u201cQUARK: control-\nlable text generation with reinforced unlearning,\u201d in\nNeurIPS , 2022.\n[385] J. Scheurer, J. A. Campos, T. Korbak, J. S. Chan,\nA. Chen, K. Cho, and E. Perez, \u201cTraining language\nmodels with language feedback at scale,\u201d CoRR , vol.\nabs/2303.16755, 2023.\n[386] G. Guo, R. Zhao, T. Tang, W. X. Zhao, and\nJ.-R. Wen, \u201cBeyond imitation: Leveraging fine-\ngrained quality signals for alignment,\u201d arXiv preprint\narXiv:2311.04072 , 2023.\n[387] R. Krishna, D. Lee, L. Fei-Fei, and M. S. Bernstein,\n\u201cSocially situated artificial intelligence enables\nlearning from human interaction,\u201d Proceedings of\nthe National Academy of Sciences of the United States102\nof America , vol. 119, 2022. [Online]. Available: https:\n//api.semanticscholar.org/CorpusID:252381954\n[388] H. Liu, C. Sferrazza, and P . Abbeel, \u201cChain of hind-\nsight aligns language models with feedback,\u201d CoRR ,\nvol. abs/2302.02676, 2023.\n[389] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon,\nC. D. Manning, and C. Finn, \u201cDirect preference\noptimization: Your language model is secretly a\nreward model,\u201d CoRR , vol. abs/2305.18290, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.\n2305.18290\n[390] Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang,\nand F. Huang, \u201cRRHF: rank responses to align\nlanguage models with human feedback without\ntears,\u201d CoRR , vol. abs/2304.05302, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2304.05302\n[391] Y. Zhao, R. Joshi, T. Liu, M. Khalman, M. Saleh, and\nP . J. Liu, \u201cSlic-hf: Sequence likelihood calibration with\nhuman feedback,\u201d CoRR , vol. abs/2305.10425, 2023.\n[392] T. Zhang, F. Liu, J. Wong, P . Abbeel, and J. E.\nGonzalez, \u201cThe wisdom of hindsight makes language\nmodels better instruction followers,\u201d CoRR , vol.\nabs/2302.05206, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2302.05206\n[393] A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne,\n\u201cImitation learning: A survey of learning methods,\u201d\nACM Comput. Surv. , vol. 50, no. 2, apr 2017. [Online].\nAvailable: https://doi.org/10.1145/3054912\n[394] S. Levine, \u201cShould i imitate or reinforce,\u201d\n2022. [Online]. Available: https://www.youtube.\ncom/watch?v=sVPm7zOrBxM\n[395] J. Schulman, \u201cReinforcement learning from human\nfeedback: Progress and challenges,\u201d 2023. [On-\nline]. Available: https://www.youtube.com/watch?\nv=hhiLw5Q UFg\n[396] X. L. Li and P . Liang, \u201cPrefix-tuning: Optimizing\ncontinuous prompts for generation,\u201d in Proceedings\nof the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint\nConference on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, August 1-\n6, 2021 , C. Zong, F. Xia, W. Li, and R. Navigli, Eds.\nAssociation for Computational Linguistics, 2021, pp.\n4582\u20134597.\n[397] B. Lester, R. Al-Rfou, and N. Constant, \u201cThe power\nof scale for parameter-efficient prompt tuning,\u201d in\nProceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2021, Virtual\nEvent / Punta Cana, Dominican Republic, 7-11 November,\n2021 , M. Moens, X. Huang, L. Specia, and S. W. Yih,\nEds. Association for Computational Linguistics, 2021,\npp. 3045\u20133059.\n[398] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone,\nQ. de Laroussilhe, A. Gesmundo, M. Attariyan, and\nS. Gelly, \u201cParameter-efficient transfer learning for\nNLP,\u201d in Proceedings of the 36th International Conference\non Machine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA , 2019, pp. 2790\u20132799.\n[399] Z. Hu, Y. Lan, L. Wang, W. Xu, E. Lim, R. K. Lee,\nL. Bing, and S. Poria, \u201cLlm-adapters: An adapter\nfamily for parameter-efficient fine-tuning of large lan-guage models,\u201d CoRR , vol. abs/2304.01933, 2023.\n[400] J. He, C. Zhou, X. Ma, T. Berg-Kirkpatrick, and\nG. Neubig, \u201cTowards a unified view of parameter-\nefficient transfer learning,\u201d in The Tenth International\nConference on Learning Representations, ICLR 2022, Vir-\ntual Event, April 25-29, 2022 . OpenReview.net, 2022.\n[401] X. Liu, K. Ji, Y. Fu, Z. Du, Z. Yang, and J. Tang, \u201cP-\ntuning v2: Prompt tuning can be comparable to fine-\ntuning universally across scales and tasks,\u201d CoRR , vol.\nabs/2110.07602, 2021.\n[402] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang,\nand J. Tang, \u201cGPT understands, too,\u201d CoRR , vol.\nabs/2103.10385, 2021.\n[403] Y. Gu, X. Han, Z. Liu, and M. Huang, \u201cPpt: Pre-trained\nprompt tuning for few-shot learning,\u201d in Proceedings\nof the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) , 2022, pp.\n8410\u20138423.\n[404] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, \u201cHow can\nwe know what language models know?\u201d Transactions\nof the Association for Computational Linguistics , vol. 8,\npp. 423\u2013438, 2020.\n[405] T. Shin, Y. Razeghi, R. L. Logan IV , E. Wallace,\nand S. Singh, \u201cAutoprompt: Eliciting knowledge\nfrom language models with automatically gener-\nated prompts,\u201d in Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing\n(EMNLP) , 2020, pp. 4222\u20134235.\n[406] Q. Zhang, M. Chen, A. Bukharin, P . He, Y. Cheng,\nW. Chen, and T. Zhao, \u201cAdaptive budget allocation\nfor parameter-efficient fine-tuning,\u201d CoRR , vol.\nabs/2303.10512, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2303.10512\n[407] M. Valipour, M. Rezagholizadeh, I. Kobyzev, and\nA. Ghodsi, \u201cDylora: Parameter efficient tuning of\npre-trained models using dynamic search-free low-\nrank adaptation,\u201d CoRR , vol. abs/2210.07558, 2022.\n[Online]. Available: https://doi.org/10.48550/arXiv.\n2210.07558\n[408] N. Ding, Y. Qin, G. Yang, F. Wei, Y. Zonghan, Y. Su,\nS. Hu, Y. Chen, C.-M. Chan, W. Chen, J. Yi, W. Zhao,\nX. Wang, Z. Liu, H.-T. Zheng, J. Chen, Y. Liu, J. Tang,\nJ. Li, and M. Sun, \u201cParameter-efficient fine-tuning\nof large-scale pre-trained language models,\u201d Nature\nMachine Intelligence , vol. 5, pp. 1\u201316, 03 2023.\n[409] R. Zhang, J. Han, A. Zhou, X. Hu, S. Yan, P . Lu, H. Li,\nP . Gao, and Y. Qiao, \u201cLlama-adapter: Efficient fine-\ntuning of language models with zero-init attention,\u201d\nCoRR , vol. abs/2303.16199, 2023.\n[410] J. Pfeiffer, I. Vulic, I. Gurevych, and S. Ruder, \u201cMAD-\nX: an adapter-based framework for multi-task cross-\nlingual transfer,\u201d in Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing,\nEMNLP 2020, Online, November 16-20, 2020 , B. Webber,\nT. Cohn, Y. He, and Y. Liu, Eds. Association for\nComputational Linguistics, 2020, pp. 7654\u20137673.\n[411] S. Mangrulkar, S. Gugger, L. Debut, Y. Belkada, and\nS. Paul, \u201cPeft: State-of-the-art parameter-efficient fine-\ntuning methods,\u201d https://github.com/huggingface/\npeft, 2022.\n[412] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W.103\nMahoney, and K. Keutzer, \u201cA survey of quantization\nmethods for efficient neural network inference,\u201d\nCoRR , vol. abs/2103.13630, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2103.13630\n[413] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer,\n\u201cLlm.int8(): 8-bit matrix multiplication for transform-\ners at scale,\u201d CoRR , vol. abs/2208.07339, 2022.\n[414] G. Xiao, J. Lin, M. Seznec, J. Demouth, and\nS. Han, \u201cSmoothquant: Accurate and efficient post-\ntraining quantization for large language models,\u201d\nCoRR , vol. abs/2211.10438, 2022. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2211.10438\n[415] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li,\nand Y. He, \u201cZeroquant: Efficient and affordable post-\ntraining quantization for large-scale transformers,\u201d in\nNeurIPS , 2022.\n[416] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han,\n\u201cAwq: Activation-aware weight quantization for llm\ncompression and acceleration,\u201d 2023.\n[417] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alis-\ntarh, \u201cGptq: Accurate post-training quantization for\ngenerative pre-trained transformers,\u201d arXiv preprint\narXiv:2210.17323 , 2022.\n[418] E. Frantar and D. Alistarh, \u201cOptimal brain compres-\nsion: A framework for accurate post-training quanti-\nzation and pruning,\u201d in NeurIPS , 2022.\n[419] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettle-\nmoyer, \u201cQlora: Efficient finetuning of quantized llms,\u201d\narXiv preprint arXiv:2305.14314 , 2023.\n[420] Z. Liu, B. Oguz, C. Zhao, E. Chang, P . Stock,\nY. Mehdad, Y. Shi, R. Krishnamoorthi, and V . Chandra,\n\u201cLlm-qat: Data-free quantization aware training for\nlarge language models,\u201d 2023.\n[421] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He, \u201cZeroquant-\nv2: Exploring post-training quantization in llms from\ncomprehensive study to low rank compensation,\u201d\n2023.\n[422] T. Dettmers and L. Zettlemoyer, \u201cThe case for 4-bit\nprecision: k-bit inference scaling laws,\u201d CoRR , vol.\nabs/2212.09720, 2022.\n[423] L. Peiyu, L. Zikang, G. Ze-Feng, G. Dawei, Z. W. Xin,\nL. Yaliang, D. Bolin, and W. Ji-Rong, \u201cDo emergent\nabilities exist in quantized large language models:\nAn empirical study,\u201d arXiv preprint arXiv:2307.08072 ,\n2023.\n[424] T. Dettmers, M. Lewis, Y. Belkada, and\nL. Zettlemoyer, \u201cLlm.int8(): 8-bit matrix mul-\ntiplication for transformers at scale,\u201d CoRR ,\nvol. abs/2208.07339, 2022. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2208.07339\n[425] X. Wei, X. Cui, N. Cheng, X. Wang, X. Zhang,\nS. Huang, P . Xie, J. Xu, Y. Chen, M. Zhang et al. ,\n\u201cZero-shot information extraction via chatting with\nchatgpt,\u201d arXiv preprint arXiv:2302.10205 , 2023.\n[426] T. Dettmers, M. Lewis, S. Shleifer, and L. Zettlemoyer,\n\u201c8-bit optimizers via block-wise quantization,\u201d 9th In-\nternational Conference on Learning Representations, ICLR ,\n2022.\n[427] C. Tao, L. Hou, W. Zhang, L. Shang, X. Jiang, Q. Liu,\nP . Luo, and N. Wong, \u201cCompression of generative\npre-trained language models via quantization,\u201d inProceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\nACL 2022, Dublin, Ireland, May 22-27, 2022 , S. Muresan,\nP . Nakov, and A. Villavicencio, Eds. Association for\nComputational Linguistics, 2022, pp. 4821\u20134836.\n[428] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and\nW. Chen, \u201cWhat makes good in-context examples for\ngpt-3?\u201d in Proceedings of Deep Learning Inside Out: The\n3rd Workshop on Knowledge Extraction and Integration for\nDeep Learning Architectures, DeeLIO@ACL 2022, Dublin,\nIreland and Online, May 27, 2022 , 2022, pp. 100\u2013114.\n[429] O. Rubin, J. Herzig, and J. Berant, \u201cLearning to re-\ntrieve prompts for in-context learning,\u201d in Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL 2022, Seattle, WA, United\nStates, July 10-15, 2022 , 2022, pp. 2655\u20132671.\n[430] H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and\nS. Lee, \u201cSelf-generated in-context learning: Leverag-\ning auto-regressive language models as a demonstra-\ntion generator,\u201d CoRR , vol. abs/2206.08082, 2022.\n[431] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis,\nH. Chan, and J. Ba, \u201cLarge language models are\nhuman-level prompt engineers,\u201d in Proc. of ICLR , 2023.\n[432] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P . Stene-\ntorp, \u201cFantastically ordered prompts and where to\nfind them: Overcoming few-shot prompt order sen-\nsitivity,\u201d in Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), ACL 2022, Dublin, Ireland, May 22-27,\n2022 , S. Muresan, P . Nakov, and A. Villavicencio, Eds.,\n2022, pp. 8086\u20138098.\n[433] Y. Fu, H. Peng, A. Sabharwal, P . Clark, and T. Khot,\n\u201cComplexity-based prompting for multi-step reason-\ning,\u201d CoRR , vol. abs/2210.00720, 2022.\n[434] Z. Zhang, A. Zhang, M. Li, and A. Smola, \u201cAutomatic\nchain of thought prompting in large language mod-\nels,\u201d CoRR , vol. abs/2210.03493, 2022.\n[435] A. Creswell, M. Shanahan, and I. Higgins, \u201cSelection-\ninference: Exploiting large language models\nfor interpretable logical reasoning,\u201d CoRR , vol.\nabs/2205.09712, 2022.\n[436] X. Wang, J. Wei, D. Schuurmans, Q. V . Le, E. H.\nChi, and D. Zhou, \u201cSelf-consistency improves chain\nof thought reasoning in language models,\u201d CoRR , vol.\nabs/2203.11171, 2022.\n[437] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J. Lou, and\nW. Chen, \u201cOn the advance of making language mod-\nels better reasoners,\u201d CoRR , vol. abs/2206.02336, 2022.\n[438] X. Wang, J. Wei, D. Schuurmans, Q. V . Le, E. H.\nChi, and D. Zhou, \u201cRationale-augmented ensembles\nin language models,\u201d CoRR , 2022.\n[439] D. Zhou, N. Sch \u00a8arli, L. Hou, J. Wei, N. Scales, X. Wang,\nD. Schuurmans, O. Bousquet, Q. Le, and E. H. Chi,\n\u201cLeast-to-most prompting enables complex reasoning\nin large language models,\u201d CoRR , vol. abs/2205.10625,\n2022.\n[440] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson,\nP . Clark, and A. Sabharwal, \u201cDecomposed prompting:\nA modular approach for solving complex tasks,\u201d\nCoRR , vol. abs/2210.02406, 2022. [Online]. Available:104\nhttps://doi.org/10.48550/arXiv.2210.02406\n[441] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K. Lee, and\nE. Lim, \u201cPlan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language\nmodels,\u201d CoRR , vol. abs/2305.04091, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2305.04091\n[442] Q. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao,\nE. Wong, M. Apidianaki, and C. Callison-Burch,\n\u201cFaithful chain-of-thought reasoning,\u201d CoRR , vol.\nabs/2301.13379, 2023.\n[443] L. Gao, A. Madaan, S. Zhou, U. Alon, P . Liu, Y. Yang,\nJ. Callan, and G. Neubig, \u201cPAL: program-aided lan-\nguage models,\u201d CoRR , vol. abs/2211.10435, 2022.\n[444] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and\nY. Zhuang, \u201cHugginggpt: Solving ai tasks with chat-\ngpt and its friends in huggingface,\u201d arXiv preprint\narXiv:2303.17580 , 2023.\n[445] H. Sun, Y. Zhuang, L. Kong, B. Dai, and C. Zhang,\n\u201cAdaplanner: Adaptive planning from feedback with\nlanguage models,\u201d arXiv preprint arXiv:2305.16653 ,\n2023.\n[446] Y. Lu, P . Lu, Z. Chen, W. Zhu, X. E. Wang, and W. Y.\nWang, \u201cMultimodal procedural planning via dual\ntext-image prompting,\u201d CoRR , vol. abs/2305.01795,\n2023.\n[447] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang,\nand Z. Hu, \u201cReasoning with language model is plan-\nning with world model,\u201d CoRR , vol. abs/2305.14992,\n2023.\n[448] Z. Chen, K. Zhou, B. Zhang, Z. Gong, W. X. Zhao, and\nJ. Wen, \u201cChatcot: Tool-augmented chain-of-thought\nreasoning on chat-based large language models,\u201d\nCoRR , vol. abs/2305.14323, 2023.\n[449] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran,\nK. Narasimhan, and Y. Cao, \u201cReact: Synergizing rea-\nsoning and acting in language models,\u201d CoRR , vol.\nabs/2210.03629, 2022.\n[450] N. Shinn, F. Cassano, B. Labash, A. Gopinath,\nK. Narasimhan, and S. Yao, \u201cReflexion: Language\nagents with verbal reinforcement learning,\u201d 2023.\n[451] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao,\nand K. Narasimhan, \u201cTree of thoughts: Deliberate\nproblem solving with large language models,\u201d CoRR ,\nvol. abs/2305.10601, 2023.\n[452] V . Liu and L. B. Chilton, \u201cDesign guidelines for\nprompt engineering text-to-image generative mod-\nels,\u201d in Proceedings of the 2022 CHI Conference on Human\nFactors in Computing Systems , 2022, pp. 1\u201323.\n[453] J. White, Q. Fu, S. Hays, M. Sandborn, C. Olea,\nH. Gilbert, A. Elnashar, J. Spencer-Smith, and D. C.\nSchmidt, \u201cA prompt pattern catalog to enhance\nprompt engineering with chatgpt,\u201d arXiv preprint\narXiv:2302.11382 , 2023.\n[454] S. K. K. Santu and D. Feng, \u201cTeler: A general\ntaxonomy of LLM prompts for benchmarking\ncomplex tasks,\u201d CoRR , vol. abs/2305.11430, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.\n2305.11430\n[455] OpenAI, \u201cGpt best practices,\u201d OpenAI , 2023.\n[Online]. Available: https://platform.openai.com/\ndocs/guides/gpt-best-practices[456] Contributors, \u201cAi short,\u201d 2023. [Online]. Available:\nhttps://www.aishort.top/\n[457] \u2014\u2014, \u201cAwesome chatgpt prompts,\u201d Github , 2023.\n[Online]. Available: https://github.com/f/awesome-\nchatgpt-prompts/\n[458] J. Jiang, K. Zhou, Z. Dong, K. Ye, W. X. Zhao, and\nJ. Wen, \u201cStructgpt: A general framework for large lan-\nguage model to reason over structured data,\u201d CoRR ,\nvol. abs/2305.09645, 2023.\n[459] L. Beurer-Kellner, M. Fischer, and M. Vechev,\n\u201cPrompting is programming: A query language for\nlarge language models,\u201d Proceedings of the ACM on\nProgramming Languages , vol. 7, no. PLDI, pp. 1946\u2013\n1969, 2023.\n[460] P . Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N.\nWu, S.-C. Zhu, and J. Gao, \u201cChameleon: Plug-and-\nplay compositional reasoning with large language\nmodels,\u201d arXiv preprint arXiv:2304.09842 , 2023.\n[461] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian,\nH. Wu, J.-R. Wen, and H. Wang, \u201cInvestigating\nthe factual knowledge boundary of large language\nmodels with retrieval augmentation,\u201d arXiv preprint\narXiv:2307.11019 , 2023.\n[462] Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. J. McAuley,\nand W. X. Zhao, \u201cLarge language models are zero-\nshot rankers for recommender systems,\u201d CoRR , vol.\nabs/2305.08845, 2023.\n[463] S. Chang and E. Fosler-Lussier, \u201cHow to prompt\nllms for text-to-sql: A study in zero-shot, single-\ndomain, and cross-domain settings,\u201d CoRR , vol.\nabs/2305.11853, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2305.11853\n[464] Y. Wen, N. Jain, J. Kirchenbauer, M. Goldblum,\nJ. Geiping, and T. Goldstein, \u201cHard prompts\nmade easy: Gradient-based discrete optimization\nfor prompt tuning and discovery,\u201d CoRR , vol.\nabs/2302.03668, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2302.03668\n[465] T. Gao, A. Fisch, and D. Chen, \u201cMaking pre-trained\nlanguage models better few-shot learners,\u201d in Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint\nConference on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, August 1-\n6, 2021 , C. Zong, F. Xia, W. Li, and R. Navigli, Eds.\nAssociation for Computational Linguistics, 2021, pp.\n3816\u20133830.\n[466] L. Chen, J. Chen, T. Goldstein, H. Huang, and T. Zhou,\n\u201cInstructzero: Efficient instruction optimization for\nblack-box large language models,\u201d CoRR , vol.\nabs/2306.03082, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2306.03082\n[467] M. Deng, J. Wang, C. Hsieh, Y. Wang, H. Guo, T. Shu,\nM. Song, E. P . Xing, and Z. Hu, \u201cRlprompt: Optimiz-\ning discrete text prompts with reinforcement learn-\ning,\u201d in Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2022,\nAbu Dhabi, United Arab Emirates, December 7-11, 2022 ,\nY. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Asso-\nciation for Computational Linguistics, 2022, pp. 3369\u2013\n3391.105\n[468] T. Zhang, X. Wang, D. Zhou, D. Schuurmans, and J. E.\nGonzalez, \u201cTEMPERA: test-time prompt editing via\nreinforcement learning,\u201d in The Eleventh International\nConference on Learning Representations, ICLR 2023, Ki-\ngali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\n[469] H. Xu, Y. Chen, Y. Du, N. Shao, Y. Wang, H. Li, and\nZ. Yang, \u201cGPS: genetic prompt search for efficient few-\nshot learning,\u201d in Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates, Decem-\nber 7-11, 2022 , Y. Goldberg, Z. Kozareva, and Y. Zhang,\nEds. Association for Computational Linguistics, 2022,\npp. 8162\u20138171.\n[470] A. Prasad, P . Hase, X. Zhou, and M. Bansal,\n\u201cGrips: Gradient-free, edit-based instruction search\nfor prompting large language models,\u201d in Proceedings\nof the 17th Conference of the European Chapter of the\nAssociation for Computational Linguistics, EACL 2023,\nDubrovnik, Croatia, May 2-6, 2023 , A. Vlachos and\nI. Augenstein, Eds. Association for Computational\nLinguistics, 2023, pp. 3827\u20133846.\n[471] Y. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis,\nH. Chan, and J. Ba, \u201cLarge language models are\nhuman-level prompt engineers,\u201d in The Eleventh In-\nternational Conference on Learning Representations, ICLR\n2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net,\n2023.\n[472] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu,\nand M. Zeng, \u201cAutomatic prompt optimization\nwith \u201dgradient descent\u201d and beam search,\u201d CoRR ,\nvol. abs/2305.03495, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2305.03495\n[473] C. Yang, X. Wang, Y. Lu, H. Liu, Q. V . Le, D. Zhou,\nand X. Chen, \u201cLarge language models as optimizers,\u201d\nCoRR , vol. abs/2309.03409, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2309.03409\n[474] X. Wang, C. Li, Z. Wang, F. Bai, H. Luo,\nJ. Zhang, N. Jojic, E. P . Xing, and Z. Hu,\n\u201cPromptagent: Strategic planning with language\nmodels enables expert-level prompt optimization,\u201d\nCoRR , vol. abs/2310.16427, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2310.16427\n[475] T. Tang, J. Li, W. X. Zhao, and J. Wen, \u201cContext-tuning:\nLearning contextualized prompts for natural language\ngeneration,\u201d in Proceedings of the 29th International\nConference on Computational Linguistics, COLING 2022,\nGyeongju, Republic of Korea, October 12-17, 2022 , N. Cal-\nzolari, C. Huang, H. Kim, J. Pustejovsky, L. Wanner,\nK. Choi, P . Ryu, H. Chen, L. Donatelli, H. Ji, S. Kuro-\nhashi, P . Paggio, N. Xue, S. Kim, Y. Hahm, Z. He, T. K.\nLee, E. Santus, F. Bond, and S. Na, Eds. International\nCommittee on Computational Linguistics, 2022, pp.\n6340\u20136354.\n[476] T. Vu, B. Lester, N. Constant, R. Al-Rfou\u2019, and D. Cer,\n\u201cSpot: Better frozen model adaptation through soft\nprompt transfer,\u201d in Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022 , S. Muresan, P . Nakov, and A. Villavicen-\ncio, Eds. Association for Computational Linguistics,\n2022, pp. 5039\u20135059.[477] J. Li, T. Tang, J. Nie, J. Wen, and X. Zhao, \u201cLearning to\ntransfer prompts for text generation,\u201d in Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL 2022, Seattle, WA, United\nStates, July 10-15, 2022 , M. Carpuat, M. de Marneffe,\nand I. V . M. Ru \u00b4\u0131z, Eds. Association for Computational\nLinguistics, 2022, pp. 3506\u20133518.\n[478] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis,\nH. Hajishirzi, and L. Zettlemoyer, \u201cRethinking the role\nof demonstrations: What makes in-context learning\nwork?\u201d in Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December 7-\n11, 2022 . Association for Computational Linguistics,\n2022, pp. 11 048\u201311 064.\n[479] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh,\n\u201cCalibrate before use: Improving few-shot perfor-\nmance of language models,\u201d in Proceedings of the\n38th International Conference on Machine Learning, ICML\n2021, 18-24 July 2021, Virtual Event , M. Meila and\nT. Zhang, Eds., 2021, pp. 12 697\u201312 706.\n[480] Y. Lee, C. Lim, and H. Choi, \u201cDoes GPT-3 generate\nempathetic dialogues? A novel in-context example\nselection method and automatic evaluation metric\nfor empathetic dialogue generation,\u201d in Proceedings\nof the 29th International Conference on Computational\nLinguistics, COLING 2022, Gyeongju, Republic of Korea,\nOctober 12-17, 2022 , N. Calzolari, C. Huang, H. Kim,\nJ. Pustejovsky, L. Wanner, K. Choi, P . Ryu, H. Chen,\nL. Donatelli, H. Ji, S. Kurohashi, P . Paggio, N. Xue,\nS. Kim, Y. Hahm, Z. He, T. K. Lee, E. Santus, F. Bond,\nand S. Na, Eds. International Committee on Compu-\ntational Linguistics, 2022, pp. 669\u2013683.\n[481] I. Levy, B. Bogin, and J. Berant, \u201cDiverse demon-\nstrations improve in-context compositional general-\nization,\u201d CoRR , vol. abs/2212.06800, 2022.\n[482] H. Su, J. Kasai, C. H. Wu, W. Shi, T. Wang, J. Xin,\nR. Zhang, M. Ostendorf, L. Zettlemoyer, N. A. Smith,\nand T. Yu, \u201cSelective annotation makes language mod-\nels better few-shot learners,\u201d CoRR , 2022.\n[483] X. Ye, S. Iyer, A. Celikyilmaz, V . Stoyanov, G. Durrett,\nand R. Pasunuru, \u201cComplementary explanations for\neffective in-context learning,\u201d CoRR , 2022.\n[484] X. Li and X. Qiu, \u201cFinding supporting examples for\nin-context learning,\u201d CoRR , 2023.\n[485] Y. Zhang, S. Feng, and C. Tan, \u201cActive example se-\nlection for in-context learning,\u201d in Proceedings of the\n2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022 , 2022, pp. 9134\u20139148.\n[486] F. Gilardi, M. Alizadeh, and M. Kubli, \u201cChatgpt out-\nperforms crowd-workers for text-annotation tasks,\u201d\n2023.\n[487] H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and\nS. Lee, \u201cSelf-generated in-context learning: Leverag-\ning auto-regressive language models as a demonstra-\ntion generator,\u201d CoRR , vol. abs/2206.08082, 2022.\n[488] S. M. Xie, A. Raghunathan, P . Liang, and T. Ma, \u201cAn\nexplanation of in-context learning as implicit bayesian\ninference,\u201d in The Tenth International Conference on106\nLearning Representations, ICLR 2022, Virtual Event, April\n25-29, 2022 , 2022.\n[489] Z. Wu, Y. Wang, J. Ye, and L. Kong, \u201cSelf-adaptive in-\ncontext learning,\u201d CoRR , vol. abs/2212.10375, 2022.\n[490] Y. Gu, L. Dong, F. Wei, and M. Huang, \u201cPre-training\nto learn in context,\u201d CoRR , vol. abs/2305.09137, 2023.\n[491] S. Min, M. Lewis, L. Zettlemoyer, and H. Hajishirzi,\n\u201cMetaicl: Learning to learn in context,\u201d in Proceedings\nof the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, NAACL 2022, Seattle, WA, United\nStates, July 10-15, 2022 , M. Carpuat, M. de Marneffe,\nand I. V . M. Ru \u00b4\u0131z, Eds., 2022, pp. 2791\u20132809.\n[492] M. Hahn and N. Goyal, \u201cA theory of emergent\nin-context learning as implicit structure induction,\u201d\nCoRR , vol. abs/2303.07971, 2023.\n[493] J. Pan, T. Gao, H. Chen, and D. Chen, \u201cWhat in-context\nlearning \u201dlearns\u201d in-context: Disentangling task recog-\nnition and task learning,\u201d CoRR , vol. abs/2305.09731,\n2023.\n[494] N. Wies, Y. Levine, and A. Shashua, \u201cThe learnability\nof in-context learning,\u201d CoRR , vol. abs/2303.07895,\n2023.\n[495] A. Webson and E. Pavlick, \u201cDo prompt-based models\nreally understand the meaning of their prompts?\u201d in\nProceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL 2022, Seattle,\nWA, United States, July 10-15, 2022 , 2022, pp. 2300\u2013\n2344.\n[496] J. von Oswald, E. Niklasson, E. Randazzo, J. Sacra-\nmento, A. Mordvintsev, A. Zhmoginov, and M. Vla-\ndymyrov, \u201cTransformers learn in-context by gradient\ndescent,\u201d CoRR , vol. abs/2212.07677, 2022.\n[497] C. Olsson, N. Elhage, N. Nanda, N. Joseph,\nN. DasSarma, T. Henighan, B. Mann, A. Askell,\nY. Bai, A. Chen, T. Conerly, D. Drain, D. Gan-\nguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston,\nA. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei,\nT. Brown, J. Clark, J. Kaplan, S. McCandlish, and\nC. Olah, \u201cIn-context learning and induction heads,\u201d\nCoRR , vol. abs/2209.11895, 2022.\n[498] E. Aky \u00a8urek, D. Schuurmans, J. Andreas, T. Ma, and\nD. Zhou, \u201cWhat learning algorithm is in-context learn-\ning? investigations with linear models,\u201d CoRR , vol.\nabs/2211.15661, 2022.\n[499] J. Wei, J. Wei, Y. Tay, D. Tran, A. Webson, Y. Lu,\nX. Chen, H. Liu, D. Huang, D. Zhou et al. , \u201cLarger\nlanguage models do in-context learning differently,\u201d\narXiv preprint arXiv:2303.03846 , 2023.\n[500] J. Coda-Forno, M. Binz, Z. Akata, M. M. Botvinick,\nJ. X. Wang, and E. Schulz, \u201cMeta-in-context learning\nin large language models,\u201d CoRR , vol. abs/2305.12907,\n2023.\n[501] J. W. Wei, L. Hou, A. K. Lampinen, X. Chen, D. Huang,\nY. Tay, X. Chen, Y. Lu, D. Zhou, T. Ma, and Q. V .\nLe, \u201cSymbol tuning improves in-context learning in\nlanguage models,\u201d CoRR , vol. abs/2305.08298, 2023.\n[502] Z. Chu, J. Chen, Q. Chen, W. Yu, T. He, H. Wang,\nW. Peng, M. Liu, B. Qin, and T. Liu, \u201cA survey of\nchain of thought reasoning: Advances, frontiers andfuture,\u201d CoRR , vol. abs/2309.15402, 2023.\n[503] S. Miao, C. Liang, and K. Su, \u201cA diverse corpus\nfor evaluating and developing english math word\nproblem solvers,\u201d in Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020 , D. Jurafsky, J. Chai,\nN. Schluter, and J. R. Tetreault, Eds. Association for\nComputational Linguistics, 2020, pp. 975\u2013984.\n[504] A. Talmor, J. Herzig, N. Lourie, and J. Berant, \u201cCom-\nmonsenseqa: A question answering challenge tar-\ngeting commonsense knowledge,\u201d in Proceedings of\nthe 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2019, Minneapolis,\nMN, USA, June 2-7, 2019, Volume 1 (Long and Short\nPapers) , J. Burstein, C. Doran, and T. Solorio, Eds.\nAssociation for Computational Linguistics, 2019, pp.\n4149\u20134158.\n[505] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwa-\nsawa, \u201cLarge language models are zero-shot reason-\ners,\u201d CoRR , vol. abs/2205.11916, 2022.\n[506] W. Chen, X. Ma, X. Wang, and W. W. Cohen, \u201cProgram\nof thoughts prompting: Disentangling computation\nfrom reasoning for numerical reasoning tasks,\u201d CoRR ,\nvol. abs/2211.12588, 2022.\n[507] L. Gao, A. Madaan, S. Zhou, U. Alon, P . Liu, Y. Yang,\nJ. Callan, and G. Neubig, \u201cPAL: program-aided lan-\nguage models,\u201d in International Conference on Machine\nLearning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,\nUSA , A. Krause, E. Brunskill, K. Cho, B. Engelhardt,\nS. Sabato, and J. Scarlett, Eds., 2023.\n[508] X. Zhao, Y. Xie, K. Kawaguchi, J. He, and Q. Xie, \u201cAu-\ntomatic model selection with large language models\nfor reasoning,\u201d CoRR , vol. abs/2305.14333, 2023.\n[509] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou,\nand W. Chen, \u201cMaking large language models better\nreasoners with step-aware verifier,\u201d 2023.\n[510] O. Yoran, T. Wolfson, B. Bogin, U. Katz, D. Deutch,\nand J. Berant, \u201cAnswering questions by meta-\nreasoning over multiple chains of thought,\u201d CoRR ,\nvol. abs/2304.13007, 2023.\n[511] Z. Ling, Y. Fang, X. Li, Z. Huang, M. Lee, R. Memi-\nsevic, and H. Su, \u201cDeductive verification of chain-of-\nthought reasoning,\u201d CoRR , vol. abs/2306.03872, 2023.\n[512] T. Xue, Z. Wang, Z. Wang, C. Han, P . Yu, and H. Ji,\n\u201cRCOT: detecting and rectifying factual inconsistency\nin reasoning by reversing chain-of-thought,\u201d CoRR ,\nvol. abs/2305.11499, 2023.\n[513] Y. Weng, M. Zhu, F. Xia, B. Li, S. He, K. Liu, and\nJ. Zhao, \u201cLarge language models are better reasoners\nwith self-verification,\u201d CoRR, abs/2212.09561 , 2023.\n[514] W. Jiang, H. Shi, L. Yu, Z. Liu, Y. Zhang, Z. Li, and\nJ. T. Kwok, \u201cForward-backward reasoning in large\nlanguage models for mathematical verification,\u201d 2023.\n[515] J. Long, \u201cLarge language model guided tree-of-\nthought,\u201d CoRR , vol. abs/2305.08291, 2023.\n[516] S. Mo and M. Xin, \u201cTree of uncertain thoughts\nreasoning for large language models,\u201d CoRR , vol.\nabs/2309.07694, 2023.\n[517] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger,\nL. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski,107\nH. Niewiadomski, P . Nyczyk, and T. Hoefler, \u201cGraph\nof thoughts: Solving elaborate problems with large\nlanguage models,\u201d CoRR , vol. abs/2308.09687, 2023.\n[518] B. Lei, P . Lin, C. Liao, and C. Ding, \u201cBoosting log-\nical reasoning in large language models through a\nnew framework: The graph of thought,\u201d CoRR , vol.\nabs/2308.08614, 2023.\n[519] R. Ding, C. Zhang, L. Wang, Y. Xu, M. Ma, W. Zhang,\nS. Qin, S. Rajmohan, Q. Lin, and D. Zhang, \u201cEv-\nerything of thoughts: Defying the law of pen-\nrose triangle for thought generation,\u201d arXiv preprint\narXiv:2311.04254 , 2023.\n[520] P . Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu,\nM. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Ku-\nmar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cos-\ngrove, C. D. Manning, C. R \u00b4e, D. Acosta-Navas, D. A.\nHudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong,\nH. Ren, H. Yao, J. Wang, K. Santhanam, L. J. Orr,\nL. Zheng, M. Y \u00a8uksekg \u00a8on\u00a8ul, M. Suzgun, N. Kim,\nN. Guha, N. S. Chatterji, O. Khattab, P . Henderson,\nQ. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Gan-\nguli, T. Hashimoto, T. Icard, T. Zhang, V . Chaudhary,\nW. Wang, X. Li, Y. Mai, Y. Zhang, and Y. Koreeda,\n\u201cHolistic evaluation of language models,\u201d CoRR , vol.\nabs/2211.09110, 2022.\n[521] Z. Bi, N. Zhang, Y. Jiang, S. Deng, G. Zheng, and\nH. Chen, \u201cWhen do program-of-thoughts work for\nreasoning?\u201d CoRR , vol. abs/2308.15452, 2023.\n[522] A. Madaan and A. Yazdanbakhsh, \u201cText and patterns:\nFor effective chain of thought, it takes two to tango,\u201d\nCoRR , vol. abs/2209.07686, 2022.\n[523] Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and\nA. Smola, \u201cMultimodal chain-of-thought reasoning in\nlanguage models,\u201d CoRR , vol. abs/2302.00923, 2023.\n[524] F. Shi, M. Suzgun, M. Freitag, X. Wang, S. Sri-\nvats, S. Vosoughi, H. W. Chung, Y. Tay, S. Ruder,\nD. Zhou, D. Das, and J. Wei, \u201cLanguage models are\nmultilingual chain-of-thought reasoners,\u201d CoRR , vol.\nabs/2210.03057, 2022.\n[525] J. Qian, H. Wang, Z. Li, S. Li, and X. Yan, \u201cLimita-\ntions of language models in arithmetic and symbolic\ninduction,\u201d CoRR , vol. abs/2208.05051, 2022.\n[526] N. Bian, X. Han, L. Sun, H. Lin, Y. Lu, and B. He,\n\u201cChatGPT is a Knowledgeable but Inexperienced\nSolver: An Investigation of Commonsense Problem in\nLarge Language Models,\u201d CoRR , 2023.\n[527] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao,\nand K. Narasimhan, \u201cTree of thoughts: Deliberate\nproblem solving with large language models,\u201d CoRR ,\nvol. abs/2305.10601, 2023.\n[528] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao,\nY. Zhu, L. Fan, and A. Anandkumar, \u201cVoyager: An\nopen-ended embodied agent with large language\nmodels,\u201d arXiv preprint arXiv:2305.16291 , 2023.\n[529] X. Jiang, Y. Dong, L. Wang, Q. Shang, and G. Li,\n\u201cSelf-planning code generation with large language\nmodel,\u201d CoRR , vol. abs/2303.06689, 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2303.06689\n[530] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu,\nJ. Tremblay, D. Fox, J. Thomason, and A. Garg, \u201cProg-\nprompt: Generating situated robot task plans usinglarge language models,\u201d CoRR , vol. abs/2209.11302,\n2022.\n[531] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang,\nJ. Biswas, and P . Stone, \u201cLLM+P: empowering large\nlanguage models with optimal planning proficiency,\u201d\nCoRR , vol. abs/2304.11477, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2304.11477\n[532] R. Rombach, A. Blattmann, D. Lorenz, P . Esser, and\nB. Ommer, \u201cHigh-resolution image synthesis with\nlatent diffusion models,\u201d in IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2022,\nNew Orleans, LA, USA, June 18-24, 2022 , 2022, pp.\n10 674\u201310 685.\n[533] J. S. Park, J. C. O\u2019Brien, C. J. Cai, M. R. Morris,\nP . Liang, and M. S. Bernstein, \u201cGenerative agents:\nInteractive simulacra of human behavior,\u201d CoRR , vol.\nabs/2304.03442, 2023.\n[534] 2023. [Online]. Available: https://github.com/\nSignificant-Gravitas/Auto-GPT\n[535] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, \u201cDescribe,\nexplain, plan and select: Interactive planning with\nlarge language models enables open-world multi-task\nagents,\u201d CoRR , vol. abs/2302.01560, 2023.\n[536] J. Wang, X. Yi, R. Guo, H. Jin, P . Xu, S. Li, X. Wang,\nX. Guo, C. Li, X. Xu et al. , \u201cMilvus: A purpose-built\nvector data management system,\u201d in Proceedings of the\n2021 International Conference on Management of Data ,\n2021, pp. 2614\u20132627.\n[537] W. Zhong, L. Guo, Q. Gao, H. Ye, and Y. Wang, \u201cMem-\norybank: Enhancing large language models with long-\nterm memory,\u201d CoRR , vol. abs/2305.10250, 2023.\n[538] M. P . Marcus, B. Santorini, and M. A. Marcinkiewicz,\n\u201cBuilding a large annotated corpus of english: The\npenn treebank,\u201d Comput. Linguistics , vol. 19, no. 2, pp.\n313\u2013330, 1993.\n[539] S. Merity, C. Xiong, J. Bradbury, and R. Socher,\n\u201cPointer sentinel mixture models,\u201d in ICLR (Poster) .\nOpenReview.net, 2017.\n[540] O. Bojar, C. Buck, C. Federmann, B. Haddow,\nP . Koehn, J. Leveling, C. Monz, P . Pecina, M. Post,\nH. Saint-Amand, R. Soricut, L. Specia, and A. Tam-\nchyna, \u201cFindings of the 2014 workshop on statistical\nmachine translation,\u201d in WMT@ACL . The Association\nfor Computer Linguistics, 2014, pp. 12\u201358.\n[541] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham,\nB. Haddow, M. Huck, A. Jimeno-Yepes, P . Koehn,\nV . Logacheva, C. Monz, M. Negri, A. N \u00b4ev\u00b4eol, M. L.\nNeves, M. Popel, M. Post, R. Rubino, C. Scarton,\nL. Specia, M. Turchi, K. Verspoor, and M. Zampieri,\n\u201cFindings of the 2016 conference on machine trans-\nlation,\u201d in WMT . The Association for Computer\nLinguistics, 2016, pp. 131\u2013198.\n[542] L. Barrault, O. Bojar, M. R. Costa-juss `a, C. Federmann,\nM. Fishel, Y. Graham, B. Haddow, M. Huck, P . Koehn,\nS. Malmasi, C. Monz, M. M \u00a8uller, S. Pal, M. Post, and\nM. Zampieri, \u201cFindings of the 2019 conference on\nmachine translation (WMT19),\u201d in Proceedings of the\nFourth Conference on Machine Translation, WMT 2019,\nFlorence, Italy, August 1-2, 2019 - Volume 2: Shared\nTask Papers, Day 1 , O. Bojar, R. Chatterjee, C. Feder-\nmann, M. Fishel, Y. Graham, B. Haddow, M. Huck,108\nA. Jimeno-Yepes, P . Koehn, A. Martins, C. Monz,\nM. Negri, A. N \u00b4ev\u00b4eol, M. L. Neves, M. Post, M. Turchi,\nand K. Verspoor, Eds. Association for Computational\nLinguistics, 2019, pp. 1\u201361.\n[543] L. Barrault, M. Biesialska, O. Bojar, M. R. Costa-\njuss`a, C. Federmann, Y. Graham, R. Grundkiewicz,\nB. Haddow, M. Huck, E. Joanis, T. Kocmi, P . Koehn,\nC. Lo, N. Ljubesic, C. Monz, M. Morishita, M. Na-\ngata, T. Nakazawa, S. Pal, M. Post, and M. Zampieri,\n\u201cFindings of the 2020 conference on machine trans-\nlation (WMT20),\u201d in Proceedings of the Fifth Con-\nference on Machine Translation, WMT@EMNLP 2020,\nOnline, November 19-20, 2020 , L. Barrault, O. Bojar,\nF. Bougares, R. Chatterjee, M. R. Costa-juss `a, C. Fe-\ndermann, M. Fishel, A. Fraser, Y. Graham, P . Guzman,\nB. Haddow, M. Huck, A. Jimeno-Yepes, P . Koehn,\nA. Martins, M. Morishita, C. Monz, M. Nagata,\nT. Nakazawa, and M. Negri, Eds. Association for\nComputational Linguistics, 2020, pp. 1\u201355.\n[544] F. Akhbardeh, A. Arkhangorodsky, M. Biesialska,\nO. Bojar, R. Chatterjee, V . Chaudhary, M. R. Costa-\njuss`a, C. Espa \u02dcna-Bonet, A. Fan, C. Federmann, M. Fre-\nitag, Y. Graham, R. Grundkiewicz, B. Haddow, L. Har-\nter, K. Heafield, C. Homan, M. Huck, K. Amponsah-\nKaakyire, J. Kasai, D. Khashabi, K. Knight, T. Kocmi,\nP . Koehn, N. Lourie, C. Monz, M. Morishita, M. Na-\ngata, A. Nagesh, T. Nakazawa, M. Negri, S. Pal,\nA. A. Tapo, M. Turchi, V . Vydrin, and M. Zampieri,\n\u201cFindings of the 2021 conference on machine transla-\ntion (WMT21),\u201d in Proceedings of the Sixth Conference\non Machine Translation, WMT@EMNLP 2021, Online\nEvent, November 10-11, 2021 , L. Barrault, O. Bojar,\nF. Bougares, R. Chatterjee, M. R. Costa-juss `a, C. Fe-\ndermann, M. Fishel, A. Fraser, M. Freitag, Y. Graham,\nR. Grundkiewicz, P . Guzman, B. Haddow, M. Huck,\nA. Jimeno-Yepes, P . Koehn, T. Kocmi, A. Martins,\nM. Morishita, and C. Monz, Eds. Association for\nComputational Linguistics, 2021, pp. 1\u201388.\n[545] T. Kocmi, R. Bawden, O. Bojar, A. Dvorkovich, C. Fe-\ndermann, M. Fishel, T. Gowda, Y. Graham, R. Grund-\nkiewicz, B. Haddow, R. Knowles, P . Koehn, C. Monz,\nM. Morishita, M. Nagata, T. Nakazawa, M. Nov \u00b4ak,\nM. Popel, and M. Popovic, \u201cFindings of the 2022\nconference on machine translation (WMT22),\u201d in Pro-\nceedings of the Seventh Conference on Machine Trans-\nlation, WMT 2022, Abu Dhabi, United Arab Emirates\n(Hybrid), December 7-8, 2022 , P . Koehn, L. Barrault,\nO. Bojar, F. Bougares, R. Chatterjee, M. R. Costa-\njuss`a, C. Federmann, M. Fishel, A. Fraser, M. Freitag,\nY. Graham, R. Grundkiewicz, P . Guzman, B. Haddow,\nM. Huck, A. Jimeno-Yepes, T. Kocmi, A. Martins,\nM. Morishita, C. Monz, M. Nagata, T. Nakazawa,\nM. Negri, A. N \u00b4ev\u00b4eol, M. Neves, M. Popel, M. Turchi,\nand M. Zampieri, Eds. Association for Computa-\ntional Linguistics, 2022, pp. 1\u201345.\n[546] N. Goyal, C. Gao, V . Chaudhary, P . Chen, G. Wen-\nzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzm \u00b4an, and\nA. Fan, \u201cThe flores-101 evaluation benchmark for low-\nresource and multilingual machine translation,\u201d Trans.\nAssoc. Comput. Linguistics , vol. 10, pp. 522\u2013538, 2022.\n[547] R. Bawden, E. Bilinski, T. Lavergne, and S. Rosset,\u201cDiabla: a corpus of bilingual spontaneous written\ndialogues for machine translation,\u201d Lang. Resour. Eval-\nuation , vol. 55, no. 3, pp. 635\u2013660, 2021.\n[548] R. Nallapati, B. Zhou, C. N. dos Santos, C \u00b8 . G \u00a8ulc \u00b8ehre,\nand B. Xiang, \u201cAbstractive text summarization using\nsequence-to-sequence rnns and beyond,\u201d in Proceed-\nings of the 20th SIGNLL Conference on Computational\nNatural Language Learning, CoNLL 2016, Berlin, Ger-\nmany, August 11-12, 2016 , Y. Goldberg and S. Riezler,\nEds. ACL, 2016, pp. 280\u2013290.\n[549] S. Narayan, S. B. Cohen, and M. Lapata, \u201cDon\u2019t give\nme the details, just the summary! topic-aware convo-\nlutional neural networks for extreme summarization,\u201d\ninEMNLP . Association for Computational Linguis-\ntics, 2018, pp. 1797\u20131807.\n[550] F. Ladhak, E. Durmus, C. Cardie, and K. Mckeown,\n\u201cWikilingua: A new benchmark dataset for cross-\nlingual abstractive summarization,\u201d in Findings of the\nAssociation for Computational Linguistics: EMNLP 2020 ,\n2020, pp. 4034\u20134048.\n[551] S. Moon, P . Shah, A. Kumar, and R. Subba, \u201cOpen-\ndialkg: Explainable conversational reasoning with\nattention-based walks over knowledge graphs,\u201d in\nACL (1) . Association for Computational Linguistics,\n2019, pp. 845\u2013854.\n[552] Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettle-\nmoyer, S. W. Yih, D. Fried, S. I. Wang, and T. Yu,\n\u201cDS-1000: A natural and reliable benchmark for data\nscience code generation,\u201d CoRR , vol. abs/2211.11501,\n2022.\n[553] Z. Wang, S. Zhou, D. Fried, and G. Neubig,\n\u201cExecution-based evaluation for open-domain code\ngeneration,\u201d CoRR , vol. abs/2212.10481, 2022.\n[554] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins,\nA. P . Parikh, C. Alberti, D. Epstein, I. Polosukhin,\nJ. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey,\nM. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov,\n\u201cNatural questions: a benchmark for question answer-\ning research,\u201d Trans. Assoc. Comput. Linguistics , pp.\n452\u2013466, 2019.\n[555] P . Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal,\nC. Schoenick, and O. Tafjord, \u201cThink you have solved\nquestion answering? try arc, the AI2 reasoning chal-\nlenge,\u201d CoRR , vol. abs/1803.05457, 2018.\n[556] S. Lin, J. Hilton, and O. Evans, \u201cTruthfulqa: Measuring\nhow models mimic human falsehoods,\u201d in Proceedings\nof the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022 , 2022, pp. 3214\u20133252.\n[557] J. Berant, A. Chou, R. Frostig, and P . Liang, \u201cSemantic\nparsing on freebase from question-answer pairs,\u201d in\nProceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2013, 18-21\nOctober 2013, Grand Hyatt Seattle, Seattle, Washington,\nUSA, A meeting of SIGDAT, a Special Interest Group of\nthe ACL , 2013, pp. 1533\u20131544.\n[558] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer,\n\u201cTriviaqa: A large scale distantly supervised challenge\ndataset for reading comprehension,\u201d in Proceedings of\nthe 55th Annual Meeting of the Association for Computa-\ntional Linguistics, ACL 2017, Vancouver, Canada, July 30109\n- August 4, Volume 1: Long Papers , 2017, pp. 1601\u20131611.\n[559] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi,\n\u201cPIQA: reasoning about physical commonsense in\nnatural language,\u201d in The Thirty-Fourth AAAI Confer-\nence on Artificial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artificial Intelligence\nConference, IAAI 2020, The Tenth AAAI Symposium\non Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020 , 2020,\npp. 7432\u20137439.\n[560] M. Dubey, D. Banerjee, A. Abdelkawi, and\nJ. Lehmann, \u201cLc-quad 2.0: A large dataset for\ncomplex question answering over wikidata and\ndbpedia,\u201d in The Semantic Web - ISWC 2019 - 18th\nInternational Semantic Web Conference, Auckland, New\nZealand, October 26-30, 2019, Proceedings, Part II , 2019,\npp. 69\u201378.\n[561] Y. Gu, S. Kase, M. Vanni, B. M. Sadler, P . Liang, X. Yan,\nand Y. Su, \u201cBeyond I.I.D.: three levels of generaliza-\ntion for question answering on knowledge bases,\u201d in\nWWW \u201921: The Web Conference 2021, Virtual Event /\nLjubljana, Slovenia, April 19-23, 2021 , 2021, pp. 3477\u2013\n3488.\n[562] S. Cao, J. Shi, L. Pan, L. Nie, Y. Xiang, L. Hou, J. Li,\nB. He, and H. Zhang, \u201cKQA pro: A dataset with\nexplicit compositional programs for complex question\nanswering over knowledge base,\u201d in Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022, Dublin,\nIreland, May 22-27, 2022 , 2022, pp. 6101\u20136119.\n[563] X. Hu, X. Wu, Y. Shu, and Y. Qu, \u201cLogical form\ngeneration via multi-task learning for complex ques-\ntion answering over knowledge bases,\u201d in Proceedings\nof the 29th International Conference on Computational\nLinguistics, COLING 2022, Gyeongju, Republic of Korea,\nOctober 12-17, 2022 , 2022, pp. 1687\u20131696.\n[564] S. Longpre, Y. Lu, and J. Daiber, \u201cMKQA: A lin-\nguistically diverse benchmark for multilingual open\ndomain question answering,\u201d Trans. Assoc. Comput.\nLinguistics , vol. 9, pp. 1389\u20131406, 2021.\n[565] T. Saikh, T. Ghosal, A. Mittal, A. Ekbal, and P . Bhat-\ntacharyya, \u201cScienceqa: a novel resource for question\nanswering on scholarly articles,\u201d Int. J. Digit. Libr. ,\nvol. 23, no. 3, pp. 289\u2013301, 2022.\n[566] T. Mihaylov, P . Clark, T. Khot, and A. Sabharwal, \u201cCan\na suit of armor conduct electricity? A new dataset\nfor open book question answering,\u201d in Proceedings of\nthe 2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018 , 2018, pp. 2381\u20132391.\n[567] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary,\nR. Majumder, and L. Deng, \u201cMS MARCO: A human\ngenerated machine reading comprehension dataset,\u201d\ninProceedings of the Workshop on Cognitive Computa-\ntion: Integrating neural and symbolic approaches 2016\nco-located with the 30th Annual Conference on Neural\nInformation Processing Systems (NIPS 2016), Barcelona,\nSpain, December 9, 2016 , 2016.\n[568] T. Khot, P . Clark, M. Guerquin, P . Jansen, and A. Sab-\nharwal, \u201cQASC: A dataset for question answering\nvia sentence composition,\u201d in The Thirty-Fourth AAAIConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Symposium\non Educational Advances in Artificial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020 , 2020,\npp. 8082\u20138090.\n[569] P . Rajpurkar, J. Zhang, K. Lopyrev, and P . Liang,\n\u201cSquad: 100, 000+ questions for machine comprehen-\nsion of text,\u201d in Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Processing,\nEMNLP 2016, Austin, Texas, USA, November 1-4, 2016 ,\n2016, pp. 2383\u20132392.\n[570] A. H. Miller, A. Fisch, J. Dodge, A. Karimi, A. Bordes,\nand J. Weston, \u201cKey-value memory networks for di-\nrectly reading documents,\u201d in Proceedings of the 2016\nConference on Empirical Methods in Natural Language\nProcessing, EMNLP 2016, Austin, Texas, USA, November\n1-4, 2016 , 2016, pp. 1400\u20131409.\n[571] B. Goodrich, V . Rao, P . J. Liu, and M. Saleh, \u201cAssessing\nthe factual accuracy of generated text,\u201d in Proceedings\nof the 25th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, KDD 2019, An-\nchorage, AK, USA, August 4-8, 2019 , 2019, pp. 166\u2013175.\n[572] K. Toutanova and D. Chen, \u201cObserved versus latent\nfeatures for knowledge base and text inference,\u201d in\nProceedings of the 3rd Workshop on Continuous Vector\nSpace Models and their Compositionality, CVSC 2015,\nBeijing, China, July 26-31, 2015 , 2015, pp. 57\u201366.\n[573] K. D. Bollacker, C. Evans, P . K. Paritosh, T. Sturge, and\nJ. Taylor, \u201cFreebase: a collaboratively created graph\ndatabase for structuring human knowledge,\u201d in Pro-\nceedings of the ACM SIGMOD International Conference\non Management of Data, SIGMOD 2008, Vancouver, BC,\nCanada, June 10-12, 2008 , 2008, pp. 1247\u20131250.\n[574] T. Dettmers, P . Minervini, P . Stenetorp, and S. Riedel,\n\u201cConvolutional 2d knowledge graph embeddings,\u201d\ninProceedings of the Thirty-Second AAAI Conference on\nArtificial Intelligence, (AAAI-18), the 30th innovative Ap-\nplications of Artificial Intelligence (IAAI-18), and the 8th\nAAAI Symposium on Educational Advances in Artificial\nIntelligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018 , 2018, pp. 1811\u20131818.\n[575] G. A. Miller, \u201cWordnet: A lexical database for en-\nglish,\u201d Commun. ACM , pp. 39\u201341, 1995.\n[576] F. Petroni, T. Rockt \u00a8aschel, S. Riedel, P . S. H. Lewis,\nA. Bakhtin, Y. Wu, and A. H. Miller, \u201cLanguage mod-\nels as knowledge bases?\u201d in Proceedings of the 2019\nConference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference\non Natural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019 , 2019, pp. 2463\u2013\n2473.\n[577] F. Mahdisoltani, J. Biega, and F. M. Suchanek,\n\u201cYAGO3: A knowledge base from multilingual\nwikipedias,\u201d in Seventh Biennial Conference on Innova-\ntive Data Systems Research, CIDR 2015, Asilomar, CA,\nUSA, January 4-7, 2015, Online Proceedings , 2015.\n[578] F. M. Suchanek, G. Kasneci, and G. Weikum, \u201cYago:\na core of semantic knowledge,\u201d in Proceedings of the\n16th International Conference on World Wide Web, WWW\n2007, Banff, Alberta, Canada, May 8-12, 2007 , 2007, pp.110\n697\u2013706.\n[579] Z. Yang, P . Qi, S. Zhang, Y. Bengio, W. W. Cohen,\nR. Salakhutdinov, and C. D. Manning, \u201cHotpotqa: A\ndataset for diverse, explainable multi-hop question\nanswering,\u201d in Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing, Brus-\nsels, Belgium, October 31 - November 4, 2018 . Associ-\nation for Computational Linguistics, 2018, pp. 2369\u2013\n2380.\n[580] C. Clark, K. Lee, M. Chang, T. Kwiatkowski,\nM. Collins, and K. Toutanova, \u201cBoolq: Exploring the\nsurprising difficulty of natural yes/no questions,\u201d in\nProceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019, Min-\nneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and\nShort Papers) , J. Burstein, C. Doran, and T. Solorio, Eds.\nAssociation for Computational Linguistics, 2019, pp.\n2924\u20132936.\n[581] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y. Choi,\n\u201cSocialiqa: Commonsense reasoning about social in-\nteractions,\u201d CoRR , vol. abs/1904.09728, 2019.\n[582] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and\nY. Choi, \u201cHellaswag: Can a machine really finish\nyour sentence?\u201d in Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers , A. Korhonen, D. R. Traum, and L. M `arquez,\nEds. Association for Computational Linguistics, 2019,\npp. 4791\u20134800.\n[583] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi,\n\u201cWinogrande: An adversarial winograd schema chal-\nlenge at scale,\u201d in AAAI . AAAI Press, 2020, pp. 8732\u2013\n8740.\n[584] M. Roemmele, C. A. Bejan, and A. S. Gordon, \u201cChoice\nof plausible alternatives: An evaluation of common-\nsense causal reasoning,\u201d in Logical Formalizations of\nCommonsense Reasoning, Papers from the 2011 AAAI\nSpring Symposium, Technical Report SS-11-06, Stanford,\nCalifornia, USA, March 21-23, 2011 . AAAI, 2011.\n[585] K. Sakaguchi, C. Bhagavatula, R. L. Bras, N. Tandon,\nP . Clark, and Y. Choi, \u201cproscript: Partially ordered\nscripts generation,\u201d in Findings of the Association for\nComputational Linguistics: EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 16-20 November, 2021 ,\nM. Moens, X. Huang, L. Specia, and S. W. Yih, Eds.\nAssociation for Computational Linguistics, 2021, pp.\n2138\u20132149.\n[586] B. Dalvi, L. Huang, N. Tandon, W. Yih, and P . Clark,\n\u201cTracking state changes in procedural text: a challenge\ndataset and models for process paragraph comprehen-\nsion,\u201d in Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HLT\n2018, New Orleans, Louisiana, USA, June 1-6, 2018, Vol-\nume 1 (Long Papers) , M. A. Walker, H. Ji, and A. Stent,\nEds. Association for Computational Linguistics, 2018,\npp. 1595\u20131604.\n[587] S. Saha, P . Yadav, L. Bauer, and M. Bansal, \u201cExpla-\ngraphs: An explanation graph generation task for\nstructured commonsense reasoning,\u201d in Proceedingsof the 2021 Conference on Empirical Methods in Natu-\nral Language Processing, EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 7-11 November, 2021 ,\nM. Moens, X. Huang, L. Specia, and S. W. Yih, Eds.\nAssociation for Computational Linguistics, 2021, pp.\n7716\u20137740.\n[588] O. Tafjord, B. Dalvi, and P . Clark, \u201cProofwriter: Gener-\nating implications, proofs, and abductive statements\nover natural language,\u201d in Findings of the Association\nfor Computational Linguistics: ACL/IJCNLP 2021, Online\nEvent, August 1-6, 2021 , ser. Findings of ACL, C. Zong,\nF. Xia, W. Li, and R. Navigli, Eds., vol. ACL/IJCNLP\n2021. Association for Computational Linguistics,\n2021, pp. 3621\u20133634.\n[589] B. Dalvi, P . Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pi-\npatanangkura, and P . Clark, \u201cExplaining answers with\nentailment trees,\u201d in Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Dominican\nRepublic, 7-11 November, 2021 , M. Moens, X. Huang,\nL. Specia, and S. W. Yih, Eds. Association for Com-\nputational Linguistics, 2021, pp. 7358\u20137370.\n[590] A. Saparov and H. He, \u201cLanguage models are greedy\nreasoners: A systematic formal analysis of chain-of-\nthought,\u201d CoRR , vol. abs/2210.01240, 2022.\n[591] C. Anil, Y. Wu, A. Andreassen, A. Lewkowycz,\nV . Misra, V . V . Ramasesh, A. Slone, G. Gur-Ari,\nE. Dyer, and B. Neyshabur, \u201cExploring length gen-\neralization in large language models,\u201d CoRR , vol.\nabs/2207.04901, 2022.\n[592] A. Patel, S. Bhattamishra, and N. Goyal, \u201cAre NLP\nmodels really able to solve simple math word prob-\nlems?\u201d in NAACL-HLT . Association for Computa-\ntional Linguistics, 2021, pp. 2080\u20132094.\n[593] S. Roy and D. Roth, \u201cSolving general arithmetic\nword problems,\u201d in Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing,\nEMNLP 2015, Lisbon, Portugal, September 17-21, 2015 ,\nL. M `arquez, C. Callison-Burch, J. Su, D. Pighin, and\nY. Marton, Eds. The Association for Computational\nLinguistics, 2015, pp. 1743\u20131752.\n[594] A. Amini, S. Gabriel, S. Lin, R. Koncel-Kedziorski,\nY. Choi, and H. Hajishirzi, \u201cMathqa: Towards inter-\npretable math word problem solving with operation-\nbased formalisms,\u201d in Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2019, Minneapolis, MN, USA, June\n2-7, 2019, Volume 1 (Long and Short Papers) , J. Burstein,\nC. Doran, and T. Solorio, Eds. Association for Com-\nputational Linguistics, 2019, pp. 2357\u20132367.\n[595] W. Ling, D. Yogatama, C. Dyer, and P . Blunsom,\n\u201cProgram induction by rationale generation: Learning\nto solve and explain algebraic word problems,\u201d in\nProceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2017, Vancouver,\nCanada, July 30 - August 4, Volume 1: Long Papers ,\nR. Barzilay and M. Kan, Eds. Association for Com-\nputational Linguistics, 2017, pp. 158\u2013167.\n[596] R. Koncel-Kedziorski, S. Roy, A. Amini, N. Kushman,\nand H. Hajishirzi, \u201cMawps: A math word problem111\nrepository,\u201d in Proceedings of the 2016 conference of the\nnorth american chapter of the association for computational\nlinguistics: human language technologies , 2016, pp. 1152\u2013\n1157.\n[597] D. Dua, Y. Wang, P . Dasigi, G. Stanovsky, S. Singh,\nand M. Gardner, \u201cDROP: A reading comprehension\nbenchmark requiring discrete reasoning over para-\ngraphs,\u201d in Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers) , 2019, pp. 2368\u2013\n2378.\n[598] S. Welleck, J. Liu, R. L. Bras, H. Hajishirzi, Y. Choi,\nand K. Cho, \u201cNaturalproofs: Mathematical theorem\nproving in natural language,\u201d in Proceedings of the Neu-\nral Information Processing Systems Track on Datasets and\nBenchmarks 1, NeurIPS Datasets and Benchmarks 2021,\nDecember 2021, virtual , J. Vanschoren and S. Yeung,\nEds., 2021.\n[599] A. Q. Jiang, W. Li, J. M. Han, and Y. Wu, \u201cLisa:\nLanguage models of isabelle proofs,\u201d in 6th Conference\non Artificial Intelligence and Theorem Proving , 2021, pp.\n378\u2013392.\n[600] K. Zheng, J. M. Han, and S. Polu, \u201cminif2f: a cross-\nsystem benchmark for formal olympiad-level mathe-\nmatics,\u201d in The Tenth International Conference on Learn-\ning Representations, ICLR 2022, Virtual Event, April 25-\n29, 2022 . OpenReview.net, 2022.\n[601] Z. Azerbayev, B. Piotrowski, H. Schoelkopf, E. W.\nAyers, D. Radev, and J. Avigad, \u201cProofnet: Autofor-\nmalizing and formally proving undergraduate-level\nmathematics,\u201d CoRR , vol. abs/2302.12433, 2023.\n[602] J. Li, X. Cheng, W. X. Zhao, J. Nie, and J. Wen,\n\u201cHalueval: A large-scale hallucination evaluation\nbenchmark for large language models,\u201d CoRR , vol.\nabs/2305.11747, 2023.\n[603] N. Nangia, C. Vania, R. Bhalerao, and S. R. Bowman,\n\u201cCrows-pairs: A challenge dataset for measuring so-\ncial biases in masked language models,\u201d in Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, November\n16-20, 2020 , 2020, pp. 1953\u20131967.\n[604] R. Rudinger, J. Naradowsky, B. Leonard, and B. V .\nDurme, \u201cGender bias in coreference resolution,\u201d in\nProceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 2 (Short\nPapers) , 2018, pp. 8\u201314.\n[605] S. Gehman, S. Gururangan, M. Sap, Y. Choi, and N. A.\nSmith, \u201cRealtoxicityprompts: Evaluating neural toxic\ndegeneration in language models,\u201d in Findings of the\nAssociation for Computational Linguistics: EMNLP 2020,\nOnline Event, 16-20 November 2020 , ser. Findings of\nACL, T. Cohn, Y. He, and Y. Liu, Eds., vol. EMNLP\n2020. Association for Computational Linguistics,\n2020, pp. 3356\u20133369.\n[606] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler,\nand A. Torralba, \u201cVirtualhome: Simulating household\nactivities via programs,\u201d in CVPR . Computer VisionFoundation / IEEE Computer Society, 2018, pp. 8494\u2013\n8502.\n[607] S. Srivastava, C. Li, M. Lingelbach, R. Mart \u00b4\u0131n-Mart \u00b4\u0131n,\nF. Xia, K. E. Vainio, Z. Lian, C. Gokmen, S. Buch,\nC. K. Liu, S. Savarese, H. Gweon, J. Wu, and L. Fei-\nFei, \u201cBEHAVIOR: benchmark for everyday household\nactivities in virtual, interactive, and ecological en-\nvironments,\u201d in CoRL , ser. Proceedings of Machine\nLearning Research, vol. 164. PMLR, 2021, pp. 477\u2013\n490.\n[608] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han,\nR. Mottaghi, L. Zettlemoyer, and D. Fox, \u201cALFRED:\nA benchmark for interpreting grounded instructions\nfor everyday tasks,\u201d in CVPR . Computer Vision\nFoundation / IEEE, 2020, pp. 10 737\u201310 746.\n[609] M. Shridhar, X. Yuan, M. C \u02c6ot\u00b4e, Y. Bisk, A. Trischler,\nand M. J. Hausknecht, \u201cAlfworld: Aligning text and\nembodied environments for interactive learning,\u201d in\n9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net, 2021.\n[610] S. Yao, H. Chen, J. Yang, and K. Narasimhan, \u201cWeb-\nshop: Towards scalable real-world web interaction\nwith grounded language agents,\u201d in NeurIPS , 2022.\n[611] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang,\nH. Sun, and Y. Su, \u201cMind2web: Towards a generalist\nagent for the web,\u201d CoRR , vol. abs/2306.06070, 2023.\n[612] W. H. Guss, B. Houghton, N. Topin, P . Wang, C. Codel,\nM. Veloso, and R. Salakhutdinov, \u201cMinerl: A large-\nscale dataset of minecraft demonstrations,\u201d in Proceed-\nings of the Twenty-Eighth International Joint Conference\non Artificial Intelligence, IJCAI 2019, Macao, China, Au-\ngust 10-16, 2019 , S. Kraus, Ed. ijcai.org, 2019, pp.\n2442\u20132448.\n[613] L. Fan, G. Wang, Y. Jiang, A. Mandlekar, Y. Yang,\nH. Zhu, A. Tang, D. Huang, Y. Zhu, and A. Anand-\nkumar, \u201cMinedojo: Building open-ended embodied\nagents with internet-scale knowledge,\u201d in NeurIPS ,\n2022.\n[614] P . Lu, L. Qiu, K. Chang, Y. N. Wu, S. Zhu, T. Ra-\njpurohit, P . Clark, and A. Kalyan, \u201cDynamic prompt\nlearning via policy gradient for semi-structured math-\nematical reasoning,\u201d CoRR , vol. abs/2209.14610, 2022.\n[615] B. Zhang, K. Zhou, X. Wei, W. X. Zhao, J. Sha, S. Wang,\nand J. rong Wen, \u201cEvaluating and improving tool-\naugmented computation-intensive math reasoning,\u201d\nCoRR , vol. abs/2306.02408, 2023.\n[616] R. Yang, L. Song, Y. Li, S. Zhao, Y. Ge, X. Li,\nand Y. Shan, \u201cGpt4tools: Teaching large language\nmodel to use tools via self-instruction,\u201d CoRR , vol.\nabs/2305.18752, 2023.\n[617] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, \u201cGo-\nrilla: Large language model connected with massive\napis,\u201d CoRR , vol. abs/2305.15334, 2023.\n[618] W. Yih, M. Richardson, C. Meek, M. Chang, and J. Suh,\n\u201cThe value of semantic parse labeling for knowledge\nbase question answering,\u201d in Proceedings of the 54th\nAnnual Meeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Germany,\nVolume 2: Short Papers . The Association for Computer\nLinguistics, 2016.112\n[619] H. Puerto, G. G. Sahin, and I. Gurevych, \u201cMetaqa:\nCombining expert agents for multi-skill question an-\nswering,\u201d in Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computational\nLinguistics, EACL 2023, Dubrovnik, Croatia, May 2-6,\n2023 , A. Vlachos and I. Augenstein, Eds. Association\nfor Computational Linguistics, 2023, pp. 3548\u20133562.\n[620] P . Pasupat and P . Liang, \u201cCompositional semantic\nparsing on semi-structured tables,\u201d in Proceedings of\nthe 53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint Con-\nference on Natural Language Processing of the Asian Fed-\neration of Natural Language Processing, ACL 2015, July\n26-31, 2015, Beijing, China, Volume 1: Long Papers . The\nAssociation for Computer Linguistics, 2015, pp. 1470\u2013\n1480.\n[621] V . Zhong, C. Xiong, and R. Socher, \u201cSeq2sql: Gener-\nating structured queries from natural language using\nreinforcement learning,\u201d CoRR , vol. abs/1709.00103,\n2017.\n[622] W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li,\nX. Zhou, and W. Y. Wang, \u201cTabfact: A large-scale\ndataset for table-based fact verification,\u201d in 8th In-\nternational Conference on Learning Representations, ICLR\n2020, Addis Ababa, Ethiopia, April 26-30, 2020 . Open-\nReview.net, 2020.\n[623] T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang,\nZ. Li, J. Ma, I. Li, Q. Yao, S. Roman, Z. Zhang, and\nD. R. Radev, \u201cSpider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-sql task,\u201d in Proceedings of the 2018\nConference on Empirical Methods in Natural Language\nProcessing, Brussels, Belgium, October 31 - November 4,\n2018 , E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii,\nEds. Association for Computational Linguistics, 2018,\npp. 3911\u20133921.\n[624] D. Bahdanau, K. Cho, and Y. Bengio, \u201cNeural machine\ntranslation by jointly learning to align and translate,\u201d\ninICLR , 2015.\n[625] K. Papineni, S. Roukos, T. Ward, and W. Zhu, \u201cBleu:\na method for automatic evaluation of machine trans-\nlation,\u201d in Proceedings of the 40th Annual Meeting of\nthe Association for Computational Linguistics, July 6-12,\n2002, Philadelphia, P A, USA . ACL, 2002, pp. 311\u2013318.\n[626] C.-Y. Lin, \u201cROUGE: A package for automatic evalu-\nation of summaries,\u201d in Text Summarization Branches\nOut. Association for Computational Linguistics, Jul.\n2004, pp. 74\u201381.\n[627] W. Jiao, W. Wang, J.-t. Huang, X. Wang, and Z. Tu, \u201cIs\nchatgpt a good translator? a preliminary study,\u201d arXiv\npreprint arXiv:2301.08745 , 2023.\n[628] T. Zhang, F. Ladhak, E. Durmus, P . Liang, K. R.\nMcKeown, and T. B. Hashimoto, \u201cBenchmarking large\nlanguage models for news summarization,\u201d CoRR ,\nvol. abs/2301.13848, 2023.\n[629] T. Goyal, J. J. Li, and G. Durrett, \u201cNews summariza-\ntion and evaluation in the era of GPT-3,\u201d CoRR , vol.\nabs/2209.12356, 2022.\n[630] S. Gehrmann, E. Clark, and T. Sellam, \u201cRepairing\nthe cracked foundation: A survey of obstacles in\nevaluation practices for generated text,\u201d CoRR , vol.abs/2202.06935, 2022.\n[631] J. Wang, Y. Liang, F. Meng, H. Shi, Z. Li, J. Xu, J. Qu,\nand J. Zhou, \u201cIs chatgpt a good NLG evaluator? A\npreliminary study,\u201d CoRR , vol. abs/2303.04048, 2023.\n[632] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, and C. Zhu, \u201cG-\neval: NLG evaluation using GPT-4 with better human\nalignment,\u201d CoRR , vol. abs/2303.16634, 2023.\n[633] K. Yang, Y. Tian, N. Peng, and D. Klein, \u201cRe3: Gen-\nerating longer stories with recursive reprompting and\nrevision,\u201d in Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December 7-11,\n2022 , Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds.\nAssociation for Computational Linguistics, 2022, pp.\n4393\u20134479.\n[634] W. Zhou, Y. E. Jiang, P . Cui, T. Wang, Z. Xiao, Y. Hou,\nR. Cotterell, and M. Sachan, \u201cRecurrentgpt: Interac-\ntive generation of (arbitrarily) long text,\u201d CoRR , vol.\nabs/2305.13304, 2023.\n[635] S. Gulwani, O. Polozov, and R. Singh, \u201cProgram syn-\nthesis,\u201d Found. Trends Program. Lang. , vol. 4, no. 1-2,\npp. 1\u2013119, 2017.\n[636] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum,\nand C. Gan, \u201cPlanning with large language models for\ncode generation,\u201d 2023.\n[637] M. Welsh, \u201cThe end of programming,\u201d Commun. ACM ,\nvol. 66, no. 1, pp. 34\u201335, 2023.\n[638] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su,\nB. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung, Q. V . Do,\nY. Xu, and P . Fung, \u201cA multitask, multilingual, mul-\ntimodal evaluation of chatgpt on reasoning, halluci-\nnation, and interactivity,\u201d CoRR , vol. abs/2302.04023,\n2023.\n[639] Y. Liu, A. R. Fabbri, P . Liu, Y. Zhao, L. Nan, R. Han,\nS. Han, S. R. Joty, C. Wu, C. Xiong, and D. Radev, \u201cRe-\nvisiting the gold standard: Grounding summarization\nevaluation with robust human evaluation,\u201d CoRR , vol.\nabs/2212.07981, 2022.\n[640] A. R. Fabbri, W. Kryscinski, B. McCann, C. Xiong,\nR. Socher, and D. R. Radev, \u201cSummeval: Re-evaluating\nsummarization evaluation,\u201d Trans. Assoc. Comput. Lin-\nguistics , vol. 9, pp. 391\u2013409, 2021.\n[641] T. Tang, H. Lu, Y. E. Jiang, H. Huang, D. Zhang,\nW. X. Zhao, and F. Wei, \u201cNot all metrics are guilty:\nImproving NLG evaluation with LLM paraphrasing,\u201d\nCoRR , vol. abs/2305.15067, 2023.\n[642] X. Wang, X. Tang, W. X. Zhao, J. Wang, and J. Wen,\n\u201cRethinking the evaluation for conversational rec-\nommendation in the era of large language models,\u201d\nCoRR , vol. abs/2305.13112, 2023.\n[643] M. Gao, J. Ruan, R. Sun, X. Yin, S. Yang, and X. Wan,\n\u201cHuman-like summarization evaluation with chat-\ngpt,\u201d CoRR , vol. abs/2304.02554, 2023.\n[644] Y. Ji, Y. Gong, Y. Peng, C. Ni, P . Sun, D. Pan, B. Ma,\nand X. Li, \u201cExploring chatgpt\u2019s ability to rank con-\ntent: A preliminary study on consistency with human\npreferences,\u201d CoRR , vol. abs/2303.07610, 2023.\n[645] Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu,\nK. Zeng, Y. Xiao, H. Lyu, J. Zhang, J. Li, and L. Hou,\n\u201cBenchmarking foundation models with language-\nmodel-as-an-examiner,\u201d CoRR , vol. abs/2306.04181,113\n2023.\n[646] Y. Liu, S. Feng, D. Wang, Y. Zhang, and H. Sch \u00a8utze,\n\u201cEvaluate what you can\u2019t evaluate: Unassessable gen-\nerated responses quality,\u201d CoRR , vol. abs/2305.14658,\n2023.\n[647] P . Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu,\nT. Liu, and Z. Sui, \u201cLarge language models are not fair\nevaluators,\u201d CoRR , vol. abs/2305.17926, 2023.\n[648] J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui,\nZ. Zhou, C. Gong, Y. Shen, J. Zhou, S. Chen, T. Gui,\nQ. Zhang, and X. Huang, \u201cA comprehensive capabil-\nity analysis of gpt-3 and gpt-3.5 series models,\u201d arXiv\npreprint arXiv:2303.10420 , 2023.\n[649] M. McCloskey and N. J. Cohen, \u201cCatastrophic interfer-\nence in connectionist networks: The sequential learn-\ning problem,\u201d in Psychology of learning and motivation ,\n1989, pp. 109\u2013165.\n[650] R. Kemker, M. McClure, A. Abitino, T. L. Hayes,\nand C. Kanan, \u201cMeasuring catastrophic forgetting in\nneural networks,\u201d in Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-18),\nthe 30th innovative Applications of Artificial Intelligence\n(IAAI-18), and the 8th AAAI Symposium on Educational\nAdvances in Artificial Intelligence (EAAI-18), New Or-\nleans, Louisiana, USA, February 2-7, 2018 , 2018, pp.\n3390\u20133398.\n[651] T. Xie, C. H. Wu, P . Shi, R. Zhong, T. Scholak, M. Ya-\nsunaga, C. Wu, M. Zhong, P . Yin, S. I. Wang, V . Zhong,\nB. Wang, C. Li, C. Boyle, A. Ni, Z. Yao, D. Radev,\nC. Xiong, L. Kong, R. Zhang, N. A. Smith, L. Zettle-\nmoyer, and T. Yu, \u201cUnifiedskg: Unifying and multi-\ntasking structured knowledge grounding with text-to-\ntext language models,\u201d in EMNLP . Association for\nComputational Linguistics, 2022, pp. 602\u2013631.\n[652] A. Roberts, C. Raffel, and N. Shazeer, \u201cHow much\nknowledge can you pack into the parameters of a\nlanguage model?\u201d in Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing,\nEMNLP 2020, Online, November 16-20, 2020 , 2020, pp.\n5418\u20135426.\n[653] G. Izacard, P . S. H. Lewis, M. Lomeli, L. Hos-\nseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin,\nS. Riedel, and E. Grave, \u201cFew-shot learning with\nretrieval augmented language models,\u201d CoRR , vol.\nabs/2208.03299, 2022.\n[654] K. Guu, K. Lee, Z. Tung, P . Pasupat, and M. Chang,\n\u201cRetrieval augmented language model pre-training,\u201d\ninProceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual\nEvent , 2020, pp. 3929\u20133938.\n[655] P . S. H. Lewis, E. Perez, A. Piktus, F. Petroni,\nV . Karpukhin, N. Goyal, H. K \u00a8uttler, M. Lewis, W. Yih,\nT. Rockt \u00a8aschel, S. Riedel, and D. Kiela, \u201cRetrieval-\naugmented generation for knowledge-intensive NLP\ntasks,\u201d in Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual , 2020.\n[656] Y. Lan, G. He, J. Jiang, J. Jiang, W. X. Zhao, and J. Wen,\n\u201cComplex knowledge base question answering: A\nsurvey,\u201d CoRR , vol. abs/2108.06688, 2021.[657] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai,\nE. Rutherford, K. Millican, G. van den Driessche,\nJ. Lespiau, B. Damoc, A. Clark, D. de Las Casas,\nA. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang,\nL. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Pa-\nganini, G. Irving, O. Vinyals, S. Osindero, K. Si-\nmonyan, J. W. Rae, E. Elsen, and L. Sifre, \u201cImprov-\ning language models by retrieving from trillions of\ntokens,\u201d in International Conference on Machine Learn-\ning, ICML 2022, 17-23 July 2022, Baltimore, Maryland,\nUSA , ser. Proceedings of Machine Learning Research,\nK. Chaudhuri, S. Jegelka, L. Song, C. Szepesv \u00b4ari,\nG. Niu, and S. Sabato, Eds., vol. 162. PMLR, 2022,\npp. 2206\u20132240.\n[658] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua,\n\u201cSearch-in-the-chain: Towards accurate, credible and\ntraceable large language models for knowledge-\nintensive tasks,\u201d CoRR , vol. abs/2304.14732, 2023.\n[659] B. Peng, M. Galley, P . He, H. Cheng, Y. Xie, Y. Hu,\nQ. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao,\n\u201cCheck your facts and try again: Improving large\nlanguage models with external knowledge and auto-\nmated feedback,\u201d CoRR , vol. abs/2302.12813, 2023.\n[660] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-\nYu, Y. Yang, J. Callan, and G. Neubig, \u201cActive retrieval\naugmented generation,\u201d CoRR , vol. abs/2305.06983,\n2023.\n[661] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang,\nQ. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, \u201cA sur-\nvey on hallucination in large language models: Prin-\nciples, taxonomy, challenges, and open questions,\u201d\nCoRR , vol. abs/2311.05232, 2023.\n[662] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and\nJ. Wen, \u201cEvaluating object hallucination in large\nvision-language models,\u201d CoRR , vol. abs/2305.10355,\n2023.\n[663] S. Kadavath, T. Conerly, A. Askell, T. J. Henighan,\nD. Drain, E. Perez, N. Schiefer, Z. Dodds, N. Das-\nSarma, E. Tran-Johnson, S. Johnston, S. El-Showk,\nA. Jones, N. Elhage, T. Hume, A. Chen, Y. Bai, S. Bow-\nman, S. Fort, D. Ganguli, D. Hernandez, J. Jacobson,\nJ. Kernion, S. Kravec, L. Lovitt, K. Ndousse, C. Olsson,\nS. Ringer, D. Amodei, T. B. Brown, J. Clark, N. Joseph,\nB. Mann, S. McCandlish, C. Olah, and J. Kaplan,\n\u201cLanguage models (mostly) know what they know,\u201d\nCoRR , vol. abs/2207.05221, 2022.\n[664] P . Manakul, A. Liusie, and M. J. F. Gales, \u201cSelfcheck-\ngpt: Zero-resource black-box hallucination detection\nfor generative large language models,\u201d ArXiv , vol.\nabs/2305.06983, 2023.\n[665] S. Agarwal, I. Akkaya, V . Balcom, M. Bavarian,\nG. Bernadett-Shapiro, G. Brockman, M. Brundage,\nJ. Chan, F. Chantzis, N. Deutsch, B. Eastman, A. Eleti,\nN. Felix, S. P . Fishman, I. Fulford, C. Gibson, J. Gross,\nM. Heaton, J. Hilton, X. Hu, S. Jain, H. Jin, L. Kil-\npatrick, C. Kim, M. Kolhede, A. Mayne, P . McMil-\nlan, D. Medina, J. Menick, A. Mishchenko, A. Nair,\nR. Nayak, A. Neelakantan, R. Nuttall, J. Parish,\nA. T. Passos, A. Perelman, F. de Avila Belbute Peres,\nV . Pong, J. Schulman, E. Sigler, N. Staudacher, N. Tur-\nley, J. Tworek, R. Greene, A. Vijayvergiya, C. Voss,114\nJ. Weng, M. Wiethoff, S. Yoo, K. Yu, W. Zaremba,\nS. Zhao, W. Zhuk, and B. Zoph, \u201cChatgpt plugins,\u201d\nOpenAI Blog , March 2023.\n[666] A. Lazaridou, E. Gribovskaya, W. Stokowiec, and\nN. Grigorev, \u201cInternet-augmented language models\nthrough few-shot prompting for open-domain ques-\ntion answering,\u201d CoRR , vol. abs/2203.05115, 2022.\n[667] H. Qian, Y. Zhu, Z. Dou, H. Gu, X. Zhang, Z. Liu,\nR. Lai, Z. Cao, J. Nie, and J. Wen, \u201cWebbrain: Learn-\ning to generate factually correct articles for queries\nby grounding on large web corpus,\u201d CoRR , vol.\nabs/2304.04358, 2023.\n[668] J. Liu, J. Jin, Z. Wang, J. Cheng, Z. Dou, and J. Wen,\n\u201cRETA-LLM: A retrieval-augmented large language\nmodel toolkit,\u201d CoRR , vol. abs/2306.05212, 2023.\n[669] D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei,\n\u201cKnowledge neurons in pretrained transformers,\u201d in\nProceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\nACL 2022, Dublin, Ireland, May 22-27, 2022 , S. Muresan,\nP . Nakov, and A. Villavicencio, Eds. Association for\nComputational Linguistics, 2022, pp. 8493\u20138502.\n[670] K. Meng, D. Bau, A. J. Andonian, and Y. Belinkov,\n\u201cLocating and editing factual associations in gpt,\u201d in\nAdvances in Neural Information Processing Systems , 2022.\n[671] M. Geva, R. Schuster, J. Berant, and O. Levy, \u201cTrans-\nformer feed-forward layers are key-value memories,\u201d\ninProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP 2021,\nVirtual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021 , M. Moens, X. Huang, L. Specia, and\nS. W. Yih, Eds. Association for Computational Lin-\nguistics, 2021, pp. 5484\u20135495.\n[672] Y. Yao, P . Wang, B. Tian, S. Cheng, Z. Li, S. Deng,\nH. Chen, and N. Zhang, \u201cEditing large language mod-\nels: Problems, methods, and opportunities,\u201d CoRR ,\nvol. abs/2305.13172, 2023.\n[673] P . Wang, N. Zhang, X. Xie, Y. Yao, B. Tian,\nM. Wang, Z. Xi, S. Cheng, K. Liu, G. Zheng, and\nH. Chen, \u201cEasyedit: An easy-to-use knowledge edit-\ning framework for large language models,\u201d CoRR , vol.\nabs/2308.07269, 2023.\n[674] Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and\nW. Chen, \u201cSynthetic prompting: Generating chain-of-\nthought demonstrations for large language models,\u201d\nCoRR , vol. abs/2302.00618, 2023.\n[675] Sifatkaur, M. Singh, V . S. B, and N. Malviya, \u201cMind\nmeets machine: Unravelling gpt-4\u2019s cognitive psychol-\nogy,\u201d CoRR , vol. abs/2303.11436, 2023.\n[676] M. I. Nye, A. J. Andreassen, G. Gur-Ari,\nH. Michalewski, J. Austin, D. Bieber, D. Dohan,\nA. Lewkowycz, M. Bosma, D. Luan, C. Sutton,\nand A. Odena, \u201cShow your work: Scratchpads for\nintermediate computation with language models,\u201d\nCoRR , vol. abs/2112.00114, 2021.\n[677] J. Qian, H. Wang, Z. Li, S. Li, and X. Yan, \u201cLimita-\ntions of language models in arithmetic and symbolic\ninduction,\u201d CoRR , vol. abs/2208.05051, 2022.\n[678] W. X. Zhao, K. Zhou, Z. Gong, B. Zhang, Y. Zhou,\nJ. Sha, Z. Chen, S. Wang, C. Liu, and J. Wen, \u201cJiuzhang:\nA chinese pre-trained language model for mathemat-ical problem understanding,\u201d in KDD \u201922: The 28th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, Washington, DC, USA, August 14 - 18,\n2022 , A. Zhang and H. Rangwala, Eds. ACM, 2022,\npp. 4571\u20134581.\n[679] Q. Wang, C. Kaliszyk, and J. Urban, \u201cFirst experi-\nments with neural translation of informal to formal\nmathematics,\u201d in Intelligent Computer Mathematics -\n11th International Conference, CICM 2018, Hagenberg,\nAustria, August 13-17, 2018, Proceedings , ser. Lecture\nNotes in Computer Science, F. Rabe, W. M. Farmer,\nG. O. Passmore, and A. Youssef, Eds., vol. 11006.\nSpringer, 2018, pp. 255\u2013270.\n[680] S. Polu and I. Sutskever, \u201cGenerative language mod-\neling for automated theorem proving,\u201d CoRR , vol.\nabs/2009.03393, 2020.\n[681] A. Q. Jiang, W. Li, S. Tworkowski, K. Czechowski,\nT. Odrzyg \u00b4ozdz, P . Milos, Y. Wu, and M. Jamnik,\n\u201cThor: Wielding hammers to integrate language mod-\nels and automated theorem provers,\u201d CoRR , vol.\nabs/2205.10893, 2022.\n[682] S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin,\nand I. Sutskever, \u201cFormal mathematics statement cur-\nriculum learning,\u201d CoRR , vol. abs/2202.01344, 2022.\n[683] Y. Wu, A. Q. Jiang, W. Li, M. N. Rabe, C. Staats,\nM. Jamnik, and C. Szegedy, \u201cAutoformalization with\nlarge language models,\u201d CoRR , vol. abs/2205.12615,\n2022.\n[684] A. Q. Jiang, S. Welleck, J. P . Zhou, W. Li, J. Liu,\nM. Jamnik, T. Lacroix, Y. Wu, and G. Lample, \u201cDraft,\nsketch, and prove: Guiding formal theorem provers\nwith informal proofs,\u201d CoRR , vol. abs/2210.12283,\n2022.\n[685] A. Madaan, N. Tandon, P . Gupta, S. Hallinan, L. Gao,\nS. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\nY. Yang, S. Welleck, B. P . Majumder, S. Gupta, A. Yaz-\ndanbakhsh, and P . Clark, \u201cSelf-refine: Iterative refine-\nment with self-feedback,\u201d CoRR , vol. abs/2303.17651,\n2023.\n[686] N. Shinn, B. Labash, and A. Gopinath, \u201cReflexion: an\nautonomous agent with dynamic memory and self-\nreflection,\u201d CoRR , vol. abs/2303.11366, 2023.\n[687] Z. Gou, Z. Shao, Y. Gong, Y. Shen, Y. Yang, N. Duan,\nand W. Chen, \u201cCRITIC: large language models can\nself-correct with tool-interactive critiquing,\u201d CoRR ,\nvol. abs/2305.11738, 2023.\n[688] J. Uesato, N. Kushman, R. Kumar, H. F. Song, N. Y.\nSiegel, L. Wang, A. Creswell, G. Irving, and I. Hig-\ngins, \u201cSolving math word problems with process- and\noutcome-based feedback,\u201d CoRR , vol. abs/2211.14275,\n2022.\n[689] H. Lightman, V . Kosaraju, Y. Burda, H. Edwards,\nB. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever,\nand K. Cobbe, \u201cLet\u2019s verify step by step,\u201d CoRR , vol.\nabs/2305.20050, 2023.\n[690] Z. Yuan, H. Yuan, C. Tan, W. Wang, and S. Huang,\n\u201cHow well do large language models perform in\narithmetic tasks?\u201d CoRR , vol. abs/2304.02015, 2023.\n[691] X. Pi, Q. Liu, B. Chen, M. Ziyadi, Z. Lin, Q. Fu, Y. Gao,\nJ. Lou, and W. Chen, \u201cReasoning like program execu-\ntors,\u201d in Proceedings of the 2022 Conference on Empirical115\nMethods in Natural Language Processing, EMNLP 2022,\nAbu Dhabi, United Arab Emirates, December 7-11, 2022 ,\n2022, pp. 761\u2013779.\n[692] H. Zhou, A. Nova, H. Larochelle, A. C. Courville,\nB. Neyshabur, and H. Sedghi, \u201cTeaching algorith-\nmic reasoning via in-context learning,\u201d CoRR , vol.\nabs/2211.09066, 2022.\n[693] A. Parisi, Y. Zhao, and N. Fiedel, \u201cTALM:\ntool augmented language models,\u201d CoRR , vol.\nabs/2205.12255, 2022.\n[694] W. Huang, P . Abbeel, D. Pathak, and I. Mordatch,\n\u201cLanguage models as zero-shot planners: Extracting\nactionable knowledge for embodied agents,\u201d in ICML ,\nser. Proceedings of Machine Learning Research, vol.\n162. PMLR, 2022, pp. 9118\u20139147.\n[695] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud,\nand P . Oudeyer, \u201cGrounding large language models\nin interactive environments with online reinforcement\nlearning,\u201d CoRR , vol. abs/2302.02662, 2023.\n[696] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang,\nG. Huang, B. Li, L. Lu, X. Wang, Y. Qiao, Z. Zhang,\nand J. Dai, \u201cGhost in the minecraft: Generally capable\nagents for open-world environments via large lan-\nguage models with text-based knowledge and mem-\nory,\u201d CoRR , vol. abs/2305.17144, 2023.\n[697] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao,\nY. Zhu, L. Fan, and A. Anandkumar, \u201cVoyager: An\nopen-ended embodied agent with large language\nmodels,\u201d CoRR , vol. abs/2305.16291, 2023.\n[698] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes,\nB. David, C. Finn, K. Gopalakrishnan, K. Hausman,\nA. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan,\nE. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. J. Joshi,\nR. Julian, D. Kalashnikov, Y. Kuang, K. Lee, S. Levine,\nY. Lu, L. Luu, C. Parada, P . Pastor, J. Quiambao,\nK. Rao, J. Rettinghouse, D. Reyes, P . Sermanet, N. Siev-\ners, C. Tan, A. Toshev, V . Vanhoucke, F. Xia, T. Xiao,\nP . Xu, S. Xu, and M. Yan, \u201cDo as I can, not as I say:\nGrounding language in robotic affordances,\u201d CoRR ,\nvol. abs/2204.01691, 2022.\n[699] J. Liang, W. Huang, F. Xia, P . Xu, K. Hausman,\nB. Ichter, P . Florence, and A. Zeng, \u201cCode as policies:\nLanguage model programs for embodied control,\u201d\nCoRR , vol. abs/2209.07753, 2022.\n[700] Y. Fu, H. Peng, T. Khot, and M. Lapata, \u201cImprov-\ning language model negotiation with self-play and\nin-context learning from AI feedback,\u201d CoRR , vol.\nabs/2305.10142, 2023.\n[701] N. Mehta, M. Teruel, P . F. Sanz, X. Deng, A. H.\nAwadallah, and J. Kiseleva, \u201cImproving grounded\nlanguage understanding in a collaborative environ-\nment by interacting with agents through help feed-\nback,\u201d CoRR , vol. abs/2304.10750, 2023.\n[702] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, \u201cGo-\nrilla: Large language model connected with massive\napis,\u201d CoRR , vol. abs/2305.15334, 2023.\n[703] S. Hao, T. Liu, Z. Wang, and Z. Hu, \u201cToolkengpt: Aug-\nmenting frozen language models with massive tools\nvia tool embeddings,\u201d CoRR , vol. abs/2305.11554,\n2023.\n[704] Y. Liang, C. Wu, T. Song, W. Wu, Y. Xia, Y. Liu, Y. Ou,S. Lu, L. Ji, S. Mao, Y. Wang, L. Shou, M. Gong,\nand N. Duan, \u201cTaskmatrix.ai: Completing tasks by\nconnecting foundation models with millions of apis,\u201d\nCoRR , vol. abs/2303.16434, 2023.\n[705] T. Cai, X. Wang, T. Ma, X. Chen, and D. Zhou,\n\u201cLarge language models as tool makers,\u201d CoRR , vol.\nabs/2305.17126, 2023.\n[706] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and\nJ. Han, \u201cLarge language models can self-improve,\u201d\nCoRR , vol. abs/2210.11610, 2022.\n[707] E. Beeching, C. Fourrier, N. Habib, S. Han, N. Lam-\nbert, N. Rajani, O. Sanseviero, L. Tunstall, and T. Wolf,\n\u201cOpen llm leaderboard,\u201d https://huggingface.co/\nspaces/HuggingFaceH4/open llm leaderboard,\n2023.\n[708] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang,\nA. Saied, W. Chen, and N. Duan, \u201cAgieval: A human-\ncentric benchmark for evaluating foundation models,\u201d\nCoRR , vol. abs/2304.06364, 2023.\n[709] H. Zeng, \u201cMeasuring massive multitask chinese un-\nderstanding,\u201d CoRR , vol. abs/2304.12986, 2023.\n[710] C. Liu, R. Jin, Y. Ren, L. Yu, T. Dong, X. Peng,\nS. Zhang, J. Peng, P . Zhang, Q. Lyu, X. Su, Q. Liu,\nand D. Xiong, \u201cM3KE: A massive multi-level multi-\nsubject knowledge evaluation benchmark for chinese\nlarge language models,\u201d CoRR , vol. abs/2305.10263,\n2023.\n[711] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su,\nJ. Liu, C. Lv, Y. Zhang, J. Lei, Y. Fu, M. Sun, and\nJ. He, \u201cC-eval: A multi-level multi-discipline chinese\nevaluation suite for foundation models,\u201d CoRR , vol.\nabs/2305.08322, 2023.\n[712] Z. Gu, X. Zhu, H. Ye, L. Zhang, J. Wang, S. Jiang,\nZ. Xiong, Z. Li, Q. He, R. Xu, W. Huang, W. Zheng,\nH. Feng, and Y. Xiao, \u201cXiezhi: An ever-updating\nbenchmark for holistic domain knowledge evalua-\ntion,\u201d CoRR , vol. abs/2306.05783, 2023.\n[713] O. Contributors, \u201cOpencompass: A universal evalua-\ntion platform for foundation models,\u201d https://github.\ncom/InternLM/OpenCompass, 2023.\n[714] Y. Fu, L. Ou, M. Chen, Y. Wan, H. Peng, and T. Khot,\n\u201cChain-of-thought hub: A continuous effort to mea-\nsure large language models\u2019 reasoning performance,\u201d\nCoRR , vol. abs/2305.17306, 2023.\n[715] J. Yu, X. Wang, S. Tu, S. Cao, D. Zhang-li, X. Lv,\nH. Peng, Z. Yao, X. Zhang, H. Li, C. Li, Z. Zhang,\nY. Bai, Y. Liu, A. Xin, N. Lin, K. Yun, L. Gong, J. Chen,\nZ. Wu, Y. Qi, W. Li, Y. Guan, K. Zeng, J. Qi, H. Jin,\nJ. Liu, Y. Gu, Y. Yao, N. Ding, L. Hou, Z. Liu, B. Xu,\nJ. Tang, and J. Li, \u201cKola: Carefully benchmarking\nworld knowledge of large language models,\u201d CoRR ,\nvol. abs/2306.09296, 2023.\n[716] T. Sawada, D. Paleka, A. Havrilla, P . Tadepalli, P . Vi-\ndas, A. Kranias, J. J. Nay, K. Gupta, and A. Ko-\nmatsuzaki, \u201cARB: advanced reasoning benchmark for\nlarge language models,\u201d CoRR , vol. abs/2307.13692,\n2023.\n[717] Y. Peng, S. Li, W. Gu, Y. Li, W. Wang, C. Gao, and\nM. R. Lyu, \u201cRevisiting, benchmarking and exploring\nAPI recommendation: How far are we?\u201d IEEE Trans.\nSoftware Eng. , vol. 49, no. 4, pp. 1876\u20131897, 2023.116\n[718] M. Li, F. Song, B. Yu, H. Yu, Z. Li, F. Huang, and Y. Li,\n\u201cApi-bank: A benchmark for tool-augmented llms,\u201d\nCoRR , vol. abs/2304.08244, 2023.\n[719] Q. Tang, Z. Deng, H. Lin, X. Han, Q. Liang, and\nL. Sun, \u201cToolalpaca: Generalized tool learning for\nlanguage models with 3000 simulated cases,\u201d CoRR ,\nvol. abs/2306.05301, 2023.\n[720] Q. Xu, F. Hong, B. Li, C. Hu, Z. Chen, and J. Zhang,\n\u201cOn the tool manipulation capability of open-source\nlarge language models,\u201d CoRR , vol. abs/2305.16504,\n2023.\n[721] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin,\nX. Cong, X. Tang, B. Qian, S. Zhao, R. Tian, R. Xie,\nJ. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun,\n\u201cToolllm: Facilitating large language models to master\n16000+ real-world apis,\u201d CoRR , vol. abs/2307.16789,\n2023.\n[722] Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke,\nR. Murthy, Y. Feng, Z. Chen, J. C. Niebles,\nD. Arpit, R. Xu, P . Mui, H. Wang, C. Xiong, and\nS. Savarese, \u201cBOLAA: benchmarking and orchestrat-\ning llm-augmented autonomous agents,\u201d CoRR , vol.\nabs/2308.05960, 2023.\n[723] X. Liu, H. Yu, H. Zhang, Y. Xu, X. Lei, H. Lai, Y. Gu,\nH. Ding, K. Men, K. Yang, S. Zhang, X. Deng, A. Zeng,\nZ. Du, C. Zhang, S. Shen, T. Zhang, Y. Su, H. Sun,\nM. Huang, Y. Dong, and J. Tang, \u201cAgentbench: Evalu-\nating llms as agents,\u201d CoRR , vol. abs/2308.03688, 2023.\n[724] K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang,\nL. Yang, W. Ye, N. Z. Gong, Y. Zhang, and X. Xie,\n\u201cPromptbench: Towards evaluating the robustness\nof large language models on adversarial prompts,\u201d\nCoRR , vol. abs/2306.04528, 2023.\n[725] R. S. Shah, K. Chawla, D. Eidnani, A. Shah, W. Du,\nS. Chava, N. Raman, C. Smiley, J. Chen, and D. Yang,\n\u201cWHEN FLUE MEETS FLANG: benchmarks and\nlarge pre-trained language model for financial do-\nmain,\u201d CoRR , vol. abs/2211.00083, 2022.\n[726] N. Guha, D. E. Ho, J. Nyarko, and C. R \u00b4e, \u201cLegalbench:\nPrototyping a collaborative benchmark for legal rea-\nsoning,\u201d CoRR , vol. abs/2209.06120, 2022.\n[727] L. Zheng, W. Chiang, Y. Sheng, S. Zhuang, Z. Wu,\nY. Zhuang, Z. Lin, Z. Li, D. Li, E. P . Xing, H. Zhang,\nJ. E. Gonzalez, and I. Stoica, \u201cJudging llm-as-a-\njudge with mt-bench and chatbot arena,\u201d CoRR , vol.\nabs/2306.05685, 2023.\n[728] X. Wang, Z. Hu, P . Lu, Y. Zhu, J. Zhang, S. Subrama-\nniam, A. R. Loomba, S. Zhang, Y. Sun, and W. Wang,\n\u201cScibench: Evaluating college-level scientific problem-\nsolving abilities of large language models,\u201d CoRR , vol.\nabs/2307.10635, 2023.\n[729] X. Li, T. Zhang, Y. Dubois, R. Taori, I. Gulrajani,\nC. Guestrin, P . Liang, and T. B. Hashimoto, \u201cAlpacae-\nval: An automatic evaluator of instruction-following\nmodels,\u201d https://github.com/tatsu-lab/alpaca eval,\n2023.\n[730] Y. Huang, Q. Zhang, P . S. Yu, and L. Sun, \u201cTrustgpt:\nA benchmark for trustworthy and responsible large\nlanguage models,\u201d CoRR , vol. abs/2306.11507, 2023.\n[731] Y. Bai, J. Ying, Y. Cao, X. Lv, Y. He, X. Wang, J. Yu,\nK. Zeng, Y. Xiao, H. Lyu, J. Zhang, J. Li, and L. Hou,\u201cBenchmarking foundation models with language-\nmodel-as-an-examiner,\u201d CoRR , vol. abs/2306.04181,\n2023.\n[732] C. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang,\nJ. Fu, and Z. Liu, \u201cChateval: Towards better llm-based\nevaluators through multi-agent debate,\u201d CoRR , vol.\nabs/2308.07201, 2023.\n[733] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen,\nL. Yang, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang,\nY. Chang, P . S. Yu, Q. Yang, and X. Xie, \u201cA survey\non evaluation of large language models,\u201d CoRR , vol.\nabs/2307.03109, 2023.\n[734] Z. Zhuang, Q. Chen, L. Ma, M. Li, Y. Han, Y. Qian,\nH. Bai, Z. Feng, W. Zhang, and T. Liu, \u201cThrough the\nlens of core competency: Survey on evaluation of large\nlanguage models,\u201d CoRR , vol. abs/2308.07902, 2023.\n[735] J. H. Clark, J. Palomaki, V . Nikolaev, E. Choi, D. Gar-\nrette, M. Collins, and T. Kwiatkowski, \u201cTydi QA: A\nbenchmark for information-seeking question answer-\ning in typologically diverse languages,\u201d Trans. Assoc.\nComput. Linguistics , vol. 8, pp. 454\u2013470, 2020.\n[736] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Fos-\nter, L. Golding, J. Hsu, K. McDonell, N. Muennighoff,\nJ. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang,\nK. Wang, and A. Zou, \u201cA framework for few-shot\nlanguage model evaluation,\u201d Sep. 2021.\n[737] R. Shah, K. Chawla, D. Eidnani, A. Shah, W. Du,\nS. Chava, N. Raman, C. Smiley, J. Chen, and D. Yang,\n\u201cWhen flue meets flang: Benchmarks and large pre-\ntrained language model for financial domain,\u201d in Pro-\nceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing , 2022, pp. 2322\u20132335.\n[738] K. Zhou, Y. Zhu, Z. Chen, W. Chen, W. X. Zhao,\nX. Chen, Y. Lin, J.-R. Wen, and J. Han, \u201cDon\u2019t make\nyour llm an evaluation benchmark cheater,\u201d arXiv\npreprint arXiv:2311.01964 , 2023.\n[739] C. Zan, K. Peng, L. Ding, B. Qiu, B. Liu, S. He, Q. Lu,\nZ. Zhang, C. Liu, W. Liu, Y. Zhan, and D. Tao, \u201cVega-\nmt: The JD explore academy machine translation sys-\ntem for WMT22,\u201d in Proceedings of the Seventh Con-\nference on Machine Translation, WMT 2022, Abu Dhabi,\nUnited Arab Emirates (Hybrid), December 7-8, 2022 ,\nP . Koehn, L. Barrault, O. Bojar, F. Bougares, R. Chat-\nterjee, M. R. Costa-juss `a, C. Federmann, M. Fishel,\nA. Fraser, M. Freitag, Y. Graham, R. Grundkiewicz,\nP . Guzman, B. Haddow, M. Huck, A. Jimeno-Yepes,\nT. Kocmi, A. Martins, M. Morishita, C. Monz, M. Na-\ngata, T. Nakazawa, M. Negri, A. N \u00b4ev\u00b4eol, M. Neves,\nM. Popel, M. Turchi, and M. Zampieri, Eds. Associa-\ntion for Computational Linguistics, 2022, pp. 411\u2013422.\n[740] Y. Zhao, M. Khalman, R. Joshi, S. Narayan, M. Saleh,\nand P . J. Liu, \u201cCalibrating sequence likelihood\nimproves conditional language generation,\u201d CoRR ,\nvol. abs/2210.00045, 2022. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2210.00045\n[741] D. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord,\nP . Clark, and H. Hajishirzi, \u201cUnifiedqa: Crossing for-\nmat boundaries with a single QA system,\u201d in EMNLP\n(Findings) , ser. Findings of ACL, vol. EMNLP 2020.\nAssociation for Computational Linguistics, 2020, pp.\n1896\u20131907.117\n[742] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang,\nand Y. Yang, \u201cSolving math word problem via co-\noperative reasoning induced language models,\u201d arXiv\npreprint arXiv:2210.16257 , 2022.\n[743] A. Nguyen, N. Karampatziakis, and W. Chen, \u201cMeet\nin the middle: A new pre-training paradigm,\u201d\nCoRR , vol. abs/2303.07295, 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2303.07295\n[744] H. Li, J. Zhang, C. Li, and H. Chen, \u201cRESDSQL:\ndecoupling schema linking and skeleton parsing\nfor text-to-sql,\u201d CoRR , vol. abs/2302.05965, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.\n2302.05965\n[745] W. Kang and J. J. McAuley, \u201cSelf-attentive sequential\nrecommendation,\u201d in IEEE International Conference on\nData Mining, ICDM 2018, Singapore, November 17-20,\n2018 . IEEE Computer Society, 2018, pp. 197\u2013206.\n[746] B. Yang, C. Han, Y. Li, L. Zuo, and Z. Yu, \u201cImprov-\ning conversational recommendation systems\u2019 quality\nwith context-aware item meta-information,\u201d in Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, Seattle, WA, United States, July 10-15,\n2022 , M. Carpuat, M. de Marneffe, and I. V . M. Ru \u00b4\u0131z,\nEds. Association for Computational Linguistics, 2022,\npp. 38\u201348.\n[747] E. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cap-\npelli, R. Cojocaru, M. Debbah, E. Goffinet, D. Hes-\nlow, J. Launay, Q. Malartic, B. Noune, B. Pannier,\nand G. Penedo, \u201cFalcon-40B: an open large language\nmodel with state-of-the-art performance,\u201d 2023.\n[748] S. Martin, J. Liermann, and H. Ney, \u201cAlgorithms for\nbigram and trigram word clustering,\u201d Speech commu-\nnication , vol. 24, no. 1, pp. 19\u201337, 1998.\n[749] R. Navigli, \u201cWord sense disambiguation: A survey,\u201d\nACM computing surveys (CSUR) , vol. 41, no. 2, pp. 1\u2013\n69, 2009.\n[750] W. H. Gomaa, A. A. Fahmy et al. , \u201cA survey of text\nsimilarity approaches,\u201d international journal of Com-\nputer Applications , vol. 68, no. 13, pp. 13\u201318, 2013.\n[751] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad,\nM. Chenaghlu, and J. Gao, \u201cDeep learning\u2013based text\nclassification: a comprehensive review,\u201d ACM comput-\ning surveys (CSUR) , vol. 54, no. 3, pp. 1\u201340, 2021.\n[752] N. Alex, E. Lifland, L. Tunstall, A. Thakur, P . Maham,\nC. J. Riedel, E. Hine, C. Ashurst, P . Sedille, A. Carlier,\nM. Noetel, and A. Stuhlm \u00a8uller, \u201cRAFT: A real-world\nfew-shot text classification benchmark,\u201d in NeurIPS\nDatasets and Benchmarks , 2021.\n[753] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga,\nand D. Yang, \u201cIs chatgpt a general-purpose nat-\nural language processing task solver?\u201d CoRR , vol.\nabs/2302.06476, 2023.\n[754] X. Chen, J. Ye, C. Zu, N. Xu, R. Zheng, M. Peng,\nJ. Zhou, T. Gui, Q. Zhang, and X. Huang, \u201cHow robust\nis gpt-3.5 to predecessors? a comprehensive study on\nlanguage understanding tasks,\u201d 2023.\n[755] D. Nadeau and S. Sekine, \u201cA survey of named entity\nrecognition and classification,\u201d Lingvisticae Investiga-\ntiones , vol. 30, no. 1, pp. 3\u201326, 2007.\n[756] A. Ratnaparkhi, \u201cA maximum entropy model for part-\nof-speech tagging,\u201d in Conference on empirical methodsin natural language processing , 1996.\n[757] V . Yadav and S. Bethard, \u201cA survey on recent ad-\nvances in named entity recognition from deep learn-\ning models,\u201d in Proceedings of the 27th International\nConference on Computational Linguistics , 2018, pp. 2145\u2013\n2158.\n[758] F. Souza, R. Nogueira, and R. Lotufo, \u201cPortuguese\nnamed entity recognition using bert-crf,\u201d arXiv\npreprint arXiv:1909.10649 , 2019.\n[759] S. Pawar, G. K. Palshikar, and P . Bhattacharyya,\n\u201cRelation extraction: A survey,\u201d arXiv preprint\narXiv:1712.05191 , 2017.\n[760] C. Walker and et al., \u201cAce 2005 multilingual training\ncorpus ldc2006t06,\u201d Philadelphia, 2006.\n[761] J. Gao, H. Zhao, C. Yu, and R. Xu, \u201cExploring the\nfeasibility of chatgpt for event extraction,\u201d CoRR , vol.\nabs/2303.03836, 2023.\n[762] Y. Ma, Y. Cao, Y. Hong, and A. Sun, \u201cLarge language\nmodel is not a good few-shot information extractor,\nbut a good reranker for hard samples!\u201d CoRR , vol.\nabs/2303.08559, 2023.\n[763] R. Tang, X. Han, X. Jiang, and X. Hu, \u201cDoes synthetic\ndata generation of llms help clinical text mining?\u201d\narXiv preprint arXiv:2303.04360 , 2023.\n[764] A. Vaswani, S. Bengio, E. Brevdo, F. Chollet,\nA. Gomez, S. Gouws, L. Jones, \u0141. Kaiser, N. Kalch-\nbrenner, N. Parmar et al. , \u201cTensor2tensor for neural\nmachine translation,\u201d in Proceedings of the 13th Con-\nference of the Association for Machine Translation in the\nAmericas (Volume 1: Research Track) , 2018, pp. 193\u2013199.\n[765] B. Zhang, B. Haddow, and A. Birch, \u201cPrompting\nlarge language model for machine translation: A case\nstudy,\u201d arXiv preprint arXiv:2301.07069 , 2023.\n[766] M. Ghazvininejad, H. Gonen, and L. Zettlemoyer,\n\u201cDictionary-based phrase-level prompting of large\nlanguage models for machine translation,\u201d arXiv\npreprint arXiv:2302.07856 , 2023.\n[767] L. Wang, C. Lyu, T. Ji, Z. Zhang, D. Yu,\nS. Shi, and Z. Tu, \u201cDocument-level machine trans-\nlation with large language models,\u201d arXiv preprint\narXiv:2304.02210 , 2023.\n[768] W. Jiao, J.-t. Huang, W. Wang, X. Wang, S. Shi, and\nZ. Tu, \u201cParrot: Translating during chat using large lan-\nguage models,\u201d arXiv preprint arXiv:2304.02426 , 2023.\n[769] W. Yang, C. Li, J. Zhang, and C. Zong, \u201cBigtrans:\nAugmenting large language models with multilin-\ngual translation capability over 100 languages,\u201d arXiv\npreprint arXiv:2305.18098 , 2023.\n[770] J. Kocon, I. Cichecki, O. Kaszyca, M. Kochanek,\nD. Szydlo, J. Baran, J. Bielaniewicz, M. Gruza, A. Janz,\nK. Kanclerz, A. Kocon, B. Koptyra, W. Mieleszczenko-\nKowszewicz, P . Milkowski, M. Oleksy, M. Piasecki,\nL. Radlinski, K. Wojtasik, S. Wozniak, and P . Kazienko,\n\u201cChatgpt: Jack of all trades, master of none,\u201d CoRR ,\nvol. abs/2302.10724, 2023.\n[771] Q. Zhong, L. Ding, J. Liu, B. Du, and D. Tao,\n\u201cCan chatgpt understand too? A comparative study\non chatgpt and fine-tuned BERT,\u201d CoRR , vol.\nabs/2302.10198, 2023.\n[772] D. Cheng, S. Huang, J. Bi, Y. Zhan, J. Liu, Y. Wang,\nH. Sun, F. Wei, D. Deng, and Q. Zhang, \u201cUprise:118\nUniversal prompt retrieval for improving zero-shot\nevaluation,\u201d arXiv preprint arXiv:2303.08518 , 2023.\n[773] R. Ren, Y. Qu, J. Liu, W. X. Zhao, Q. She, H. Wu,\nH. Wang, and J.-R. Wen, \u201cRocketqav2: A joint train-\ning method for dense passage retrieval and passage\nre-ranking,\u201d in Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing , 2021,\npp. 2825\u20132835.\n[774] W. Sun, L. Yan, X. Ma, P . Ren, D. Yin, and Z. Ren,\n\u201cIs chatgpt good at search? investigating large lan-\nguage models as re-ranking agent,\u201d arXiv preprint\narXiv:2304.09542 , 2023.\n[775] Z. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, J. Shen,\nT. Liu, J. Liu, D. Metzler, X. Wang et al. , \u201cLarge lan-\nguage models are effective text rankers with pairwise\nranking prompting,\u201d arXiv preprint arXiv:2306.17563 ,\n2023.\n[776] S. Cho, S. Jeong, J. Seo, and J. C. Park, \u201cDiscrete\nprompt optimization via constrained generation for\nzero-shot re-ranker,\u201d arXiv preprint arXiv:2305.13729 ,\n2023.\n[777] R. Tang, X. Zhang, X. Ma, J. Lin, and F. Ture, \u201cFound\nin the middle: Permutation self-consistency improves\nlistwise ranking in large language models,\u201d arXiv\npreprint arXiv:2310.07712 , 2023.\n[778] X. Ma, X. Zhang, R. Pradeep, and J. Lin, \u201cZero-shot\nlistwise document reranking with a large language\nmodel,\u201d arXiv preprint arXiv:2305.02156 , 2023.\n[779] S. Zhuang, H. Zhuang, B. Koopman, and G. Zuccon,\n\u201cA setwise approach for effective and highly efficient\nzero-shot ranking with large language models,\u201d arXiv\npreprint arXiv:2310.09497 , 2023.\n[780] H. Zhuang, Z. Qin, K. Hui, J. Wu, L. Yan, X. Wang, and\nM. Berdersky, \u201cBeyond yes and no: Improving zero-\nshot llm rankers via scoring fine-grained relevance\nlabels,\u201d arXiv preprint arXiv:2310.14122 , 2023.\n[781] N. Ziems, W. Yu, Z. Zhang, and M. Jiang, \u201cLarge\nlanguage models are built-in autoregressive search\nengines,\u201d arXiv preprint arXiv:2305.09612 , 2023.\n[782] X. Ma, L. Wang, N. Yang, F. Wei, and J. Lin, \u201cFine-\ntuning llama for multi-stage text retrieval,\u201d arXiv\npreprint arXiv:2310.08319 , 2023.\n[783] R. Pradeep, S. Sharifymoghaddam, and J. Lin,\n\u201cRankvicuna: Zero-shot listwise document rerank-\ning with open-source large language models,\u201d arXiv\npreprint arXiv:2309.15088 , 2023.\n[784] Y. Tay, V . Q. Tran, M. Dehghani, J. Ni, D. Bahri,\nH. Mehta, Z. Qin, K. Hui, Z. Zhao, J. Gupta et al. ,\n\u201cTransformer memory as a differentiable search in-\ndex,\u201d in Advances in Neural Information Processing Sys-\ntems , 2022.\n[785] R. Ren, W. X. Zhao, J. Liu, H. Wu, J.-R. Wen,\nand H. Wang, \u201cTOME: A two-stage approach for\nmodel-based retrieval,\u201d in Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) . Association\nfor Computational Linguistics, 2023, pp. 6102\u20136114.\n[Online]. Available: https://aclanthology.org/2023.\nacl-long.336\n[786] Y. Qu, Y. Ding, J. Liu, K. Liu, R. Ren, W. X. Zhao,\nD. Dong, H. Wu, and H. Wang, \u201cRocketqa: An op-timized training approach to dense passage retrieval\nfor open-domain question answering,\u201d in Proceedings\nof the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies , 2021, pp. 5835\u20135847.\n[787] R. Ren, S. Lv, Y. Qu, J. Liu, W. X. Zhao, Q. She, H. Wu,\nH. Wang, and J.-R. Wen, \u201cPair: Leveraging passage-\ncentric similarity relation for improving dense pas-\nsage retrieval,\u201d in Findings of the Association for Compu-\ntational Linguistics: ACL-IJCNLP 2021 , 2021, pp. 2173\u2013\n2183.\n[788] Z. Peng, X. Wu, and Y. Fang, \u201cSoft prompt tuning\nfor augmenting dense retrieval with large language\nmodels,\u201d arXiv preprint arXiv:2307.08303 , 2023.\n[789] Z. Dai, V . Y. Zhao, J. Ma, Y. Luan, J. Ni, J. Lu,\nA. Bakalov, K. Guu, K. Hall, and M.-W. Chang,\n\u201cPromptagator: Few-shot dense retrieval from 8 ex-\namples,\u201d in The Eleventh International Conference on\nLearning Representations , 2023.\n[790] A. Askari, M. Aliannejadi, E. Kanoulas, and S. Ver-\nberne, \u201cGenerating synthetic documents for cross-\nencoder re-rankers: A comparative study of chatgpt\nand human experts,\u201d arXiv preprint arXiv:2305.02320 ,\n2023.\n[791] K. Mao, Z. Dou, H. Chen, F. Mo, and H. Qian, \u201cLarge\nlanguage models know your contextual search intent:\nA prompting framework for conversational search,\u201d\narXiv preprint arXiv:2303.06573 , 2023.\n[792] L. Gao, X. Ma, J. Lin, and J. Callan, \u201cPrecise zero-\nshot dense retrieval without relevance labels,\u201d in Pro-\nceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers) .\nAssociation for Computational Linguistics, 2023, pp.\n1762\u20131777.\n[793] L. Wang, N. Yang, and F. Wei, \u201cQuery2doc: Query\nexpansion with large language models,\u201d arXiv preprint\narXiv:2303.07678 , 2023.\n[794] G. Ma, X. Wu, P . Wang, Z. Lin, and S. Hu, \u201cPre-\ntraining with large language model-based document\nexpansion for dense passage retrieval,\u201d arXiv preprint\narXiv:2308.08285 , 2023.\n[795] W. Sun, Z. Chen, X. Ma, L. Yan, S. Wang, P . Ren,\nZ. Chen, D. Yin, and Z. Ren, \u201cInstruction distilla-\ntion makes large language models efficient zero-shot\nrankers,\u201d arXiv preprint arXiv:2311.01555 , 2023.\n[796] X. Wang, W. Zhu, and W. Y. Wang, \u201cLarge language\nmodels are implicitly topic models: Explaining and\nfinding good demonstrations for in-context learning,\u201d\nCoRR , vol. abs/2301.11916, 2023.\n[797] C. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang,\nand J. Gao, \u201cMultimodal foundation models: From\nspecialists to general-purpose assistants,\u201d CoRR , vol.\nabs/2309.10020, 2023.\n[798] W. X. Zhao, S. Mu, Y. Hou, Z. Lin, Y. Chen, X. Pan,\nK. Li, Y. Lu, H. Wang, C. Tian, Y. Min, Z. Feng, X. Fan,\nX. Chen, P . Wang, W. Ji, Y. Li, X. Wang, and J. Wen,\n\u201cRecbole: Towards a unified, comprehensive and ef-\nficient framework for recommendation algorithms,\u201d\ninCIKM , G. Demartini, G. Zuccon, J. S. Culpepper,\nZ. Huang, and H. Tong, Eds. ACM, 2021, pp. 4653\u2013\n4664.119\n[799] K. Zhou, H. Wang, W. X. Zhao, Y. Zhu, S. Wang,\nF. Zhang, Z. Wang, and J. Wen, \u201cS3-rec: Self-\nsupervised learning for sequential recommendation\nwith mutual information maximization,\u201d in CIKM ,\nM. d\u2019Aquin, S. Dietze, C. Hauff, E. Curry, and\nP . Cudr \u00b4e-Mauroux, Eds. ACM, 2020, pp. 1893\u20131902.\n[800] W. X. Zhao, Y. Hou, X. Pan, C. Yang, Z. Zhang, Z. Lin,\nJ. Zhang, S. Bian, J. Tang, W. Sun, Y. Chen, L. Xu,\nG. Zhang, Z. Tian, C. Tian, S. Mu, X. Fan, X. Chen,\nand J. Wen, \u201cRecbole 2.0: Towards a more up-to-date\nrecommendation library,\u201d in CIKM , M. A. Hasan and\nL. Xiong, Eds. ACM, 2022, pp. 4722\u20134726.\n[801] L. Xu, Z. Tian, G. Zhang, J. Zhang, L. Wang, B. Zheng,\nY. Li, J. Tang, Z. Zhang, Y. Hou, X. Pan, W. X. Zhao,\nX. Chen, and J. Wen, \u201cTowards a more user-friendly\nand easy-to-use benchmark library for recommender\nsystems,\u201d in SIGIR , H. Chen, W. E. Duh, H. Huang,\nM. P . Kato, J. Mothe, and B. Poblete, Eds. ACM,\n2023, pp. 2837\u20132847.\n[802] S. Rendle, C. Freudenthaler, Z. Gantner, and\nL. Schmidt-Thieme, \u201cBPR: bayesian personalized\nranking from implicit feedback,\u201d CoRR , vol.\nabs/1205.2618, 2012.\n[803] W. Fan, Z. Zhao, J. Li, Y. Liu, X. Mei, Y. Wang, J. Tang,\nand Q. Li, \u201cRecommender systems in the era of large\nlanguage models (llms),\u201d CoRR , 2023.\n[804] L. Wu, Z. Zheng, Z. Qiu, H. Wang, H. Gu, T. Shen,\nC. Qin, C. Zhu, H. Zhu, Q. Liu, H. Xiong, and E. Chen,\n\u201cA survey on large language models for recommenda-\ntion,\u201d CoRR , 2023.\n[805] Y. Gao, T. Sheng, Y. Xiang, Y. Xiong, H. Wang, and\nJ. Zhang, \u201cChat-rec: Towards interactive and explain-\nable llms-augmented recommender system,\u201d CoRR ,\nvol. abs/2303.14524, 2023.\n[806] S. Dai, N. Shao, H. Zhao, W. Yu, Z. Si, C. Xu, Z. Sun,\nX. Zhang, and J. Xu, \u201cUncovering chatgpt\u2019s capabil-\nities in recommender systems,\u201d in RecSys , J. Zhang,\nL. Chen, S. Berkovsky, M. Zhang, T. D. Noia, J. Basil-\nico, L. Pizzato, and Y. Song, Eds. ACM, 2023, pp.\n1126\u20131132.\n[807] Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. J. McAuley,\nand W. X. Zhao, \u201cLarge language models are zero-shot\nrankers for recommender systems,\u201d CoRR , 2023.\n[808] J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, \u201cIs chatgpt\na good recommender? A preliminary study,\u201d CoRR ,\nvol. abs/2304.10149, 2023.\n[809] K. Bao, J. Zhang, Y. Zhang, W. Wang, F. Feng,\nand X. He, \u201cTallrec: An effective and efficient tun-\ning framework to align large language model with\nrecommendation,\u201d in RecSys , J. Zhang, L. Chen,\nS. Berkovsky, M. Zhang, T. D. Noia, J. Basilico, L. Piz-\nzato, and Y. Song, Eds. ACM, 2023, pp. 1007\u20131014.\n[810] Y. Zhu, L. Wu, Q. Guo, L. Hong, and J. Li, \u201cCollabora-\ntive large language model for recommender systems,\u201d\narXiv preprint arXiv:2311.01343 , 2023.\n[811] B. Zheng, Y. Hou, H. Lu, Y. Chen, W. X.\nZhao, and J.-R. Wen, \u201cAdapting large language\nmodels by integrating collaborative semantics for\nrecommendation,\u201d 2023. [Online]. Available: https:\n//api.semanticscholar.org/CorpusID:265213194\n[812] Y. Xi, W. Liu, J. Lin, J. Zhu, B. Chen, R. Tang, W. Zhang,R. Zhang, and Y. Yu, \u201cTowards open-world recom-\nmendation with knowledge augmentation from large\nlanguage models,\u201d CoRR , vol. abs/2306.10933, 2023.\n[813] Q. Liu, N. Chen, T. Sakai, and X. Wu, \u201cA first look\nat llm-powered generative news recommendation,\u201d\nCoRR , vol. abs/2305.06566, 2023.\n[814] R. Li, W. Deng, Y. Cheng, Z. Yuan, J. Zhang, and\nF. Yuan, \u201cExploring the upper limits of text-based\ncollaborative filtering using large language models:\nDiscoveries and insights,\u201d CoRR , vol. abs/2305.11700,\n2023.\n[815] W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng,\nJ. Wang, D. Yin, and C. Huang, \u201cLlmrec: Large lan-\nguage models with graph augmentation for recom-\nmendation,\u201d CoRR , vol. abs/2311.00423, 2023.\n[816] X. Li, B. Chen, L. Hou, and R. Tang, \u201cCtrl: Connect\ntabular and language model for ctr prediction,\u201d arXiv\npreprint arXiv:2306.02841 , 2023.\n[817] A. Muhamed, I. Keivanloo, S. Perera, J. Mracek, Y. Xu,\nQ. Cui, S. Rajagopalan, B. Zeng, and T. Chilimbi, \u201cCtr-\nbert: Cost-effective knowledge distillation for billion-\nparameter teacher models,\u201d in NeurIPS Efficient Natu-\nral Language and Speech Processing Workshop , 2021.\n[818] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang,\nJ. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, W. X.\nZhao, Z. Wei, and J. Wen, \u201cA survey on large lan-\nguage model based autonomous agents,\u201d CoRR , vol.\nabs/2308.11432, 2023.\n[819] L. Wang, J. Zhang, X. Chen, Y. Lin, R. Song, W. X.\nZhao, and J. Wen, \u201cRecagent: A novel simulation\nparadigm for recommender systems,\u201d CoRR , vol.\nabs/2306.02552, 2023.\n[820] E. Ie, C. Hsu, M. Mladenov, V . Jain, S. Narvekar,\nJ. Wang, R. Wu, and C. Boutilier, \u201cRecsim: A con-\nfigurable simulation platform for recommender sys-\ntems,\u201d CoRR , vol. abs/1909.04847, 2019.\n[821] J. Zhang, Y. Hou, R. Xie, W. Sun, J. J. McAuley, W. X.\nZhao, L. Lin, and J. Wen, \u201cAgentcf: Collaborative\nlearning with autonomous language agents for recom-\nmender systems,\u201d CoRR , vol. abs/2310.09233, 2023.\n[822] A. Zhang, L. Sheng, Y. Chen, H. Li, Y. Deng, X. Wang,\nand T. Chua, \u201cOn generative agents in recommenda-\ntion,\u201d CoRR , vol. abs/2310.10108, 2023.\n[823] Y. Du, Z. Liu, J. Li, and W. X. Zhao, \u201cA survey of\nvision-language pre-trained models,\u201d in Proceedings of\nthe Thirty-First International Joint Conference on Artificial\nIntelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022 ,\nL. D. Raedt, Ed. ijcai.org, 2022, pp. 5436\u20135443.\n[824] Z. Gan, L. Li, C. Li, L. Wang, Z. Liu, and\nJ. Gao, \u201cVision-language pre-training: Basics, recent\nadvances, and future trends,\u201d Found. Trends Comput.\nGraph. Vis. , vol. 14, no. 3-4, pp. 163\u2013352, 2022.\n[825] P . K. Rubenstein, C. Asawaroengchai, D. D. Nguyen,\nA. Bapna, Z. Borsos, F. de Chaumont Quitry, P . Chen,\nD. E. Badawy, W. Han, E. Kharitonov et al. , \u201cAu-\ndiopalm: A large language model that can speak and\nlisten,\u201d CoRR , 2023.\n[826] J. Alayrac, J. Donahue, P . Luc, A. Miech, I. Barr,\nY. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han,\nZ. Gong, S. Samangooei, M. Monteiro, J. L. Menick,120\nS. Borgeaud, A. Brock, A. Nematzadeh, S. Shar-\nifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zis-\nserman, and K. Simonyan, \u201cFlamingo: a visual lan-\nguage model for few-shot learning,\u201d in NeurIPS , 2022.\n[827] C. Schuhmann, R. Beaumont, R. Vencu, C. Gor-\ndon, R. Wightman, M. Cherti, T. Coombes, A. Katta,\nC. Mullis, M. Wortsman, P . Schramowski, S. Kun-\ndurthy, K. Crowson, L. Schmidt, R. Kaczmarczyk,\nand J. Jitsev, \u201cLAION-5B: an open large-scale dataset\nfor training next generation image-text models,\u201d in\nNeurIPS , 2022.\n[828] S. Changpinyo, P . Sharma, N. Ding, and R. Soricut,\n\u201cConceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts,\u201d in\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, CVPR 2021, virtual, June 19-25, 2021 . Computer\nVision Foundation / IEEE, 2021, pp. 3558\u20133568.\n[829] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang,\nA. Hu, P . Shi, Y. Shi, C. Li, Y. Xu, H. Chen, J. Tian,\nQ. Qi, J. Zhang, and F. Huang, \u201cmplug-owl: Mod-\nularization empowers large language models with\nmultimodality,\u201d CoRR , vol. abs/2304.14178, 2023.\n[830] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P . Wang, J. Lin,\nC. Zhou, and J. Zhou, \u201cQwen-vl: A frontier large\nvision-language model with versatile abilities,\u201d CoRR ,\nvol. abs/2308.12966, 2023.\n[831] H. Liu, C. Li, Y. Li, and Y. J. Lee, \u201cImproved\nbaselines with visual instruction tuning,\u201d CoRR , vol.\nabs/2310.03744, 2023.\n[832] P . Zhang, X. Dong, B. Wang, Y. Cao, C. Xu, L. Ouyang,\nZ. Zhao, S. Ding, S. Zhang, H. Duan, W. Zhang,\nH. Yan, X. Zhang, W. Li, J. Li, K. Chen, C. He,\nX. Zhang, Y. Qiao, D. Lin, and J. Wang, \u201cInternlm-\nxcomposer: A vision-language large model for ad-\nvanced text-image comprehension and composition,\u201d\nCoRR , vol. abs/2309.15112, 2023.\n[833] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and\nR. Zhao, \u201cShikra: Unleashing multimodal llm\u2019s ref-\nerential dialogue magic,\u201d CoRR , vol. abs/2306.15195,\n2023.\n[834] F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang,\n\u201cAligning large multi-modal model with robust in-\nstruction tuning,\u201d CoRR , vol. abs/2306.14565, 2023.\n[835] Y. Du, H. Guo, K. Zhou, W. X. Zhao, J. Wang, C. Wang,\nM. Cai, R. Song, and J.-R. Wen, \u201cWhat makes for\ngood visual instructions? synthesizing complex visual\nreasoning instructions for visual instruction tuning,\u201d\n2023.\n[836] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grau-\nman, J. Luo, and J. P . Bigham, \u201cVizwiz grand chal-\nlenge: Answering visual questions from blind peo-\nple,\u201d in CVPR . Computer Vision Foundation / IEEE\nComputer Society, 2018, pp. 3608\u20133617.\n[837] A. Mishra, K. Alahari, and C. V . Jawahar, \u201cTop-down\nand bottom-up cues for scene text recognition,\u201d in\nCVPR . IEEE Computer Society, 2012, pp. 2687\u20132694.\n[838] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao,\nY. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin,\n\u201cMmbench: Is your multi-modal model an all-around\nplayer?\u201d CoRR , vol. abs/2307.06281, 2023.\n[839] C. Fu, P . Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin,Z. Qiu, W. Lin, J. Yang, X. Zheng, K. Li, X. Sun, and\nR. Ji, \u201cMME: A comprehensive evaluation benchmark\nfor multimodal large language models,\u201d CoRR , vol.\nabs/2306.13394, 2023.\n[840] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang,\nE. Zhao, Y. Zhang, Y. Chen, L. Wang, A. T. Luu, W. Bi,\nF. Shi, and S. Shi, \u201cSiren\u2019s song in the AI ocean: A\nsurvey on hallucination in large language models,\u201d\nCoRR , vol. abs/2309.01219, 2023.\n[841] A. Gunjal, J. Yin, and E. Bas, \u201cDetecting and prevent-\ning hallucinations in large vision language models,\u201d\nCoRR , vol. abs/2308.06394, 2023.\n[842] J. Lu, J. Rao, K. Chen, X. Guo, Y. Zhang, B. Sun,\nC. Yang, and J. Yang, \u201cEvaluation and mitigation of\nagnosia in multimodal large language models,\u201d CoRR ,\nvol. abs/2309.04041, 2023.\n[843] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell,\nand K. Saenko, \u201cObject hallucination in image cap-\ntioning,\u201d in EMNLP . Association for Computational\nLinguistics, 2018, pp. 4035\u20134045.\n[844] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and\nJ.-R. Wen, \u201cEvaluating object hallucination in large\nvision-language models,\u201d in The 2023 Conference on\nEmpirical Methods in Natural Language Processing , 2023.\n[Online]. Available: https://openreview.net/forum?\nid=xozJw0kZXF\n[845] D. A. Hudson and C. D. Manning, \u201cGQA: A new\ndataset for real-world visual reasoning and compo-\nsitional question answering,\u201d in CVPR . Computer\nVision Foundation / IEEE, 2019, pp. 6700\u20136709.\n[846] P . Lu, S. Mishra, T. Xia, L. Qiu, K. Chang, S. Zhu,\nO. Tafjord, P . Clark, and A. Kalyan, \u201cLearn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering,\u201d in NeurIPS , 2022.\n[847] A. Singh, V . Natarjan, M. Shah, Y. Jiang, X. Chen,\nD. Parikh, and M. Rohrbach, \u201cTowards vqa models\nthat can read,\u201d in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2019, pp.\n8317\u20138326.\n[848] F. Liu, T. Guan, Z. Li, L. Chen, Y. Yacoob, D. Manocha,\nand T. Zhou, \u201cHallusionbench: You see what you\nthink? or you think what you see? an image-context\nreasoning benchmark challenging for gpt-4v(ision),\nllava-1.5, and other multi-modality models,\u201d CoRR ,\nvol. abs/2310.14566, 2023.\n[849] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,\nC. L. Zitnick, and D. Parikh, \u201cVQA: visual question\nanswering,\u201d in ICCV . IEEE Computer Society, 2015,\npp. 2425\u20132433.\n[850] R. Vedantam, C. L. Zitnick, and D. Parikh, \u201cCider:\nConsensus-based image description evaluation,\u201d in\nCVPR . IEEE Computer Society, 2015, pp. 4566\u20134575.\n[851] H. Liu, C. Li, Q. Wu, and Y. J. Lee, \u201cVisual instruction\ntuning,\u201d CoRR , vol. abs/2304.08485, 2023.\n[852] P . Xu, W. Shao, K. Zhang, P . Gao, S. Liu, M. Lei,\nF. Meng, S. Huang, Y. Qiao, and P . Luo, \u201cLvlm-ehub:\nA comprehensive evaluation benchmark for large\nvision-language models,\u201d CoRR , vol. abs/2306.09265,\n2023.\n[853] Z. Li, Y. Wang, M. Du, Q. Liu, B. Wu, J. Zhang,\nC. Zhou, Z. Fan, J. Fu, J. Chen, X. Huang, and Z. Wei,121\n\u201cReform-eval: Evaluating large vision language mod-\nels via unified re-formulation of task-oriented bench-\nmarks,\u201d CoRR , vol. abs/2310.02569, 2023.\n[854] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and\nY. Shan, \u201cSeed-bench: Benchmarking multimodal\nllms with generative comprehension,\u201d CoRR , vol.\nabs/2307.16125, 2023.\n[855] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu,\nX. Wang, and L. Wang, \u201cMm-vet: Evaluating large\nmultimodal models for integrated capabilities,\u201d CoRR ,\nvol. abs/2308.02490, 2023.\n[856] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y. Jiang,\n\u201cTo see is to believe: Prompting GPT-4V for better\nvisual instruction tuning,\u201d CoRR , vol. abs/2311.07574,\n2023.\n[857] Y. Zhang, R. Zhang, J. Gu, Y. Zhou, N. Lipka, D. Yang,\nand T. Sun, \u201cLlavar: Enhanced visual instruction tun-\ning for text-rich image understanding,\u201d arXiv preprint\narXiv:2306.17107 , 2023.\n[858] X. Qi, K. Huang, A. Panda, M. Wang, and P . Mittal,\n\u201cVisual adversarial examples jailbreak aligned large\nlanguage models,\u201d in The Second Workshop on New\nFrontiers in Adversarial Machine Learning , 2023.\n[859] Y. Zhou, C. Cui, J. Yoon, L. Zhang, Z. Deng, C. Finn,\nM. Bansal, and H. Yao, \u201cAnalyzing and mitigating\nobject hallucination in large vision-language models,\u201d\narXiv preprint arXiv:2310.00754 , 2023.\n[860] Z. Sun, S. Shen, S. Cao, H. Liu, C. Li, Y. Shen, C. Gan,\nL.-Y. Gui, Y.-X. Wang, Y. Yang et al. , \u201cAligning large\nmultimodal models with factually augmented rlhf,\u201d\narXiv preprint arXiv:2309.14525 , 2023.\n[861] S. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and\nX. Wu, \u201cUnifying large language models and knowl-\nedge graphs: A roadmap,\u201d CoRR , vol. abs/2306.08302,\n2023.\n[862] E. Jim \u00b4enez-Ruiz, O. Hassanzadeh, V . Efthymiou,\nJ. Chen, and K. Srinivas, \u201cSemtab 2019: Resources to\nbenchmark tabular data to knowledge graph match-\ning systems,\u201d in The Semantic Web - 17th International\nConference, ESWC 2020, Heraklion, Crete, Greece, May\n31-June 4, 2020, Proceedings , ser. Lecture Notes in Com-\nputer Science, vol. 12123. Springer, 2020, pp. 514\u2013530.\n[863] Y. Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang,\nJ. Liu, X. Chen, Y. Zhao, Y. Lu, W. Liu, Z. Wu,\nW. Gong, J. Liang, Z. Shang, P . Sun, W. Liu,\nX. Ouyang, D. Yu, H. Tian, H. Wu, and H. Wang,\n\u201cERNIE 3.0: Large-scale knowledge enhanced pre-\ntraining for language understanding and generation,\u201d\nCoRR , vol. abs/2107.02137, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2107.02137\n[864] Z. Zhang, X. Han, Z. Liu, X. Jiang, M. Sun, and\nQ. Liu, \u201cERNIE: enhanced language representation\nwith informative entities,\u201d in Proceedings of the 57th\nConference of the Association for Computational Linguis-\ntics, ACL 2019, Florence, Italy, July 28- August 2, 2019,\nVolume 1: Long Papers . Association for Computational\nLinguistics, 2019, pp. 1441\u20131451.\n[865] X. Wang, T. Gao, Z. Zhu, Z. Zhang, Z. Liu, J. Li, and\nJ. Tang, \u201cKEPLER: A unified model for knowledge\nembedding and pre-trained language representation,\u201d\nTrans. Assoc. Comput. Linguistics , vol. 9, pp. 176\u2013194,2021.\n[866] J. Zhang, X. Zhang, J. Yu, J. Tang, J. Tang, C. Li,\nand H. Chen, \u201cSubgraph retrieval enhanced model\nfor multi-hop knowledge base question answering,\u201d\ninProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: Long\nPapers), ACL 2022, Dublin, Ireland, May 22-27, 2022 .\nAssociation for Computational Linguistics, 2022, pp.\n5773\u20135784.\n[867] P . Ke, H. Ji, Y. Ran, X. Cui, L. Wang, L. Song, X. Zhu,\nand M. Huang, \u201cJointgt: Graph-text joint represen-\ntation learning for text generation from knowledge\ngraphs,\u201d in Findings of the Association for Computational\nLinguistics: ACL/IJCNLP 2021, Online Event, August 1-\n6, 2021 , ser. Findings of ACL, vol. ACL/IJCNLP 2021.\nAssociation for Computational Linguistics, 2021, pp.\n2526\u20132538.\n[868] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou, \u201cLarge\nscale knowledge graph based synthetic corpus gener-\nation for knowledge-enhanced language model pre-\ntraining,\u201d CoRR , vol. abs/2010.12688, 2020.\n[869] W. Chen, Y. Su, X. Yan, and W. Y. Wang, \u201cKGPT:\nknowledge-grounded pre-training for data-to-text\ngeneration,\u201d in Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing,\nEMNLP 2020, Online, November 16-20, 2020 . Associ-\nation for Computational Linguistics, 2020, pp. 8635\u2013\n8648.\n[870] Y. Gu, X. Deng, and Y. Su, \u201cDon\u2019t generate, discrimi-\nnate: A proposal for grounding language models to\nreal-world environments,\u201d in Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2023, Toronto,\nCanada, July 9-14, 2023 . Association for Computa-\ntional Linguistics, 2023, pp. 4928\u20134949.\n[871] L. Luo, Y. Li, G. Haffari, and S. Pan, \u201cReasoning\non graphs: Faithful and interpretable large language\nmodel reasoning,\u201d CoRR , vol. abs/2310.01061, 2023.\n[872] Y. Lan and J. Jiang, \u201cQuery graph generation for\nanswering multi-hop complex questions from knowl-\nedge bases,\u201d in Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020 , D. J. and, Ed. Association for\nComputational Linguistics, 2020, pp. 969\u2013974.\n[873] P . Wang, N. Zhang, X. Xie, Y. Yao, B. Tian,\nM. Wang, Z. Xi, S. Cheng, K. Liu, G. Zheng, and\nH. Chen, \u201cEasyedit: An easy-to-use knowledge edit-\ning framework for large language models,\u201d CoRR , vol.\nabs/2308.07269, 2023.\n[874] Y. Yao, P . Wang, B. Tian, S. Cheng, Z. Li, S. Deng,\nH. Chen, and N. Zhang, \u201cEditing large language mod-\nels: Problems, methods, and opportunities,\u201d CoRR ,\nvol. abs/2305.13172, 2023.\n[875] S. Choi, T. Fang, Z. Wang, and Y. Song, \u201cKCTS:\nknowledge-constrained tree search decoding with\ntoken-level hallucination detection,\u201d CoRR , vol.\nabs/2310.09044, 2023.\n[876] S. Zhang, L. Pan, J. Zhao, and W. Y. Wang, \u201cMit-\nigating language model hallucination with inter-\nactive question-knowledge alignment,\u201d CoRR , vol.\nabs/2305.13669, 2023.122\n[877] Y. Zhu, X. Wang, J. Chen, S. Qiao, Y. Ou,\nY. Yao, S. Deng, H. Chen, and N. Zhang, \u201cLlms\nfor knowledge graph construction and reasoning:\nRecent capabilities and future opportunities,\u201d CoRR ,\nvol. abs/2305.13168, 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2305.13168\n[878] S. Russell and P . Norvig, Artificial Intelligence:\nA Modern Approach (4th Edition) . Pearson, 2020.\n[Online]. Available: http://aima.cs.berkeley.edu/\n[879] B. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J.\nGershman, \u201cBuilding machines that learn and think\nlike people,\u201d CoRR , vol. abs/1604.00289, 2016.\n[880] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran,\nK. Narasimhan, and Y. Cao, \u201cReact: Synergizing rea-\nsoning and acting in language models,\u201d CoRR , vol.\nabs/2210.03629, 2022.\n[881] 2023. [Online]. Available: https://github.com/\nAntonOsika/gpt-engineer\n[882] X. Team, \u201cXagent: An autonomous agent for complex\ntask solving,\u201d 2023.\n[883] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin,\nand B. Ghanem, \u201cCAMEL: communicative agents for\n\u201dmind\u201d exploration of large scale language model\nsociety,\u201d CoRR , vol. abs/2303.17760, 2023.\n[884] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang,\nC. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou,\nC. Ran, L. Xiao, and C. Wu, \u201cMetagpt: Meta pro-\ngramming for multi-agent collaborative framework,\u201d\nCoRR , vol. abs/2308.00352, 2023.\n[885] C. Pham, B. Liu, Y. Yang, Z. Chen, T. Liu, J. Yuan,\nB. A. Plummer, Z. Wang, and H. Yang, \u201cLet mod-\nels speak ciphers: Multiagent debate through embed-\ndings,\u201d CoRR , vol. abs/2310.06272, 2023.\n[886] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mor-\ndatch, \u201cImproving factuality and reasoning in lan-\nguage models through multiagent debate,\u201d CoRR , vol.\nabs/2305.14325, 2023.\n[887] M. Karpinska, N. Akoury, and M. Iyyer, \u201cThe per-\nils of using mechanical turk to evaluate open-ended\ntext generation,\u201d in Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Dominican\nRepublic, 7-11 November, 2021 , M. Moens, X. Huang,\nL. Specia, and S. W. Yih, Eds. Association for Com-\nputational Linguistics, 2021, pp. 1265\u20131285.\n[888] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard,\nC. Bishop, V . Carbune, and A. Rastogi, \u201cRLAIF: scal-\ning reinforcement learning from human feedback with\nAI feedback,\u201d CoRR , vol. abs/2309.00267, 2023.\n[889] T. Wang, P . Yu, X. E. Tan, S. O\u2019Brien, R. Pa-\nsunuru, J. Dwivedi-Yu, O. Golovneva, L. Zettlemoyer,\nM. Fazel-Zarandi, and A. Celikyilmaz, \u201cShepherd:\nA critic for language model generation,\u201d CoRR , vol.\nabs/2308.04592, 2023.\n[890] G. Cui, L. Yuan, N. Ding, G. Yao, W. Zhu, Y. Ni,\nG. Xie, Z. Liu, and M. Sun, \u201cUltrafeedback: Boosting\nlanguage models with high-quality feedback,\u201d CoRR ,\nvol. abs/2310.01377, 2023.\n[891] X. Wang, Z. Wang, J. Liu, Y. Chen, L. Yuan, H. Peng,\nand H. Ji, \u201cMINT: evaluating llms in multi-turn inter-\naction with tools and language feedback,\u201d CoRR , vol.abs/2309.10691, 2023.\n[892] S. Saha, O. Levy, A. Celikyilmaz, M. Bansal, J. Weston,\nand X. Li, \u201cBranch-solve-merge improves large lan-\nguage model evaluation and generation,\u201d CoRR , vol.\nabs/2310.15123, 2023.\n[893] X. Zhang, B. Yu, H. Yu, Y. Lv, T. Liu, F. Huang, H. Xu,\nand Y. Li, \u201cWider and deeper LLM networks are fairer\nLLM evaluators,\u201d CoRR , vol. abs/2308.01862, 2023.\n[894] C. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang,\nJ. Fu, and Z. Liu, \u201cChateval: Towards better llm-based\nevaluators through multi-agent debate,\u201d CoRR , vol.\nabs/2308.07201, 2023.\n[895] R. Li, T. Patel, and X. Du, \u201cPRD: peer rank and\ndiscussion improve large language model based eval-\nuations,\u201d CoRR , vol. abs/2307.02762, 2023.\n[896] L. Zhu, X. Wang, and X. Wang, \u201cJudgelm: Fine-tuned\nlarge language models are scalable judges,\u201d CoRR , vol.\nabs/2310.17631, 2023.\n[897] Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen,\n\u201cEvaluating large language models at evaluating in-\nstruction following,\u201d CoRR , vol. abs/2310.07641, 2023.\n[898] R. Koo, M. Lee, V . Raheja, J. I. Park, Z. M. Kim,\nand D. Kang, \u201cBenchmarking cognitive biases in\nlarge language models as evaluators,\u201d CoRR , vol.\nabs/2309.17012, 2023.\n[899] P . West, X. Lu, N. Dziri, F. Brahman, L. Li, J. D.\nHwang, L. Jiang, J. Fisher, A. Ravichander, K. Chandu,\nB. Newman, P . W. Koh, A. Ettinger, and Y. Choi, \u201cThe\ngenerative AI paradox: \u201dwhat it can create, it may not\nunderstand\u201d,\u201d CoRR , vol. abs/2311.00059, 2023.\n[900] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu,\nX. Song, and D. Zhou, \u201cLarge language models cannot\nself-correct reasoning yet,\u201d CoRR , vol. abs/2310.01798,\n2023.\n[901] K. Stechly, M. Marquez, and S. Kambhampati, \u201cGPT-\n4 doesn\u2019t know it\u2019s wrong: An analysis of itera-\ntive prompting for reasoning problems,\u201d CoRR , vol.\nabs/2310.12397, 2023.\n[902] O. Nov, N. Singh, and D. M. Mann, \u201cPutting chat-\ngpt\u2019s medical advice to the (turing) test,\u201d CoRR , vol.\nabs/2301.10035, 2023.\n[903] K. Yang, S. Ji, T. Zhang, Q. Xie, and S. Ananiadou,\n\u201cOn the evaluations of chatgpt and emotion-enhanced\nprompting for mental health analysis,\u201d CoRR , vol.\nabs/2304.03347, 2023.\n[904] K. Jeblick, B. Schachtner, J. Dexl, A. Mittermeier, A. T.\nSt\u00a8uber, J. Topalis, T. Weber, P . Wesp, B. O. Sabel,\nJ. Ricke, and M. Ingrisch, \u201cChatgpt makes medicine\neasy to swallow: An exploratory case study on sim-\nplified radiology reports,\u201d CoRR , vol. abs/2212.14882,\n2022.\n[905] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn,\nL. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal,\nM. Schaekermann, A. Wang, M. Amin, S. Lachgar,\nP . A. Mansfield, S. Prakash, B. Green, E. Dominowska,\nB. A. y Arcas, N. Tomasev, Y. Liu, R. Wong, C. Sem-\nturs, S. S. Mahdavi, J. K. Barral, D. R. Webster, G. S.\nCorrado, Y. Matias, S. Azizi, A. Karthikesalingam, and\nV . Natarajan, \u201cTowards expert-level medical question\nanswering with large language models,\u201d CoRR , vol.\nabs/2305.09617, 2023.123\n[906] S. Yang, H. Zhao, S. Zhu, G. Zhou, H. Xu, Y. Jia, and\nH. Zan, \u201cZhongjing: Enhancing the chinese medical\ncapabilities of large language model through expert\nfeedback and real-world multi-turn dialogue,\u201d CoRR ,\nvol. abs/2308.03549, 2023.\n[907] S. Chen, B. H. Kann, M. B. Foote, H. J. Aerts, G. K.\nSavova, R. H. Mak, and D. S. Bitterman, \u201cThe utility\nof chatgpt for cancer treatment information,\u201d medRxiv ,\n2023.\n[908] K. Malinka, M. Peres \u00b4\u0131ni, A. Firc, O. Hujnak, and\nF. Janus, \u201cOn the educational impact of chatgpt: Is\nartificial intelligence ready to obtain a university de-\ngree?\u201d CoRR , vol. abs/2303.11146, 2023.\n[909] T. Susnjak, \u201cChatgpt: The end of online exam in-\ntegrity?\u201d CoRR , vol. abs/2212.09292, 2022.\n[910] K. Tan, T. Pang, and C. Fan, \u201cTowards applying pow-\nerful large ai models in classroom teaching: Opportu-\nnities, challenges and prospects,\u201d 2023.\n[911] F. Kamalov and I. Gurrib, \u201cA new era of artificial\nintelligence in education: A multifaceted revolution,\u201d\nCoRR , vol. abs/2305.18303, 2023.\n[912] E. Kasneci, K. Se\u00dfler, S. K \u00a8uchemann, M. Bannert,\nD. Dementieva, F. Fischer, U. Gasser, G. Groh,\nS. G \u00a8unnemann, E. H \u00a8ullermeier et al. , \u201cChatgpt for\ngood? on opportunities and challenges of large lan-\nguage models for education,\u201d Learning and Individual\nDifferences , vol. 103, p. 102274, 2023.\n[913] A. Blair-Stanek, N. Holzenberger, and B. V . Durme,\n\u201cCan GPT-3 perform statutory reasoning?\u201d CoRR , vol.\nabs/2302.06100, 2023.\n[914] D. Trautmann, A. Petrova, and F. Schilder, \u201cLegal\nprompt engineering for multilingual legal judgement\nprediction,\u201d CoRR , vol. abs/2212.02199, 2022.\n[915] J. H. Choi, K. E. Hickman, A. Monahan, and\nD. Schwarcz, \u201cChatgpt goes to law school,\u201d Available\nat SSRN , 2023.\n[916] J. J. Nay, \u201cLaw informs code: A legal informatics\napproach to aligning artificial intelligence with hu-\nmans,\u201d CoRR , vol. abs/2209.13020, 2022.\n[917] F. Yu, L. Quartey, and F. Schilder, \u201cLegal prompting:\nTeaching a language model to think like a lawyer,\u201d\nCoRR , vol. abs/2212.01326, 2022.\n[918] D. Trautmann, A. Petrova, and F. Schilder, \u201cLegal\nprompt engineering for multilingual legal judgement\nprediction,\u201d CoRR , vol. abs/2212.02199, 2022.\n[919] A. Tamkin, M. Brundage, J. Clark, and D. Ganguli,\n\u201cUnderstanding the capabilities, limitations, and so-\ncietal impact of large language models,\u201d CoRR , vol.\nabs/2102.02503, 2021.\n[920] Z. Sun, \u201cA short survey of viewing large language\nmodels in legal aspect,\u201d CoRR , vol. abs/2303.09136,\n2023.\n[921] A. Abid, M. Farooqi, and J. Zou, \u201cPersistent anti-\nmuslim bias in large language models,\u201d in AIES \u201921:\nAAAI/ACM Conference on AI, Ethics, and Society, Virtual\nEvent, USA, May 19-21, 2021 , M. Fourcade, B. Kuipers,\nS. Lazar, and D. K. Mulligan, Eds. ACM, 2021, pp.\n298\u2013306.\n[922] A. Shah and S. Chava, \u201cZero is not hero yet: Bench-\nmarking zero-shot performance of llms for financial\ntasks,\u201d CoRR , vol. abs/2305.16633, 2023.[923] D. Araci, \u201cFinbert: Financial sentiment analysis\nwith pre-trained language models,\u201d CoRR , vol.\nabs/1908.10063, 2019.\n[924] J. C. S. Alvarado, K. Verspoor, and T. Baldwin, \u201cDo-\nmain adaption of named entity recognition to sup-\nport credit risk assessment,\u201d in Proceedings of the\nAustralasian Language Technology Association Workshop,\nALTA 2015, Parramatta, Australia, December 8 - 9, 2015 ,\nB. Hachey and K. Webster, Eds. ACL, 2015, pp. 84\u201390.\n[925] G. Son, H. Jung, M. Hahm, K. Na, and S. Jin, \u201cBeyond\nclassification: Financial reasoning in state-of-the-art\nlanguage models,\u201d CoRR , vol. abs/2305.01505, 2023.\n[926] X. Zhang, Q. Yang, and D. Xu, \u201cXuanyuan 2.0: A large\nchinese financial chat model with hundreds of billions\nparameters,\u201d arXiv preprint arXiv:2305.12002 , 2023.\n[927] H. Yang, X.-Y. Liu, and C. D. Wang, \u201cFingpt: Open-\nsource financial large language models,\u201d CoRR , vol.\nabs/2306.06031, 2023.\n[928] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu,\n\u201cPubmedqa: A dataset for biomedical research ques-\ntion answering,\u201d in Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong Kong,\nChina, November 3-7, 2019 , 2019, pp. 2567\u20132577.\n[929] A. Krithara, A. Nentidis, K. Bougiatiotis, and\nG. Paliouras, \u201cBioasq-qa: A manually curated corpus\nfor biomedical question answering,\u201d 2022.\n[930] Z. Bi, N. Zhang, Y. Xue, Y. Ou, D. Ji, G. Zheng, and\nH. Chen, \u201cOceangpt: A large language model for\nocean science tasks,\u201d CoRR , vol. abs/2310.02031, 2023.\n[931] C. Zhang, C. Zhang, C. Li, Y. Qiao, S. Zheng, S. K.\nDam, M. Zhang, J. U. Kim, S. T. Kim, J. Choi, G. Park,\nS. Bae, L. Lee, P . Hui, I. S. Kweon, and C. S. Hong,\n\u201cOne small step for generative ai, one giant leap for\nAGI: A complete survey on chatgpt in AIGC era,\u201d\nCoRR , vol. abs/2304.06488, 2023.\n[932] M. Haman and M. Skolnik, \u201cUsing chatgpt to conduct\na literature review.\u201d Accountability in research , 2023.\n[933] \u00a8O. Ayd\u0131n and E. Karaarslan, \u201cOpenai chatgpt gen-\nerated literature review: Digital twin in healthcare,\u201d\nSSRN Electronic Journal , 2022.\n[934] Y. J. Park, D. Kaplan, Z. Ren, C. Hsu, C. Li, H. Xu, S. Li,\nand J. Li, \u201cCan chatgpt be used to generate scientific\nhypotheses?\u201d CoRR , vol. abs/2304.12208, 2023.\n[935] M. M. Hassan, R. A. Knipper, and S. K. K. Santu,\n\u201cChatgpt as your personal data scientist,\u201d CoRR , vol.\nabs/2305.13657, 2023.\n[936] L. Cheng, X. Li, and L. Bing, \u201cIs GPT-4 a good data\nanalyst?\u201d CoRR , vol. abs/2305.15038, 2023.\n[937] S. I. M. Hussam Alkaissi, \u201cArtificial hallucinations in\nchatgpt: Implications in scientific writing,\u201d PubMed ,\n2023.\n[938] A. Azaria, R. Azoulay, and S. Reches, \u201cChatgpt\nis a remarkable tool \u2013 for experts,\u201d CoRR , vol.\nabs/2306.03102, 2023.\n[939] O. O. Buruk, \u201cAcademic writing with GPT-3.5: reflec-\ntions on practices, efficacy and transparency,\u201d CoRR ,\nvol. abs/2304.11079, 2023.\n[940] R. Liu and N. B. Shah, \u201cReviewergpt? an exploratory\nstudy on using large language models for paper re-124\nviewing,\u201d CoRR , vol. abs/2306.00622, 2023.\n[941] M. Kosinski, \u201cTheory of mind may have sponta-\nneously emerged in large language models,\u201d CoRR ,\nvol. abs/2302.02083, 2023.\n[942] M. M. Amin, E. Cambria, and B. W. Schuller, \u201cWill\naffective computing emerge from foundation models\nand general ai? A first evaluation on chatgpt,\u201d CoRR ,\nvol. abs/2303.03186, 2023.\n[943] G. Sridhara, R. H. G., and S. Mazumdar, \u201cChatgpt: A\nstudy on its utility for ubiquitous software engineer-\ning tasks,\u201d CoRR , vol. abs/2305.16837, 2023.\n[944] W. Sun, C. Fang, Y. You, Y. Miao, Y. Liu, Y. Li, G. Deng,\nS. Huang, Y. Chen, Q. Zhang, H. Qian, Y. Liu, and\nZ. Chen, \u201cAutomatic code summarization via chatgpt:\nHow far are we?\u201d CoRR , vol. abs/2305.12865, 2023.\n[945] C. S. Xia and L. Zhang, \u201cConversational automated\nprogram repair,\u201d CoRR , vol. abs/2301.13246, 2023.\n[946] W. Kuang, B. Qian, Z. Li, D. Chen, D. Gao, X. Pan,\nY. Xie, Y. Li, B. Ding, and J. Zhou, \u201cFederatedscope-\nllm: A comprehensive package for fine-tuning large\nlanguage models in federated learning,\u201d 2023."
}