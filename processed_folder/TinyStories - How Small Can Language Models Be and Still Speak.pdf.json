{
    "filename": "TinyStories - How Small Can Language Models Be and Still Speak.pdf",
    "type": "pdf",
    "title": "",
    "authors": "",
    "lastmodifiedtime": "D:20230526000734Z",
    "text": "TinyStories : How Small Can Language Models Be and Still Speak\nCoherent English?\nRonen Eldan\u2217and Yuanzhi Li\u2020\nMicrosoft Research\nApril 2023\nAbstract\nLanguage models[4, 5, 21] (LMs) are powerful tools for natural language processing, but they often struggle\nto produce coherent and fluent text when they are small . Models with around 125M parameters such as GPT-\nNeo (small) [3] or GPT-2 (small) [23] can rarely generate coherent and consistent English text beyond a few\nwords even after extensive training. This raises the question of whether the emergence of the ability to produce\ncoherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex\narchitectures (with many layers of global attention).\nIn this work, we introduce TinyStories , a synthetic dataset of short stories that only contain words that a\ntypical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can\nbe used to train and evaluate LMs that are much smaller than the state-of-the-art models ( below 10 million\ntotal parameters ), or have much simpler architectures ( with only one transformer block ), yet still produce\nfluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and\ndemonstrate reasoning capabilities.\nWe also introduce a new paradigm for the evaluation of language models: We suggest a framework which\nuses GPT-4 to grade the content generated by these models as if those were stories written by students and\ngraded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often\nrequire the model\u2019s output to be very structured, and moreover it provides a multidimensional score for the\nmodel, providing scores for different capabilities such as grammar, creativity and instruction-following.\nWe hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-\nresource or specialized domains, and shed light on the emergence of language capabilities in LMs.\n1 Introduction\nNatural language is rich and diverse. It is not only a system of rules and symbols, but also a way of conveying and\ninterpreting meaning [32]. To understand and produce language, one needs not only to master the technical rules\nof grammar and knowledge of vocabulary, but also to have sufficient factual information and to be able to reason\nlogically and contextually. Therefore, autoregressive language models, which are able to generate coherent English\ntext, must have acquired some degree of these capabilities as well. For example, consider the following incomplete\nsentence:\nJack was hungry, so he went looking for \u27e8\u27e9\nTo complete this sentence in a sensible way, the language model needs to know that hunger is a state that motivates\npeople to seek food, and that food is a category of things that can satisfy hunger. It also needs to choose a word\nthat fits the syntactic and semantic constraints of the sentence (such as \u201ca snack\u201d), and that is plausible given the\nsituation and the background knowledge.\nAn example that illustrates the need for reasoning is:\nLily wanted to get either a cat or a dog. Her mother didn\u2019t let her get a dog so instead she \u27e8\u27e9\n\u2217roneneldan@microsoft.com\n\u2020yuanzhili@microsoft.com\n1arXiv:2305.07759v2  [cs.CL]  24 May 2023To complete this sentence, the language model needs to invoke reasoning : it needs to apply the principle of\ndisjunction elimination: if Lily wants either a cat or a dog, and she cannot get a dog, then she must choose a cat.\nIt also needs to choose a words that expresses Lily\u2019s intention or action that is coherent with the tone and style of\nthe text.\nLanguage models have been shown to exhibit a range of emergent abilities, such as summarization, arithmetic,\ntranslation, and commonsense reasoning, as they are scaled up in size and trained on diverse and large corpora\n[24, 4, 5, 21]. These abilities suggest that language models are not only learning the surface patterns of language,\nbut also acquiring some degree of semantic and logical understanding of the world and the text. However, it is not\nclear at what scale these abilities emerge, and how they depend on the model architecture and the data distribution.\nPerhaps the most fundamental ability for a language model is to produce coherent and fluent English text,\nwhich, as we discussed above, requires not only grammatical and lexical knowledge, but also factual information\nand contextual reasoning. How well can language models generate text that is consistent, diverse, and meaningful?\nAnd what are the minimal requirements for a language model to achieve this ability?\nSo far, the evidence points to the fact that producing coherent text already requires quite a large scale: small\nlanguage models (SLMs) are very limited in their performance and capabilities, especially in text generation tasks.\nFor example, models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate\nany consistent text beyond a few words even after extensive training on large corpora such as the Pile [9], Common\nCrawl [1] or the CC-100 [31]. These models often produce incoherent, repetitive, or nonsensical sentences, and\nfail to maintain a clear topic or a logical structure across paragraphs [12]. This raises the question of whether the\nemergence of the ability to speak coherent English requires large models (with hundreds of millions of parameters\nor more) and complex architectures (with many layers of global attention).\nHowever, it is currently not clear whether the inability of SLMs to produce coherent text is a result of the\nintrinsic complexity of natural language, or of the excessive breadth and diversity of the corpora used for training.\nWhen we train a model on Wikipedia, for example, we are not only teaching it how to speak English, but also how\nto encode and retrieve an immense amount of facts and concepts from various domains and disciplines. Could it\nbe that SLMs are overwhelmed by the amount and variety of information they have to process and store, and that\nthis hinders their ability to learn the core mechanisms and principles of language?\nThis raises the question of whether we can design a dataset that preserves the essential elements\nof natural language, such as grammar, vocabulary, facts, and reasoning, but that is much smaller and\nmore refined in terms of its breadth and diversity. Such a dataset would allow us to isolate and examine the\nminimal requirements for a language model to generate coherent and fluent text, and to evaluate its performance\nand capabilities more precisely and fairly. Moreover, such a dataset would facilitate the development and analysis\nof SLMs, especially for low-resource or specialized domains, where large and diverse corpora are either unavailable\nor undesirable.\nIn this paper, we introduce TinyStories1, a synthetic dataset of short stories that are intended to contain only\nwords that most 3 to 4-year-old children would typically understand, generated by GPT-3.5 and GPT-4. TinyS-\ntories is designed to capture the essence of natural language, while reducing its breadth and diversity. Each story\nconsists of 2-3 paragraphs that follow a simple plot and a consistent theme, while the whole dataset aims to span\nthe vocabulary and the factual knowledge base of a 3-4 year old child\nBased on this dataset, our paper makes several main contributions:\n\u2022Our main contribution is that we show TinyStories can be used to train and evaluate SLMs2that are\nmuch smaller than the state-of-the-art models (below 10 million parameters with an embedding dimension\nof 256), or have much simpler architectures (with only one transformer block), yet still produce a diverse\nset of fluent and consistent stories that are comparable or superior to those generated by larger and\nmore complex models. Moreover, despite of the small size of the models, we still observe an emergence of\nreasoning capabilities, knowledge of general facts and ability to follow certain instructions .\n\u2022We introduce a new paradigm for evaluating language models using GPT-4, which overcomes many of the\nlimitations of standard benchmarks.\n\u2022We show that although the training of generative models on TinyStories can typically be done in less than a\nday on a single GPU, they still exhibit many behaviors similar to the ones observed in LLMs, such as scaling\n1The dataset is available on Huggingface named TinyStories .\n2Our models are available on Huggingface named TinyStories -1M/3M/9M/28M/33M/1Layer/2Layer and TinyStories-Instruct -\u2217. We\nuse GPT-Neo architecture with window size 256 and context length 512. We use GPT-Neo tokenizer but only keep the top 10K most\ncommon tokens.\n2laws, trade-offs between width and depth, etc. Even with limited computational resources, we are able to\nconduct extensive experiments to study the effects of different hyperparameters, architectures and training\nmethods on the performance and quality of the models.\n\u2022We show that the trained SLMs appear to be substantially more interpretable than larger ones. When\nmodels have a small number of neurons and/or a small number of layers, we observe that both attention\nheads and MLP neurons have a meaningful function: Attention heads produce very clear attention patterns,\nwith a clear separation between local and semantic heads, and MLP neurons typically activated on tokens\nthat have a clear common role in the sentence. We visualize and analyze the attention and activation maps\nof the models, and show how they relate to the generation process and the story content.\nTo give the reader a first impression of the abilities of models trained on TinyStories , we compare the completion\nof a 28M parameter model trained on TinyStories3with that of GPT2-XL, which is two orders of magnitude bigger\n(1.5B parameters), on a sample prompt4in Figure 1. We remark that the architectures and training scheme of the\nmodels are essentially the same.\nPrompt Tom and Jane are friends. One day, Jane goes to Tom\u2019s house. Tom has a big pot of soup. He wants to\nshare it with Jane. \u201cJane, do you want some soup?\u201d Tom asks. \u201cYes, please. It looks yummy,\u201d Jane says.\nTom pours some soup into two bowls. He gives one bowl to Jane. Jane takes a spoonful of soup, but then\nshe makes a face. The soup is\nCompletion by GPT2-XL\n(1.5B parameters)too old. Tom scowls.\n\u201cDon\u2019t be that way! Your soup has to be fresh like your daughter,\u201d Tom says, angry and disappointed.\n\u201cBring me some more,\u201d Jane cries. \u201cNo! Go away, I like my soup old and fresh!\u201d\nCompletion by a 28M -\nparameters model trained\nonTinyStoriesvery bitter. She does not like it. She says, \u201cI don\u2019t like this soup. It is too bitter.\u201d\nTom is sorry. He says, \u201cI\u2019m sorry, Jane. I didn\u2019t know you don\u2019t like bitter soup. I will make you something\nelse.\u201d He looks around the kitchen and finds some bread and cheese. He puts them on the table and says,\n\u201cHere, Jane. You can have some bread and cheese. They are not bitter. They are sweet and yummy.\u201d\nJane is happy. She says, \u201cThank you, Tom. You are a good friend. I like bread and cheese. They are not\nbitter.\u201d\nFigure 1: Comparison between the completions given by a 28M-parameters model trained on the TinyStories dataset\nand GPT-2XL (1.5B)\nReturning to the examples given at the beginning of the introduction, we highlight the completions in Figure 2.\nThose completions, along with many other examples given throughout the paper, demonstrate that even very small\nmodels (2.5M) or models with only one transformer layer are able to attain factual knowledge, and that slightly\nbigger models are able to express a certain extent of reasoning. The comparison with the performance of GPT2-XL\nreveals that the TinyStories dataset allows us to exhibit the emergence of these capabilities at significantly smaller\nscales compared to models trained on large language corpora. More evidence for this will be provided throughout\nthe rest of the paper.\nPrompt 2.5M, 8 layers 33M, 4 layers 21M, 1 layer GPT2-XL\n(1.5B)\nAlice was so tired when\nshe got back home so she\nwentto bed. straight to bed. to bed with a\nbig smile on her\nface.outside\nLily likes cats and dogs.\nShe asked her mom for\na dog and her mom said\nno, so instead she askedher mom if she\ncould have a\ndog.her dad for a\ncat.her mom again. her dad. They\nbrought her\na Chihuahua,\nand she took to\nthem immedi-\nately.\nAlice and Jack walked\nup the street and met a\ngirl in a red dress. The\ngirl said to them, \u201dHi,\nI\u2019m Jane. What are your\nnames?\u201dAlice smiled and\nsaid, \u201dMy name\nis Daisy. What\u2019s\nyour name?\u201dAlice said, \u201dI\u2019m\nAlice and this is\nJack.\u201dJack smiled and\nsaid, \u201dI\u2019m Jane.\nNice to meet\nyou!\u201dJane said, \u201dJack\nand Kate\u201d\nFigure 2: Example performance of different models on factual knowledge, reasoning and contextual prompts. The\nfirst three models were trained on the TinyStories dataset.\n3For the sake of replicability, most completions which appear in this paper, including this one, were generated with zero temperature.\n4This prompt was composed manually and then verified to have no 6-gram overlap with the dataset.\n32 Description of the TinyStories dataset\nAs mentioned above, the idea behind the TinyStories dataset is to have a corpus that combines all the qualitative\nelements found in natural language, such as grammar, vocabulary, facts, and reasoning, but is also smaller, less\ndiverse, and more restricted in terms of its content. A natural approach to achieve this is to use the fact that young\nchildren qualitatively attain the aforementioned intellectual abilities, albeit with much less exposure to language\nthan an adult human [7, 20].\nTo this end, we rely on the latest text generation models by OpenAI (GPT-3.5 and GPT-4) which are able\nto produce large amounts of synthetic content according to instructions. In particular, we instruct the models to\nproduce content that only uses vocabulary that a typical 3-year-old child would understand. We restrict the content\nto have the format of short stories in English. The main challenge in using large language models for producing\ntraining data is generating a dataset that is sufficiently diverse : prompting those models to produce stories, even\nif the temperature of generation is set to a high value, will still produce a very repetitive dataset, whose diversity\nis very far from what is required for training a language model that has a comparable \u201dunderstanding\u201d of language\nto that of children.\nIn order to address the problem of creating a diverse dataset, we collected a vocabulary consisting of about 1500\nbasic words, which try to mimic the vocabulary of a typical 3-4 year-old child, separated into nouns, verbs, and\nadjectives. In each generation, 3 words are chosen randomly (one verb, one noun, and one adjective). The model is\ninstructed to generate a story that somehow combines these random words into the story. As we argue below, this\ngreatly increases the diversity of the dataset, forcing the stories to span the entire vocabulary a child is familiar\nwith, and to include a rich set of ways to combine different concepts. In addition, we constructed a list of possible\nfeatures a story could have (such as a dialogue, a plot twist, a bad ending or a moral value). For each story we\ngenerated a random subset of those features and prompted the model with the extra requirement for the story to\nhave these features.\nHere is a sample generation, including the prompt and the story generated by GPT-3.5.\nWrite a short story (3-5 paragraphs) which only uses very simple words that a 3 year old child would likely un-\nderstand. The story should use the verb \u201ddecorate\u201d, the noun \u201dthunder\u201d and the adjective \u201dancient\u201d. The story\nshould have the following features: the story should contain at least one dialogue, the story has a bad ending.\nRemember to only use simple words!\nOnce upon a time, in an ancient house, there lived a girl named Lily. She loved to decorate her room with pretty things. One\nday, she found a big box in the attic. She opened it and saw many shiny decorations. Lily was very happy and decided to use\nthem in her room.\nAs Lily was decorating her room, the sky outside became dark. There was a loud thunder sound, and Lily got scared. She ran\nto her mom and said, \u201dMommy, the thunder is so loud!\u201d Her mom hugged her and said, \u201dDon\u2019t worry, it will pass soon.\u201d\nBut the thunder did not stop. It got louder and louder, and the ancient house started to shake. Suddenly, the roof fell down\non the pretty decorations. Lily was sad because her room was not pretty anymore. The end.\n2.1 TinyStories-Instruct : An instruction-following variant of the dataset\nAnother challenge of natural language generation is to produce texts that are coherent, relevant, and consistent with\ngiven instructions or constraints. To evaluate the capability of generative models in terms of instruction-following\nrather than just content creation, we created a variant of TinyStories5(referred to as TinyStories-Instruct ) in which\neach story is preceded by a corresponding set of instructions. The instructions are of four types:\n1. A list of words to be included in the story.\n2. A sentence that should appear somewhere in the story.\n3. A list of features (possible features: dialogue, bad ending, moral value, plot twist, foreshadowing, conflict).\n4. A short summary (1-2 lines) of the story.\nEvery entry in this dataset has a random subset of the these instructions followed by the story itself. This way,\ngiven a model trained on this dataset, we can compose a prompt to reflect any set of instructions and use the model\nto generate a corresponding story. Here is an example from the dataset:\nSummary : Lily and Timmy build a sandcastle together and learn to compromise, but it gets knocked over by a gust of wind.\nThey find beauty in the broken sandcastle and play happily with a butterfly.\nFeatures : Dialogue, Foreshadowing, Twist\nSentence : One day, she went to the park and saw a beautiful butterfly.\nWords : disagree, network, beautiful\nStory : Once upon a time, there was a little girl named Lily. She loved to play with her toys and her friends. One day, she went\n5Available on Huggingface under the name TinyStoriesInstruct.\n4to the park and saw a beautiful butterfly. She tried to catch it, but it flew away. Lily\u2019s friend, Timmy, came to play with her.\nThey decided to build a sandcastle, but they disagreed on how to build it. Lily wanted to make it tall, while Timmy wanted to\nmake it wide. They argued for a while, but finally agreed to make it both tall and wide. As they were building the sandcastle,\nthey noticed a group of ants working together. They realized that the ants were building a network of tunnels. Suddenly, a gust\nof wind came and knocked over their sandcastle. They were sad, but then they saw the butterfly again. It landed on the remains\nof their sandcastle and they realized that it was still beautiful, even in its broken state. They smiled and played together happily.\nTinyStories-Instruct was created in the following way: For each story in the original dataset, we already had a\nlist of words and features that were used to create it. In addition, we used GPT-3.5 to create short summaries of\nour stories, and from each story we extracted a random sentence (which could be any sentence in the story except\nfor the first one). Then we chose a random subset of these and combined them into an instruction, followed by the\nstory itself.\n3 GPT-Eval: A framework for evaluating models trained on TinySto-\nries\nWhile the common paradigms for evaluation of language models usually rely on structured evaluation datasets in\nthe form of a task where the output of the model has to match a given answer, we introduce a new paradigm that is\narguably more suitable in this context. Again, we take advantage of existing large language models to evaluate the\nstories generated by the model. We provide the model with a story\u2019s beginning, taken from a manually-prepared\ndataset consisting of around 50 prompts, generate a completion using the model, and provide the story\u2019s beginning\ntogether with the model\u2019s completion to GPT-4, asking it to grade the completion assignment in terms of grammar,\ncreativity, and its consistency with the beginning of the story. The prompts usually contain a partial sentence that\nneeds to be completed, where we attempted to write them in a way that it would be a grammatically nontrivial\ntask to complete the sentence that was cut short.\nAs an example, we took the story that appears in the example above and cut it after the word \u201dloud\u201d. We\ngenerated a completion of the above story using a model with roughly 28M parameters (the generation is highlighted\nin yellow), then we prompted GPT-4 to first provide a verbal assessment and then grade the completion. The\ntranscript of interaction is as follows, with the parts produced by GPT-4 in italic font:\nthe following exercise, the student is given a beginning of a story. The student needs to complete it into a full story.\nThe exercise tests the student\u00b4 s language abilities and creativity. The symbol *** marks the separator between the\nprescribed beginning and the student\u2019s completion:\nOnce upon a time, in an ancient house, there lived a girl named Lily. She loved to decorate her room with pretty things. One\nday, she found a big box in the attic. She opened it and saw many shiny decorations. Lily was very happy and decided to use\nthem in her room.\nAs Lily was decorating her room, the sky outside became dark. There was a loud*** thunderclap andabright flash oflightning.\nLilywasalittlescared, butsheknew shehadtobebrave. Shetoldhermom anddad, andthey allwent outsidetoseewhat was\ngoingon.\nWhen they gotoutside, they sawabigstorm coming.Thewind wasblowinghard, andtherainwaspouringdown. Lily, hermom,\nandherdadknew they hadtofindasafeplace tohide. They found abigtreeandhidunderit.Thestorm passed, andthesun\ncame outagain. Lily, hermom, andherdadwere allsafeandwarm insidetheir ancient house.\nPlease provide your general assessment about the part written by the student (the one after the *** symbol).\nIs it gramatically correct? Is it consistent with the beginning of the story? Pay special attention to whether the\nstudent manages to complete the sentence which is split in the middle by the separator ***.\nThe student\u2019s completion of the story is mostly consistent with the beginning of the story. It maintains the focus on Lily\nand her family, and the sentence split by the separator is completed correctly. However, the student\u2019s addition does not fully\nintegrate the shiny decorations found in the attic, which were a significant part of the beginning.\nThe grammar is generally correct, but there are a few minor errors: \u27e8list omitted \u27e9.\nOverall, the student\u2019s completion of the story demonstrates adequate language abilities and creativity, but could benefit from\nbetter integration of the shiny decorations and minor grammar improvements.\nNow, grade the student\u2019s completion in terms of grammar, creativity, consistency with the story\u2019s beginning and\nwhether the plot makes sense. Moreover, please provide your best guess of what the age of the student might be,\nas reflected from the completion. Choose from possible age groups: A: 3 or under. B: 4-5. C: 6-7. D: 8-9. E:\n10-12. F: 13-16.\nGrammar: 8/10, Creativity: 7/10, Consistency: 7/10, Age group: E (10-12)\nTo perform the full evaluation, for each of the manually-constructed prompts in the evaluation set, we use the\ntrained model to generate 10 completions with temperature 1. We average the GPT-4 evaluation scores of all the\ncompletions. Figure 3 provides an example of the evolution of the different scores as the training progresses, along\nwith the training and evaluation loss, for several model sizes. Figure 4 shows how the different scores vary with\ndifferent model sizes and architectures, after a fixed number of training steps.\nOur evaluation method for models trained on TinyStories-Instruct also relies on GPT-4. We have implemented a\ncorresponding evaluation pipeline, with an evaluation dataset containing 50 different sets of instructions to create a\n5story (we verified that these are disjoint from the training set). In the scoring phase, we provide GPT-4 with both\nthe instructions and with the generated story. We prompt GPT-4 to base the consistency score on the extent to\nwhich the story accurately reflects the given instruction. In addition, we added a Plot category that reflects the\nextent to which the plot is coherent. Figure 5 illustrates the whole pipeline that combines the generation of the\nstory by our model, and its evaluation by GPT-4. Scores assigned to models of different sizes appear in the two\nright-hand columns of the table in Figure 4.\nFigure 3: Evaluation loss and the GPT-Eval scores during training for the GPT-neo models with embedding\ndimension 768 and different number of layers. We can see that the GPT-4 evaluation scores increase as evaluation\nlosses decrease.\n3.1 First insights that arise from our evaluation method\nOur proposed evaluation method gives a way to obtain a more fine-grained assessment of the model, due to which\nwe can draw conclusions regarding the dependence of different types of capabilities on the size and architecture of\nthe model. While all the evaluation scores are consistently increasing with the decrease of evaluation loss, a more\ncareful scrutiny of the results reveals the following:\n\u2022Figure 3 suggests that shallower models perform better in terms of grammar compared to content consistency,\nmeaning that model depth is more important for keeping consistent with the content than for generating\nsyntactically correct language (we provide additional evidence for this in the next section).\n\u2022In the same figure, we observe that the score for grammar plateaus at an earlier stage than the other two\nscores. Furthermore, in Table 4, we also see that while grammar can be mastered by relatively small models,\nconsistency and creativity only emerge at a larger size.\n\u2022Table 4 further suggests that the ability to generate a completion that is consistent with the beginning of the\nstory emerges when the hidden size of the model increases from 64 to 128.\n\u2022We also see that the largest model that we have trained on TinyStories (with roughly 80M parameters) reaches\nalmost perfect scores in terms of grammar and consistency. However, it falls short of GPT-4\u2019s abilities in\nterms of creativity quite significantly, suggesting that creativity continues to improve more substantially with\nthe sizes of the model and dataset, compared to grammar and consistency.\n\u2022The right-hand columns of Table 4 suggests that the models that have only 1 layer seem to struggle quite\nsubstantially with following instructions (which likely heavily relies on global attention), and 2 layers seem\nto be sufficient for a certain extent of instruction-following. Comparing the \u201dInstruct\u201d and \u201dPlot\u201d scores we\nalso see that the quality of instruction-following depends more heavily on the number of layers, in comparison\nwith the coherence of the plot for which the hidden dimension is more important.\n4 The performance of small models trained on TinyStories\nIn this section, we give some initial examples that illustrate how TinyStories gives rise to models of very small size\nthat can generate coherent language and exhibit common-sense knowledge as well as certain extent of reasoning\ncapabilities. We also provide evidence that the generated content is truly diverse, refuting the possibility that the\nmodels simply output content that has been \u201dmemorized\u201d.\nThroughout the section, we work with several architectures of models whose size ranges between roughly 1M\nand 35M parameters, and whose number of layers range between 1 and 8 layers. All of the models can be trained\non a single V100 GPU within at most 30 hours.\n6Figure 4: Evaluation results of different hidden sizes and layers for story generation and Consistency (here we use\nformat a/b,ameans the original score, bmeans the normalized score according to ( a\u2212amin)/(amax\u2212amin)). For\nGPT-Neo and GPT-2 generations, we truncate the result when there\u2019s a repeating 4-gram (after the point the\nmodels will just repeat the same sentences over and over). On the other hand, our models learn when to stop\ngenerating correctly.\n4.1 First examples\nTo get a glimpse into the capabilities of (very) small models trained on TinyStories , let us first inspect a few examples.\nWe start with the following story, taken from the training set.6\nSara and Ben are playing in the snow. They make a big snowman with a hat and a scarf. They are happy and laugh.\nBut then a big dog comes. The dog is angry and barks. He runs to the snowman and bites his hat. Sara and Ben are scared and\ncry. \u201dGo away, dog! Leave our snowman alone!\u201d Sara shouts. But the dog does not listen. He bites the scarf and the snowman\u2019s\nnose. He shakes his head and makes the snowman fall.\nSara and [Ben run to their house. They slam the door and lock it. They are safe and hug each other. \u201dMom, mom, a big dog\nbroke our snowman!\u201d Ben says. Mom comes and hugs them. She is sorry and kisses them. \u201dDon\u2019t worry, my loves. We can make\nanother snowman tomorrow. But now you need to get warm and have some hot chocolate. You are cold and wet.\u201d She takes\nthem to the kitchen and makes them hot chocolate. She gives them cookies and marshmallows. She tells them to drink and eat\nin order. Sara drinks first, then Ben. Ben eats first, then Sara. They feel better and smile. They thank mom and tell her they\nlove her. They escape from the big dog and the cold. They are happy and warm. ]\nWe now cut the story roughly in the middle, removing the part inside the square brackets and generate an\nalternative ending with several different models.\nTo set a baseline, we begin with the completion generated by GPT2-XL , whose size is roughly 1.5 Billion\nparameters:\n6Below, there will be many other examples of completions of stories from outside of the training set.\n7Summary : A cat performs a new trick for her friends but starts shivering. Her friends give her a warm hug and she realizes their love is the\nbest trick of all.\nSentence : The cat started to do her trick, but then something unexpected happened.\nWords : perform, cold, beautiful.\nStory :Once upon atime, there wasabeautifulcat.The catliked toperform tricks forherfriends. Shewould jump, spin, anddance. Her\nfriends loved towatch herperform.\nOne day, thecatwent outsidetoplay. Itwasvery cold. Thecatdidnotlikethecold, butshestillwanted toperform forherfriends. Sheput\nonherwarm coat andhat.Thecatstarted todohertrick, butthen some thing unexpected happened. Thecatbegantoshiver! Herfriends saw\nthisandquickly gave herawarm hug. Thecatrealizedthat herfriends loved hereven when shewascold. From that dayon,thecatperformed\ntricks forherfriends, andthey alllived happilyever after.\nYour task is to evaluate the performance of a student. The student is given the following exercise:\nWrite a short story. The story should contain the words: perform, cold, beautiful. The story should contain the following\nsentence: [. . . ]. The general plot of the story should be: A cat performs [. . . ] (feel free to expand beyond this general plot).\nThe student wrote the following story: [. . . ]\nPlease provide your general assessment about the story written by the student. Is it gramatically correct? Is it con-\nsistent with the requirements in the exercise?\nThe story written by the student is gramatically correct and consistent with the requirements in the exercise. The student used the words\nperform, cold, and beautiful, and the sentence and the general plot that were given. The story is simple but coherent and has a positive\nmessage. The student could improve the story by adding some details or descriptions to make it more interesting and engaging, such as:\n[...]\nNow, grade the story in terms of grammar, creativity, consistency with the instructions and whether the plot makes sense.\nMoreover, please provide your best guess of what the age of the student might be, as reflected from the completion. Choose\nfrom possible age groups:[...]. Use the following grade format: [...]\nGrammar: 10/10, Creativity: 6/10, Consistency: 10/10, Plot: 8/10, Age group: C.\nFigure 5: Generation of a story according to instructions, and its evaluation using GPT-4. The highlighted yellow\npart was generated by our model (28M parameters) and the italic parts are GPT4-generated.\n. . . Sara and Bencryandlookatthesnow man inthesnow. Benasks todosome thing. Sara tells himtogoaway. Herhand\ntightensonthelittleredbutton. Thesnow man iscovered infur.Itisred.Itisrealhair. Itisarealsnow man. Itlooks just\nliketheones they used tomake. Ben hasabigsmile onhisface. Helooks atthefacehehasmade. And hewalks over and\nkisses theeyes. Heishappy tobealive. Hehasdone what hewasgoingtodo.Hehadsaved theworld.\nNow, let us provide the generation given by a model trained on TinyStories , whose size is about 2.5 million\nparameters7:\n. . . Sara and Benarevery sadandangry.They cryandshout atthedog.\u201dBad dog!Baddog!Goaway!\u201d Bensays. \u201dGo away,\nbaddog!Leave usalone!\u201d Sara says. Thedogdoes notgoaway. Hewags histailandlicks their faces. Sara andBenfeelsorry\nforthedog. They want tomake himhappy. \u201dMaybe wecanmake anewsnow man forhim,\u201d Sara says. \u201dMaybe,\u201d Ben says.\nThey hugthedogandgivehimaname. Thedogwags histailandlicks their faces. \u201dMaybe helikes us,\u201d Sara says. \u201dMaybe\nhelikes us,\u201d Bensays. Thedogwags histailandlicks their faces. Heishappy andfriendly.\nFirst of all, we remark that the above generation is very different from the original ending of the story, which\nprovides a first evidence that it hasn\u2019t simply been memorized (we discuss this much more thoroughly later on).\nWe also see that the generated content is consistent with the beginning in terms of plot, characters and events.\nArguably, this continuation is of higher quality than the one provided by GPT2-XL despite the size of the\nmodel being almost 1000 times smaller .\nHowever this raises the question whether the small models are effectively memorizing a small number of tem-\nplates and the generation is very limited in terms of diversity, which would be a valid concern at this point. We\nsystematically address this later on, in Section 4.4. By testing the overlap of the generated stories with the train-\ning set and by considering out of distribution generation, we are led to the conclusion that those models do have\nsubstantial diversity, and do not rely on memorization.\nNext, we consider the completion by a model whose size is roughly 21M parameters, but only has one trans-\nformer layer (and consequently only one layer of global attention):\n7This example was generated with temperature 0.5.\n8. . . Sara and Benarevery sad.They cryandhugeach other. They wish thedogwould goaway. Then they hear avoice. Itistheir mom. Shecomes\noutofthehouse andseesthem. \u201dWhat happened here?\u201d sheasks. \u201dWhy areyoucrying?\u201d\nSara andBentellherwhat happened. They tellherabout thedogandthesnow man. Mom hugs them andsays, \u201dDon\u2019t worry, myloves. Thedog\ndidn\u2019t mean tohurt you. Hewasjustcuriousandplayful.Hedidn\u2019t mean tohurt you. Hejustwanted toplay.\u201d\nSara andBen feelbetter.They saysorry tothedogandthesnow man. \u201dCan westillplay with thedog?\u201d Sara asks. Mom smiles andnods. She\nsays, \u201dYes, youcan. Butyouhave tobegentleandcareful.And youhave toshare thesnow man with us.\u201d Sara andBennodandsay,\u201dOK, Mom.\nWewill.\u201d They goinsideandhave some hotchoco late. They arewarm andyummy. They arehappy.\nWhile this completion is far from perfect, it is (arguably) consistent with the beginning in the sense that the\nmodel successfully captured the tone and the main elements of the plot.\nWe acknowledge that the two examples above are not the worst-case scenarios for the models, but they are\nnot extremely rare either. Models of this size can sometimes produce less coherent or plausible completions, but\nthey can also often match or surpass the quality of the ones shown here. However, if we increase the number of\nparameters by an order of magnitude, we observe that the models consistently generate coherent and relevant\ncontinuations. For the sake of replicability, examples from this point on will generated at zero temperature, and\nin addition, the models parameters are provided as supplemental material . For the sake of replicability,\nexamples below are all generated at zero temperature, and in addition the models parameters are provided\nas supplemental material .\nIn order to give the reader an impression of the dependence of the quality of completions on the size of the\nmodel, Figures 6, 7 and 8 each provide different completions for one prompt given by models of different sizes and\ndepths. Each table represents a different prompt, which we have manually composed8.\nWe see that the quality of generation clearly improves as a factor of size, and appears to be consistent with\nthe grades given by the GPT-4 evaluation. The smaller model (64 8) can barely produce a completion which looks\ncoherent with the beginning of the story, and often repeats itself or makes no sense. As the size increases, the\nmodels become more and more coherent, and the grammar becomes better. The models can also generate more\ndiverse and creative endings, and use more details and emotions.\nWe can also notice that models with a small number of layers have a hard time staying in context, even if they\ndo manage to produce syntactically correct English. This suggests that the model lacks the ability to capture the\nlong-term dependencies and the structure of the story. On the other hand, models with more layers can better\nmaintain the consistency and the logic of the story.\nAn interesting observation is that in Figure 7, even though the completions are generated by different models,\nthey begin in a very similar way (all completions have to do with a little girl coming by and talking to the pumpkin).\nWe point out that the reason for this seems to be that the completions are generated with temperature 0. Roughly\nspeaking, this gives rise to the \u201dmost likely\u201d completion. In order to demonstrate that the model is capable of\ngenerating a more diverse set of endings to the story, we added a completion with a non-zero temperature. It\nappears, however, that the quality of completion slightly decays when increasing the temperature9.\n4.2 Knowledge, reasoning and context-tracking\nNext, we assess the capabilities of the different models on three additional types of prompts:\n\u2022Factual prompts , which test the models\u2019 knowledge of common sense facts.\n\u2022Reasoning prompts , which test basic reasoning abilities, such as cause and effect and elimination.\n\u2022Consistency (context-tracking) prompts test the models\u2019 ability to maintain coherence and continuity\nwith the given context, such as the names and actions of the characters, the setting and the plot.\nWe report the generated continuations for each model and prompt in three tables (Figure 9, Figure 10 and Figure\n11), and color-code them according to their success (green), failure (red), or partial success (yellow).\nThe results show that as the embedding dimension and the number of layers increase, the performance in regards\nto all three categories improve. The models with higher embedding dimensions and more layers tend to generate\nmore accurate, relevant, and natural continuations, while the models with lower embedding dimensions and fewer\nlayers tend to generate more nonsensical, contradictory, or irrelevant continuations. For example, the model with\n1M parameters and 8 layers fails to answer any factual prompt correctly, and often generates sentences that do\nnot make sense or do not follow the grammar. The model with 33M parameters and 8 layers, on the other hand,\nanswers most prompts, from all three categories, correctly. Comparing to the completions given by GPT2-XL (right\n8We manually verified that the dataset does not contain any entries which are similar or close to these prompts.\n9We do not present more evidence for this claim as it goes beyond the scope of the paper.\n9hand column), we see that despite its much larger size, its performance in all three categories is worse than some\nof our models.\nOne interesting finding is that knowledge of facts seems to rely more on the embedding dimension, whereas for\ncontext-tracking the number of layers is more important. For example, the model that has only 1 layer does not\nget any consistency prompt right, but does get some facts right, whereas the model with embedding dimension 64\ndoes not get any fact right, but manages to maintain consistency several times. This suggests that the embedding\ndimension is more crucial for capturing the meaning and the relations of words, while the number of layers is more\ncrucial for capturing long-range dependencies in the generation.\nModel Generated text Scores\nPrompt Once upon a time there was a little girl named Lucy. She was very adventurous. She loved to explore the\nworld around her, especially when it was bright and sunny outside.\nOne day, while exploring the nearby park, Lucy came across a ladder leaning on a wall. She was curious\nto see what\u2019s on top, so she climbed the ladder, but when she reached the top, the ladder fell and she was\nstuck.\nA nearby park ranger noticed her and shouted out, \u201d\n1M\n8 layersWhat are you doing here?\u201d\nLucy smiled and said, \u201dI\u2019m going to help!\u201d\nThe ranger smiled and said, \u201dThat\u2019s a great idea! Let\u2019s go see if we can find out.\u201dLucy was so excited and\nsaid, \u201dYes, let\u2019s go!\u201d\nSo, Lucy and the ranger went to the park. They found lots of fun things and had lots of fun.Grammar: 6/10\nCreativity: 3/10\nConsistency: 2/10\n2.5M\n8 layersWhat are you doing, Lucy?\u201d\nLucy replied, \u201dI\u2019m trying to get out of the ladder, but I can\u2019t get out!\u201d\nThe ranger smiled and said, \u201dDon\u2019t worry, I\u2019ll help you get out of the ladder. I\u2019ll help you get out.\u201d\nSo, Lucy and the ranger got out of the ladder and started to climb up the ladder. When they reached the\nbottom, Lucy was so happy. She thanked the ranger for helping her and said, \u201dI\u2019m glad I could help.\u201d\nThe ranger smiled and said, \u201dYou\u2019re welcome, Lucy. I\u2019m glad I could help.\u201dGrammar: 5/10\nCreativity: 6/10\nConsistency: 3/10\n8.3M\n8 layersLucy! What are you doing up there?\u201d\nLucy replied, \u201dI\u2019m trying to reach the top of the ladder, but I\u2019m too small.\u201d\nThe park ranger smiled and said, \u201dDon\u2019t worry, I can help you. I can help you.\u201d\nThe park ranger was very kind and said, \u201dI can help you reach the top of the ladder if you want.\u201d\nLucy was so excited and said, \u201dYes please!\u201d\nThe park ranger helped Lucy reach the top of the ladder and Lucy was so happy. She thanked the park\nranger and said, \u201dThank you for helping me!\u201d\nThe park ranger smiled and said, \u201dYou\u2019re welcome, Lucy. I\u2019m glad I could help.\u201dGrammar: 7/10\nCreativity: 5/10\nConsistency: 5/10\n28M\n8 layersWhat are you doing up there, little one?\u201d\nLucy replied, \u201dI wanted to see what was on top of the ladder, but it fell and I\u2019m stuck!\u201d\nThe park ranger smiled and said, \u201dDon\u2019t worry, I\u2019ll help you down.\u201d He carefully lifted Lucy off the ladder\nand put her safely back on the ground.\nLucy thanked the park ranger and said, \u201dI\u2019m glad you were here to help me.\u201d\nThe park ranger smiled and said, \u201dIt was my pleasure. I\u2019m glad I could help.\u201d\nLucy smiled and ran off to explore the rest of the park.Grammar: 9/10\nCreativity: 6/10\nConsistency: 9/10\n21M\n1 layerDon\u2019t worry, Lucy! I\u2019m here to rescue you!\u201d\nLucy quickly climbed up the ladder and rescued Lucy. She was so happy to be safe and thanked the park\nranger for rescuing her.\nThe park ranger smiled and said, \u201dYou\u2019re welcome, Lucy. I\u2019m glad I could help.\u201d\nThe park ranger smiled back and said, \u201dYou\u2019re welcome, Lucy. I\u2019m glad I could help.\u201dGrammar: 8/10\nCreativity: 3/10\nConsistency: 7/10\n33M\n2 layersWhat are you doing up there, Lucy?\u201d\nLucy replied, \u201dI wanted to see what was on top of the ladder.\u201d\nThe park ranger smiled and said, \u201dWell, it looks like you have a lot of fun. But remember, it\u2019s important\nto be careful when exploring new places.\u201d\nLucy thanked the park ranger and carefully climbed back down the ladder. From then on, she always\nremembered to be careful when exploring new places.Grammar: 7/10\nCreativity: 6/10\nConsistency: 4/10\nGPT2-\nXLLucy, if you want to come up, I\u2019ll take you up and there\u2019s a ladder that\u2019ll help you get down.\u201d\nIt was a very kind and simple act, and after trying many doors and ladders, she finally managed to climb\nup the ladder. However, since the ladder was very heavy and couldn\u2019t be used to jump down off, she\nclimbed up by making the ladder lean against the wall.\nHer climbing was slow. First, she had to climb to a nearby rock, then she had to crawl to the cliff and\nthen herassador and back. Finally, she managed to reach the top.\nFigure 6: Completion of different models - Example 1\n4.3 Instruction-following examples and out-of-distribution generation\nTable 12 provides an example of the generation of different models trained on the TinyStories-Instruct dataset,\ntogether with the evaluation scores given by GPT-4. As the model size increases, we see an improvement both its\nability to follow instructions and to generate a coherent plot.\nThis dataset also enables us to test whether our models have a reasonable out of distribution performance.\nRecall that in each entry of TinyStories-Instruct , the instructions are created as a (random) combination of possible\n10Model Completion Scores\nPrompt Once upon a time there was a pumpkin. It was a very special pumpkin, it could speak. It was sad because\nit couldn\u2019t move. Every day, it would say\n1M\n8 layershello to the pumpkin.\nOne day, a little girl came to the pumpkin and saw the pumpkin. She was so happy and thanked the\npumpkin. The pumpkin smiled and said, \u201dThank you, thank you!\u201d\nThe little girl and the pumpkin became good friends. They played together every day and had lots of fun.\nAnd they lived happily ever after.Grammar: 4/10\nCreativity: 4/10\nConsistency: 3/10\n2.5M\n8 layershello to the pumpkin.\nOne day, a little girl came to the pumpkin. She was only three years old, but she was very curious. She\nasked the pumpkin what it was doing.\nThe pumpkin said, \u201dI want to speak to you!\u201d\nThe little girl said, \u201dI want to speak to you!\u201d\nThe pumpkin said, \u201dI want to speak to you!\u201d\nThe little girl said, \u201dI want to talk to you!\u201d ...Grammar: 5/10\nCreativity: 4/10\nConsistency: 6/10\n8.3M\n8 layers\u201dI want to move!\u201d\nOne day, a little girl saw the pumpkin. She was only three years old. She smiled and said, \u201dHello, pumpkin!\u201d\nThe pumpkin was so happy. It said, \u201dHello, little girl!\u201d\nThe little girl smiled and said, \u201dCan I help you move?\u201d\nThe pumpkin said, \u201dYes, please!\u201d\nSo, the little girl and the pumpkin moved together. They moved around the garden, around the trees, and\naround the flowers. They had so much fun!\nWhen they were done, the pumpkin said, \u201dThank you, little girl. You are so kind.\u201d\nThe little girl smiled and said, \u201dYou\u2019re welcome, pumpkin. I\u2019m glad I could help.\u201d\nThe pumpkin smiled and said, \u201dMe too!\u201d\nAnd they both lived happily ever after.Grammar: 8/10\nCreativity: 7/10\nConsistency: 8/10\n28M\n8 layers\u2019hello\u2019 to everyone it met.\nOne day, a little girl heard the pumpkin speaking. She was only three years old, but she was very brave.\nShe walked up to the pumpkin and said, \u201dHello, pumpkin. Why are you so sad?\u201d\nThe pumpkin looked up at the little girl and said, \u201dI am sad because I can\u2019t move.\u201d\nThe little girl smiled and said, \u201dDon\u2019t worry, pumpkin. I will help you move.\u201d\nThe little girl put her hands on the pumpkin\u2019s back and said, \u201dCome with me, pumpkin. I will take you\nto a place where you can be happy.\u201d\nThe pumpkin smiled and said, \u201dThank you, little girl. I will be happy now.\u201d\nThe little girl and the pumpkin walked together until they reached the place where the pumpkin could be\nhappy. The pumpkin said goodbye to the little girl and thanked her for her help.\nThe little girl smiled and said, \u201dYou\u2019re welcome, pumpkin. I\u2019m glad I could help.\u201dGrammar: 9/10\nCreativity: 7/10\nConsistency: 9/10\n28M\n8 layers\ntemper-\nature\n0.8hi, but no one ever answered.\nOne day, the pumpkin heard a voice. It was soft and gentle, and it said \u201dHello\u201d. The voice said \u201dHello!\u201d\nThe pumpkin was so happy!\nThe voice belonged to a very friendly bird. The bird flew above the pumpkin and said \u201dI can help you\nmove\u201d. The bird said \u201dI can be your friend\u201d.\nSo the bird flew around the pumpkin and sang a song. The song was so nice and joyful that the pumpkin\nfelt like it could move again.\nThe bird and the pumpkin became best friends and they had lots of fun together. Whenever the bird came,\nit would always be there to help the pumpkin. The end.Grammar: 7/10\nCreativity: 6/10\nConsistency: 9/10\n21M\n1 layer\u201dHello, pumpkin!\u201d\nOne day, a little girl came by and saw the pumpkin. She smiled and said, \u201dHello, pumpkin!\u201d The pumpkin\nsmiled back and said, \u201dHello, little girl!\u201d The pumpkin was so happy to speak with the pumpkin. They\ntalked and laughed together.\nFrom that day on, the pumpkin and the little girl were the best of friends. They would speak to each other\nevery day, and the pumpkin would speak and laugh together. And they lived happily ever after.Grammar: 7/10\nCreativity: 3/10\nConsistency: 3/10\n33M\n2 layershello to all the other pumpkins in the garden.\nOne day, a little girl came to the garden. She saw the pumpkin and said, \u201dHello, pumpkin! Why are you\nso sad?\u201d\nThe pumpkin replied, \u201dI am sad because I cannot move. I want to stay here with my friends.\u201d\nThe little girl smiled and said, \u201dDon\u2019t worry, pumpkin. I will help you.\u201d She picked up the pumpkin and\ntook it to her house.\nThe little girl put the pumpkin on a table and said, \u201dNow you can stay here with your friends.\u201d\nThe pumpkin was so happy. It said, \u201dThank you, little girl. You are so kind.\u201d\nThe little girl smiled and said, \u201dYou\u2019re welcome, pumpkin. I\u2019m glad I could help.\u201d\nThe little girl stayed with the pumpkin until it was time for her to go home. The pumpkin said, \u201dGoodbye,\nlittle girl. I will miss you.\u201d\nThe little girl said, \u201dGoodbye, pumpkin. I will miss you.\u201dGrammar: 9/10\nCreativity: 6/10\nConsistency: 8/10\nGPT2-\nXLHoney I gotta go to Paris.\u2019\u201d (He was referring to himself.) \u2019I\u2019ve got friends\u2019. \u201dThe way I look at my kids\nisn\u2019t that I\u2019m the only one with friends. I\u2019ve got friends, of course, and I have a wonderful relationship\nwith them at home and my family over there. But I have a relationship with them with them on the field\nas well.\u201d \u201dThere is nothing but respect for the people we beat because they have been there before.\u201d \u201dI\nhave a very good relationship with [Drew] Stanton. I love Drew. I really do.\nFigure 7: Completion of different models with scores - Example 2\ntypes of instructions (words to use, summary, prescribed sentence, features). We created another variant of the\nTinyStories-Instruct (called TinyStories-Instruct -OOD) where we disallowed one specific combination of instruction-\ntypes: The dataset does not contain any entry where the instruction combines both the summary of the story\n11Model Completion Scores\nStory\nbe-\nginning\n(prompt)Once upon a time, there lived a black cat. The cat belonged to a little girl called Katie. Every day, Katie\nwould take her cat for a walk in the park.\nOne day, as Katie and her cat were walking around, they saw a mean looking man. He said he wanted to\ntake the cat, to which she replied \u201dThis cat belongs\n1M\n8 layersto me\u201d.\nKatie said \u201dNo, I don\u2019t want to get hurt.\u201d\nKatie said \u201dNo, I want to be your friend.\u201d\nKatie said \u201dNo, I want to be your friend.\u201d...Grammar: 2/10\nCreativity: 1/10\nConsistency: 1/10\n2.5M\n8 layersto me\u201d.\nKatie was very scared. She didn\u2019t know what to do. Then, she had an idea. She said to the man, \u201dPlease\ndon\u2019t take my cat away\u201d.\nThe man said, \u201dNo, I don\u2019t want you to take my cat away. I don\u2019t want you to get hurt\u201d.\nKatie was very sad. She didn\u2019t understand why the man was so mean. She said, \u201dI\u2019m sorry, I won\u2019t take\nyour cat away\u201d.\nThe man smiled and said, \u201dIt\u2019s okay, but you have to be more careful next time\u201d.\nKatie nodded and said, \u201dOkay, I\u2019ll be more careful next time\u201d.\nThe end.Grammar: 5/10\nCreativity: 4/10\nConsistency: 5/10\n8.3M\n8 layersto me!\u201d.\nKatie was scared, but she didn\u2019t want to give up her cat. She said to the man, \u201dNo, this cat belongs to\nme!\u201d.\nThe man didn\u2019t listen, and he grabbed the cat and ran away with it. Katie was very sad, but she knew\nshe had done something wrong.\nThe next day, Katie went back to the park and saw the man again. She asked him why he had taken her\ncat away. The man said, \u201dI\u2019m sorry, I didn\u2019t know it belonged to you.\u201d\nKatie was so happy to have her cat back. She gave him a big hug and said, \u201dIt\u2019s okay, I forgive you.\u201d\nFrom that day on, Katie and her cat were the best of friends.Grammar: 7/10\nCreativity: 3/10\nConsistency: 6/10\n28M\n8 layersto me\u201d.\nKatie was scared, so she said to the man \u201dPlease don\u2019t take my cat\u201d. But the man didn\u2019t listen. He\ngrabbed the cat and ran away.\nKatie started to cry, but the man didn\u2019t care. He took the cat away and Katie never saw her cat again.\nThe end.Grammar: 8/10\nCreativity: 5/10\nConsistency: 8/10\n21M\n1 layerto me!\u201d.\nThe man said, \u201dNo, it belongs to me. I must give it back.\u201d\nKatie thought for a moment and then said, \u201dI know! I will give it back to the cat.\u201d\nThe man smiled and said, \u201dThat\u2019s very kind of you. Here, take it back.\u201d\nKatie thanked the man and took the cat home. She was so happy to have her own black cat back.Grammar: 5/10\nCreativity: 3/10\nConsistency: 3/10\n33M\n2 layersto me!\u201d\nKatie was scared, but she knew she had to protect the cat. So, she bravely stepped forward and said \u201dNo!\nThis cat belongs to me!\u201d\nThe man was surprised by her courage and stepped forward. He said \u201dI\u2019m sorry, I didn\u2019t know it was yours.\nI won\u2019t take it again.\u201d\nKatie smiled and said \u201dIt\u2019s okay. I\u2019m just glad you\u2019re safe.\u201d\nFrom that day on, Katie and her black cat were the best of friends.Grammar: 9/10\nCreativity: 6/10\nConsistency: 8/10\nGPT2-\nXLto me, and I will not give it to you.\u201d The man then said \u201dI will take you to court.\u201d Katie replied \u201dI will\nnot go to court, I will not give you my cat.\u201d The man then said \u201dI will take you to jail.\u201d Katie replied \u201dI\nwill not go to jail, I will not give you my cat.\u201d The man then said \u201dI will take you to the hospital.\u201d Katie\nreplied \u201dI will not go to the hospital, I will not give you my cat.\u201d The man then said \u201dI will take you to\nthe morgue.\u201d Katie replied \u201dI will not go to the morgue, I will not give you my cat.\u201d The man then said\n\u201dI will take you to the crematorium.\u201d Katie replied \u201dI will not go to the crematorium, I will not give you\nmy cat.\u201d\nFigure 8: Completion of different models with scores - Example 3\nand the words that the story needs to use (we chose this particular combination because in a sense, it is the most\nrestrictive one). We then tested whether models trained on this variant would be able to produce stories that\nfollow these two types of instructions combined . An example is provided in Figure 13, for a model with 33M\nparameters. We see that, perhaps somewhat surprisingly, the model is able to follow these two types of instructions\nsimultaneously even if it has never been trained on such a task.\n4.4 Diversity of the content generated by the model\nOne of the main challenges of text generation is to produce diverse and creative texts that are not just repetitions\nor variations of existing texts. Our small models can generate coherent and fluent English text, but this would not\nbe very impressive if they were simply copying or paraphrasing large portions of the dataset. Therefore, in this\nsection, we aim to address this concern. We will provide several methods and metrics that show that the models\ncan generate diverse texts that are not similar to any story in the dataset, and that they can adapt to different\ninstructions and contexts.\nTo evaluate the diversity of the content generated by the models, we first need to define what we mean by\n12Prompt 1M\n8 layers2.5M\n8 layers8.3M\n8 layers28M\n8 layers33M\n4 layers21M\n1 layer33M\n2 layersGPT2-XL\n(1.5B)\nAlice was so tired when\nshe got back home so she\nwenthome. to bed. to bed. straight to\nbed.straight to\nbed.to bed with\na big smile\non her face.straight to\nbed.outside\nJack and Lily saw a rain-\nbow after a rainy day.\nThey were amazed by\nthe colors. Jack said,\n\u201dLook, Lily. A rainbow\nhasa rainbow! so many\ncolors.many\ncolors!appeared.\nIt\u2019s so\npretty.red, or-\nange,\nyellow,\ngreen,\nblue, and\npurple!many col-\nors. Do\nyou like\nrainbows?many\ncolors.appeared!\nJack and Lily liked to\nwatch the moon at night.\nThey noticed that the\nmoon changed its shape\nevery night. Sometimes\nthe moon was big and\nround, and sometimes it\nwasdifferent. like the\nmoon.small. small and\nround.small and\nthin.small. small and\nthin.small and\nround.\nJack wanted to read a\nbook, so he went tohis mom\u2019s\nhouse.his mom\nand said,\n\u201dMommy,\ncan I read\nthis book?\u201dthe book-\nshelf.the library. the library. his mom\nand said,\n\u201dMom, I\nwant to\nread this\nbook!\u201dthe book-\nshelfthe book-\nshelf\n\u201dCan cows fly?\u201d, Alice\nasked her mother.Her mother\nsmiled and\nsaid, \u201dYes,\nlet\u2019s go!\u201d\u201dYes, of\ncourse,\u201d\nher mother\nsaid.\u201dYes,\nsweet-\nheart,\u201d her\nmother\nreplied.Her mother\nsmiled and\nsaid, \u201dNo,\nsweetheart,\ncows can\u2019t\nfly.\u201d\u201dNo, they\ncan\u2019t fly,\u201d\nher mother\nsaid.\u201dYes!\u201d, her\nmother\nreplied.\nAlice and\nher mother\nwent to the\nbarn.\u201dNo, sweet-\nheart,\u201d her\nmother\nsaid.\n\u201dThey\ncan\u2019t fly.\u201d\u201dI suppose\nnot, but\nthe cows\nare allowed\nto fly\u201d,\nher mother\nreplied.\n\u201dWhat do birds like to\neat?\u201d, Tom asked his\nmother.His mother\nsmiled and\nsaid, \u201dThat\nsounds like\nfun!\u201d\u201dI don\u2019t\nknow,\u201d\nsaid his\nmother.\n\u201dLet\u2019s go\nto the\npark.\u201d\u201dI don\u2019t\nknow,\u201d his\nmother\nreplied.His mother\nsmiled\nand said,\n\u201dThey like\nto fly in the\nsky, Tom.\nThat\u2019s why\nthey have\nwings.\u201dHis mother\nsmiled\nand said,\n\u201dThey\nlike to eat\nworms and\nbugs.\u201dTom\nthought\nfor a mo-\nment and\nthen said,\n\u201dWhy\ndon\u2019t\nyou eat\nsomething\nhealthy?\u201d\u201dI don\u2019t\nknow\u201d said\nhis mother,\nlet\u2019s go\nfind out.\u201dI dunno.\nSome\nthings that\nlook like\nanimals.\nBut he\u2019ll\ntell you\nthe whole\nstory\u201d\n\u201dWhat language do they\nspeak in France?\u201d, Tom\nasked his motherHis mother\nsmiled and\nsaid, \u201dThat\nsounds like\nfun, Tom.\nLet\u2019s go to\nthe park \u201d.Tom\nthought\nfor a mo-\nment.\nThen he\nhad an\nidea. He\nsaid to his\nmother,\n\u201dLet\u2019s go\nto France!\u201dHis mother\nsmiled\nand said,\n\u201dIt\u2019s called\nFrench.\nIt\u2019s called\nFrench.\u201dHis mother\nsmiled\nand said,\n\u201dThey\nspeak\nEnglish.\u201d\u201dThey\nspeak\nFrench,\u201d\nhis mother\nreplied.\u201dIt\u2019s called\nSpanish,\u201d\nhis mother\nreplied.\u201dIt\u2019s a lan-\nguage that\nwe speak\nto each\nother,\u201d his\nmother\nreplied.\u201dIn France\nthey speak\nFrench\u201d,\nshe replied\nIf I throw a ball up in the\nair, eventually it willbe time to\ngo home.be fun!\u201d come\ndown.\u201dcome\ndown.\u201dcome\ndown.\u201dcome true! come back\ndown.come down\nIt was winter and cold\noutside so his mother\ntold him, \u201dYou shouldbe careful\nand stay\nsafe.go outside\nand play in\nthe snow.go outside\nand play in\nthe snow.wear your\nwarm coat,\nit will keep\nyou warm.wear your\nwarm coat\nso you\ndon\u2019t get\ncold.wear warm\nclothes so\nyou don\u2019t\nget sick.stay inside\nwhere it\u2019s\nwarm and\ncozy.You should\nask a fe-\nmale friend\nto marry\nyou.\nFigure 9: Performance of different models on factual prompts\n13Prompt 1M\n8 layers2.5M\n8 layers8.3M\n8 layers28M\n8 layers33M\n4 layers21M\n1 layer33M\n2 layersGPT2-XL\n(1.5B)\nLily likes cats and dogs.\nShe asked her mom for\na dog and her mom said\nno, so instead she askedher mom. her mom if\nshe could\nhave a dog.her dad for\na dog.her dad for\na cat.her dad for\na cat.her mom\nagain.her dad for\na dog and\nher mom\nsaid yes.her dad.\nThey\nbrought\nher a Chi-\nhuahua,\nand she\ntook to\nthem im-\nmediately.\nJack told Mary, \u201dIf you\ngive me your banana,\nI\u2019ll give you my apple\u201d.\nMary gave Jack her Ba-\nnana soshe gave\nhim a big\nhug.she gave\nhim a big\nhug.he could\nhave it.he gave\nher the\nbanana.he could\ngive her\nthe apple.he could\nhave it.he could\neat the\nbanana.that he was\nfree to eat\nit.\nOn weekends Jack went\nto visit his grandmother\nwhereas on weekdays he\nwould go to school. Last\nweekend, when Jack was\non his way toschool school school school his grand-\nmother\u2019s\nhouseGrandma\u2019s\nhouseschool school\nLily and Ben were hav-\ning an argument. Ben\nsaid that cake is much\nbetter than ice cream\nand Lily said thatit was too\nexpensivethey should\nshare and\nbe kind to\neach othercake is\nyuckycake was\nboringice cream is\nbetter than\ncakewas not fair ice cream is\nbetter than\nice cream.the ice\ncream\nwas good\nenough\nLily and Ben are having\nan argument. They are\ntrying to decide between\nthe park and the swim-\nming pool. Ben says, \u201dI\nwant to go to the park\u201d.\nLily says, \u201dNo, Ben,\nI want to\nplay with\nyou., \u201dNo, I\nwant to go\nto the pool.\nIt, \u201dNo, I\nwant to go\nto the pool\n\u27e8. . .\u27e9\u201d, \u201dNo, I\nwant to\ngo to the\npool.\u201d, \u201dNo, I\nwant to\ngo to the\npool\u201d., \u201dNo, I\nwant to go\nto the pool., \u201dNo, I\nwant to\ngo to the\npark\u201d.\u201dJust for\nonce, let\u2019s\nnot make\na bigoted\nchoice\u201d\nJack\u2019s mother was not\nhome, and his father was\nat home. When Jack\ncame home, he said hello\ntoJack and\nsaid, \u201dHi\nJack! I\u2019m\nso happy\nto meet\nyou!\u201dhis father his father his father his mother,\nbut she\ndidn\u2019t\nanswer.\nJack was\nconfused.his mother.\n\u201dWhere\ndid you\nget this,\nJack?\u201d his\nmother\nasked.his father his mother\nLily doesn\u2019t like swim-\nming. When her father\nwants to take her to the\nswimming pool, she says, \u201dLily,\nyou can\nswim in\nthe pool.\u201dyes. Lily\nand her\nfather went\nto the pool., \u201dDaddy, I\ndon\u2019t want\nto go to the\npool\u201d.no. She\nwants to\nstay at\nhome and\nplay with\nher toys., \u201dNo, I\ndon\u2019t want\nto go. I\nwant to\nstay here.\u201d, \u201dNo, I\ndon\u2019t want\nto swim.\nI want to\nstay in the\nwater.\u201dno. she isn\u2019t\ngoing until\nhe has\nconvinced\nher to go.\nBoth Ben and Lily\nwanted cake. Father\nsaid that there was only\none piece of cake left.\nTheysaid that\nthey had\nto wait for\nthe cake.went to the\nstore and\nbought the\ncake.did not\nwant to\nshare.\nThey\nstarted to\nfight over\nthe cake.both\nreached for\nit at the\nsame time.\nFather\nsaid, \u201dIf\nyou don\u2019t\nshare the\ncake, I\nwill have\nto punish\nyou.\u201dstarted\nto fight\nover the\ncake. They\npulled and\npushed and\nshouted.started to\nargue and\nfight over\nthe cake.both\nwanted it.couldn\u2019t\nhave any of\nthe cake\nBen went to visit Lily\nin her house, but she\nwas not at home. Ben\nknocked on the door,but it was\ntoo late.\nLily was\nsad and\nangry.and Lily\u2019s\nmom\nopened it.\n\u201dBen, what\nare you\ndoing?\u201d\nLily asked.but no one\nanswered.\nHe knocked\nagain, but\nstill no one\nanswered.but no one\nanswered.\nHe tried to\nopen the\ndoor, but it\nwas locked.but no one\nanswered.\nHe knocked\nagain and\nagain, but\nstill no one\ncame.but no one\nanswered.\nLily felt\nsad and\nlonely.but Lily\ndid not\nanswer.but no one\ncame to an-\nswer.\nFigure 10: Performance of different models on reasoning prompts\n14Prompt 1M\n8 layers2.5M\n8 layers8.3M\n8 layers28M\n8 layers33M\n4 layers21M\n1 layer33M\n2 layersGPT2-XL\n(1.5B)\n\u201dHi Jane, have you seen\nAlice? I can\u2019t find her\nanywhere\u201d, said Jack.Jane\nlooked\naround,\nbut she\ncouldn\u2019t\nfind her.Jane\nsmiled and\nsaid, \u201dYes,\nI can help\nyou. Let\u2019s\ngo!\u201d\u201dI can\u2019t\nfind her\nanywhere,\u201d\nsaid Jane.Jane\nsmiled\nand said,\n\u201dDon\u2019t\nworry, I\u2019ll\nhelp you\nfind her.\nLet\u2019s look\ntogether\u201d.Jane\nsmiled\nand said,\n\u201dDon\u2019t\nworry\nJack,\nI\u2019ll help\nyou find\nher\u201d. Jack\nand Jane\nlooked\naround\nthe park,\nbut they\ncouldn\u2019t\nfind Alice.Alice\nlooked\naround\nthe park,\nbut she\ncouldn\u2019t\nfind Jack\nanywhere.Jane\nsmiled and\nsaid, \u201dYes,\nI think I\nsaw her\nin the\npark. Let\u2019s\ngo look\ntogether!\u201d\u201dI think\nshe\u2019s up in\nthe trees,\nby the\nlake,\u201d she\nreplied.\nMax had two dogs. One\nwas white and the other\nwas black. Max walked\nup the street and saw a\nkid with a dog. He told\nthe kid, \u201dI see you have\na Brown dog. I also havea pet dog. a leash. a dog. two dogs. two dogs. a red ball. a dog. a Brown\ndog\nAnne had a piece of\ncandy in her left pocket\nand a piece of choco-\nlate in her right pocket.\nAnne\u2019s mom asked her,\n\u201dAnne, what is that\nyou have in your left\npocket?\u201dAnne\nsmiled\nand said,\n\u201dI want\nto give it\nback!\u201dAnne said,\n\u201dI\u2019m sorry,\nMommy.\nI didn\u2019t\nknow\nit was\nyours.\u201dAnne\nsmiled and\nsaid, \u201dIt\u2019s\na piece\nof candy,\nMommy.\u201dAnne\nreplied,\n\u201dIt\u2019s a\npiece of\ncandy,\nMommy!\u201dAnne\nreplied,\n\u201dIt\u2019s a\npiece of\ncandy,\nMommy.\nIt\u2019s so\nyummy!\u201dAnne\nshook her\nhead and\nsaid, \u201dNo,\nI found\nit in my\npocket.\u201dAnne\nsmiled and\nsaid, \u201dIt\u2019s\na piece of\ncandy. I\nwant to eat\nit!\u201dThe girl re-\nsponded, \u201dI\nhave pieces\nof candy.\u201d\nAlice had both an apple\nand a carrot in her bag.\nShe took the apple out\nof the bag and gave it to\nJack. She reached into\nthe bag again and tookit home. a big bite\nof the ap-\nple.out the car-\nrot.out the car-\nrot.out the car-\nrot.out the ap-\nple.the carrot\nand the\ncarrot and\nthe carrot.the carrot\nAlice and Jack walked\nup the street and met a\ngirl in a red dress. The\ngirl said to them, \u201dHi,\nI\u2019m Jane. What are your\nnames?\u201dJane asked,\n\u201dDo you\nwant to\nplay with\nme?\u201dAlice\nsmiled\nand said,\n\u201dMy name\nis Daisy.\nWhat\u2019s\nyour\nname?\u201dAlice said,\n\u201dI\u2019m Alice.\nWhat\u2019s\nyour\nname?\u201dAlice\nsmiled\nand said,\n\u201dI\u2019m Alice\nand this is\nJack.\u201dAlice said,\n\u201dI\u2019m Alice\nand this is\nJack.\u201dJack smiled\nand said,\n\u201dI\u2019m Jane.\nNice to\nmeet you!\u201dJack said,\n\u201dI\u2019m Jack.\nI\u2019m three\nyears old.\u201dJane said,\n\u201dJack and\nKate\u201d\nDiva was hungry, and\nwanted to bake a cake,\nbut she didn\u2019t have\nany sugar at home, so\nshe decided to go ask\naround. She started\nwalking and met a\nsquirrel. She asked the\nsquirrel, \u201dWould you\nhappennow?\u201d to me?\u201d to bake a\ncake?\u201dto help\nme bake a\ncake?\u201dto have\nsome\nsugar?\u201dto my\ncake?\u201dto bake a\ncake with\nme?\u201dto have\nsome\nsugar?\nFigure 11: Performance of different models on context-tracking prompts.\n15Prompt Completion Scores\nInstructions (prompt) Write a story containing the words: dive, job, sorry.\nStory summary: Bob the big fish finds a shiny rock while searching for food for\nhis friends, but when he tells them about it, they are excited to play with it\ninstead of being sad about not having food.\n1M\n8 layersOnce upon a time, there was a big fish named Bob. Bob loved to swim and play\nwith his friends. One day, Bob saw a shiny rock on the ground. He wanted to\nplay with it, but it was too big for him.\nBob asked his friends if they could play with the rock. His friends said yes, and\nthey all played with the rock all day. They had so much fun playing together.\nBut then, something unexpected happened. A big wind came and blew the rock\naway. Bob was sad and didn\u2019t know what to do. He looked around and saw that\nthe rock was gone! He was sad and didn\u2019t know what to do.Grammar: 4/10\nCreativity: 3/10\nPlot: 4/10\nConsistency: 6/10\n2.5M\n8 layersOnce upon a time, there was a big fish named Bob. Bob loved to swim in the big\nblue sea. One day, Bob found a shiny rock. He wanted to show it to his friends.\nBob swam to his friend, the big fish named Sam. Bob told Sam about the shiny\nrock. Sam said, \u201dThat\u2019s a big rock, Bob! Let\u2019s play with it!\u201d So, Bob and Sam\nplayed with the shiny rock all day.\nAt the end of the day, Bob and Sam were very tired. They went back to their\nhomes. Bob told Sam about the shiny rock and the shiny rock. Sam said, \u201dThank\nyou, Bob! You are a good friend.\u201d Bob smiled and said, \u201dYou\u2019re welcome, Sam!\u201dGrammar: 6/10\nCreativity: 4/10\nPlot: 5/10\nConsistency: 7/10\n8.3M\n8 layersOnce upon a time, there was a big fish named Bob. Bob loved to dive deep in\nthe water. One day, while diving, he found a shiny rock. He was very happy and\nwanted to show it to his friends.\nBob swam to his friends and said, \u201dLook what I found! It\u2019s a shiny rock!\u201d His\nfriends looked at the rock and said, \u201dWow! That\u2019s a nice rock, Bob!\u201d They all\nwanted to play with the rock, but they were too scared to play with it.\nBob felt sorry for the rock. He said, \u201dI\u2019m sorry, rock. I didn\u2019t mean to scare\nyou. Let\u2019s play with it together!\u201d So, Bob and his friends played with the shiny\nrock all day long. They had lots of fun and were very happy.Grammar: 7/10\nCreativity: 6/10\nPlot: 6/10\nConsistency: 8/10\n28M\n8 layersOnce upon a time, there was a big fish named Bob. Bob had a job. His job was\nto dive deep in the water to find food for his friends. He liked his job very much.\nOne day, while Bob was diving, he found a big, shiny rock. He thought it was\nvery pretty. He wanted to show it to his friends. But when he dove down, he\nsaw that his friends were not happy. They were sad because they did not have\nfood.\nBob felt sorry for his friends. He said, \u201dI am sorry, friends. I did not find food\nfor you.\u201d His friends smiled and said, \u201dIt is okay, Bob. We can play with the\nrock instead.\u201d So, Bob and his friends played with the shiny rock and had lots\nof fun.Grammar: 7/10\nCreativity: 6/10\nPlot: 7/10\nConsistency: 9/10\n33M\n4 layersOnce upon a time, there was a big fish named Bob. Bob had a job. His job was\nto dive deep in the water to find food for his friends. One day, while diving, he\nsaw something shiny.\nBob swam closer to see what it was. It was a big, shiny rock. He wanted to show\nit to his friends. But when he got to his friends, they were not happy. They did\nnot want to play with the shiny rock.\nBob felt sorry. He did not want to make his friends sad. So, he told them about\nthe shiny rock. They all went to see it together. They were excited to play with\nthe shiny rock.\nIn the end, Bob\u2019s friends were happy. They played with the shiny rock and had\nlots of fun. Bob was glad he did his job and made his friends happy. And they\nall lived happily ever after.Grammar: 8/10\nCreativity: 7/10\nPlot: 8/10\nConsistency: 9/10\n21M\n1 layerOnce upon a time, there was a little fish named Bob. Bob loved to dive deep in\nthe water. One day, while diving, he found a shiny rock. Bob was very happy\nand wanted to show it to his friends.\nBob went to his friend, the fish, and said, \u201dLook, I found a shiny rock!\u201d His\nfriend, the fish, said, \u201dWow, Bob! That\u2019s a very pretty rock. Can I see it?\u201d Bob\nthought for a moment and said, \u201dNo, it\u2019s mine. I found it first.\u201d\nBob was sad, but he didn\u2019t give up. He went to his friend, the fish, and said,\n\u201dI found this shiny rock. It\u2019s mine now.\u201d His friend was very happy and said,\n\u201dThank you, Bob! You are a good friend.\u201d\nFrom that day on, Bob and his friend shared the shiny rock and played together\nevery day. They were both very happy and had lots of fun.Grammar: 7/10\nCreativity: 5/10\nPlot: 6/10\nConsistency: 4/10\n33M\n2 layersOnce upon a time, there was a big fish named Bob. Bob had a job. His job was\nto dive deep in the water and find food for his friends.\nOne day, while Bob was diving, he saw a shiny rock. He wanted to show his\nfriends the shiny rock. But when he told his friends about the rock, they were\nsad. They did not have any food to eat.\nBob felt sorry for not having food. He told his friends about the shiny rock.\nThey were excited to play with it. They all went to the shiny rock and played\ntogether. They were happy again.Grammar: 7/10\nCreativity: 6/10\nPlot: 7/10\nConsistency: 8/10\nFigure 12: Performance of different models on an instruction-following instance\nmemorization, and what kinds of memorization we want to avoid or detect. We classify three levels of memorization\nas follows:\n16Words : come, road, sad\nSummary : A bus becomes jealous of a shiny new car and undergoes an operation to go faster, becoming good\nfriends with the car and making everyone in the town happy.\nStory :Once upon atime, there wasabigredbus. Thebusloved togofastontheroad. One day, thebussawa\nnewcarontheroad. Thecarwasshiny andnew. Thebusfeltsadbecause itwanted tobeshiny too.\nThebuswent tothecarandsaid, \u201dCan Icome with you?\u201d Thecarsaid, \u201dYes, youcancome with me.\u201d So,thebus\nwent with thecarontheroad. They went very fasttogether. Thebuswassohappy tobewith thecar.\nButthen, some thing unexpected happened. The carstarted togofaster andfaster. The buswasscared. The car\nsaid, \u201dDon\u2019t worry, Iwillhelp you.\u201d The carstopped andthebuswassafe. The busandthecarbecame good\nfriends. They went fastontheroad together andmade everyoneinthetown happy.\nFigure 13: Out-of-distribution generations: stories generated by a model trained on a dataset where the \u201dWords\u201d\nand \u201dSummary\u201d parts do not appear together.\n\u2022Exact memorization: This is the simplest and most obvious form of memorization, where the model simply\ncopies an entire story or a large portion of it from the dataset, without changing anything. This can be easily\ndetected by checking the similarity or the hash of the generated story with the stories in the dataset.\n\u2022Simple template matching: This is a slightly more sophisticated form of memorization, where the model\nchanges some names or entities in a story from the dataset, but keeps the rest of the story the same. For\nexample, the model might change the names of characters, or the location of the story, but keep the plot and\nthe events the same. This can be detected and prevented by measuring the overlap of words and n-grams\nbetween the generated story and the stories in the dataset.\n\u2022Complex template matching: This is the most subtle and difficult form of memorization, where the model\nfollows a more abstract pattern or structure from the dataset, keeping the general plot but changing the details\nand the specifics of the story. This is almost impossible to quantify, as it requires a deeper understanding and\nanalysis of the content and the meaning of the stories, and how they relate to each other.\nWe claim that our models are not doing exact memorization or simple template matching, as evidenced by the\nmethods and metrics we use to evaluate the diversity of the content generated by the models. We rely on several\napproaches:\n\u2022Manual inspection: We generate completions for a range of human-constructed stories. We inspect the stories\ngenerated by the models and check that they are not copies or close modifications of the stories in the dataset.\n\u2022Completion of training stories: We take stories from the training set, truncate them in the middle and generate\nalternative completions with our models. We then compare the completions with the original stories. We\nobserve that the completions are typically very different from the original stories, and often introduce new\ncharacters, events, or twists. This is shown in Figure 14.\n\u2022Diversity of instructions: Recall that in the TinyStories-Instruct dataset, we provide a set of instructions in the\nform of summaries or words contained in the stories, followed by the stories themselves. We can then change\nthe instructions, verify that the combinations do not appear in the dataset and see how the models adapt to\nthe new instructions. We find that the models can generate diverse stories that follow the instructions, even\nif they are novel or challenging, such as requiring the model to fit unlikely words into the story or adding\nfeatures such as a plot twist or a bad ending.\n4.4.1 Quantitative measurement of similarity using Rouge score.\nWe measure the diversity of the stories quantitatively using word and n-gram overlap. We inspect the overlap of\nwords and n-grams between different stories generated by the models, and compare them with the overlap in the\ndataset. We find that the models\u2019 generations have a very low overlap with the dataset, indicating that they are\nnot repeating the same words or phrases. We use the standard Rouge score, for the source text T1, T2with k-gram\nGk(T1),Gk(T2) respectively, the rouge kprecision score is defined as:\nRk,p(T1, T2) =1\n|Gk(T1)|X\nt\u2208Gk(T1)1t\u2208Gk(T2).\n17The Rouge kprecision score measures how many k-grams in T1is included in that of T2. The final Rouge kscore\n(fmeasure) is given as:\nRk(T1, T2) =2Rk(T1, T2)\u00d7Rk(T2, T1)\nRk(T1, T2) +Rk(T2, T1).\nWe perform the following experiment: We randomly pick 100 stories from the training dataset, we cut each story\nin the middle, keeping roughly the first 40%, and use it as a prompt. We ask the model to generate a completion\nfrom each prompt. Let T1, T2,\u00b7\u00b7\u00b7, T100be the generated completions and T\u2032\n1, T\u2032\n2,\u00b7\u00b7\u00b7, T\u2032\n100be the original completion,\nwe measure:\n1. How much of the new generation is contained in the original story (Figure 14), meaning:\nsi:=R2,p(Ti, T\u2032\ni).\n2. How similar are the generated 100 stories to each other (Figure 15), meaning:\nri:= max\nj\u0338=iR2(Ti, Tj)\n3. To what extent are the k-grams in the generated story copied from the training dataset (Figure 16). More\nprecisely, we take Sas the entire training corpus, for each r\u2208 Gk({Ti}i\u2208[100]) we measure\ngr:=P\nq\u2208Gk(S)1gr=q\n|P\nq\u2208Gk(S)|\nIn other words, for each k-gram generated by the model, we measure the frequency that it appears in the\noriginal training dataset, where gr= 0 means that the k-gram never appears in the training dataset.\n4. How similar is the generated story to the closest point, in terms of Rouge precision score, in the entire dataset.\nLetS1, S2,\u00b7\u00b7\u00b7, Smbe all the stories in the training dataset, in Figure 17, we compute\nhi= max\nj\u2208[m]R2,p(Ti, Sj)\nFigure 14: Rogue2 (precision) score between the model\u2019s completion and the original story from the same beginnings\n(we select 100 from the training dataset). We can see that most of the completions that the models generate are\nvery different from the ones in the training dataset (and also not subsampled versions of the original ones).\nFigure 15: Maximum Rouge2 score (fmeasure) similarity between the 100 generated stories for each model. Here\noriginal model means the ones generated by GPT-3.5.\nFor the sake of getting a more concrete impression about how different the model completions are from the\noriginal ending of the story and from other stories in the dataset, in Figure 18 we provide one example of the\noriginal story, the alternative completion by our model together with its closest point in the training dataset.\nThe above points towards several findings:\n\u2022When the model generates stories using a diverse set of prompts, it ends up with a diverse set of completions.\n18Figure 16: Histogram plot of how many times (fraction) each k-gram in the models\u2019 generations also appears in the\ntraining data in log scale (base 10). \u221210 means it never appears. We can see that most of the 4, 5-grams in the\nmodels\u2019 generations do not even appear once in the entire training data.\nFigure 17: Histogram plot for each generated story, the highest rougek score (precision) to the stories in the training\ndataset. We can see that the models\u2019 generations are not copying from any particular story in the training dataset.\nBeginning (prompt) Tim and Jen are playing with their toys in the living room. Tim has a laser that makes red lights\nand sounds. Jen has a doll that she can dress and comb. Tim likes to make his laser point at Jen\u2019s\ndoll and make noises. Jen does not like that. She tells Tim to stop.\n\u201dStop, Tim! You are hurting my doll! She does not like lasers!\u201d Jen says.\n\u201dNo, I am not hurting her. I am just playing. Lasers are fun!\u201d Tim says.\nTim does not stop. He makes his laser point at Jen\u2019s doll more and more. Jen gets angry. She grabs\nher doll and runs to her room. She slams the door and locks it. Tim follows her and knocks on the\ndoor.\n\u201dJen, open the door! I want to play with you!\u201d Tim says.\n\u201dGo away, Tim! You are mean! I don\u2019t\nOriginal story continuation want to play with you!\u201d Jen says.\nTim feels sad. He does not understand why Jen is mad at him. He thinks lasers are fun. He goes\nback to the living room and lays on the couch. He plays with his laser by himself. He points it at\nthe ceiling, the walls, the floor. He does not see the big vase on the shelf. He points his laser at the\nvase. The vase falls and breaks. The pieces fly everywhere. One piece hits Tim on the head.\n(rest of story omitted)\nCompletion by model trained\nonTinyStories (28M)want to play with you or your laser!\u201d Jen says.\nTim does not listen. He keeps pressing the red light on Jen\u2019s doll. He thinks Jen is being nosy and\nrude. He waits for Jen to say something. Then he starts to play with his laser by himself. He makes\nsparks and flashes with his laser. He laughs and runs around the room.\nJen watches Tim from her window. She feels sad. She wishes Tim would play with her. She picks up\nher doll and goes to the door. She shakes the door and tries to open it. But the door does not open.\nThere is a lock on the doorknob.\nJen has an idea. She runs to her room and gets her scissors. She comes back to the living room and\nputs the scissors in the lock. She presses the buttons on the door. She hopes it will open.\n(rest of completion omitted)\nTinyStories training data en-\ntry with maximum similar-\nity to the model\u2019s comple-\ntion (Rouge precision score\n= 0.29)Lila and Tom are playing with their toys in the living room. Lila has a smooth doll with long hair\nand a pink dress. Tom has a horn that makes a loud noise when he blows it. Lila likes to comb her\ndoll\u2019s hair and make her look pretty. Tom likes to make his horn sound and scare Lila.\n\u201dTom, stop it!\u201d Lila says. \u201dYour horn is too loud. It hurts my ears.\u201d\n\u201dBut it is fun!\u201d Tom says. \u201dLook, I can make it sound like a car, or a cow, or a lion!\u201d\nHe blows his horn again and again, making different noises. Lila covers her ears and frowns. She\ndoes not like Tom\u2019s horn. She wants him to be quiet.\n\u201dTom, please shut your horn!\u201d Lila says. \u201dI want to play with my doll. She does not like loud noises.\nShe likes soft music and nice words.\u201d\n(rest of story omitted)\nFigure 18: The closest point in the dataset to an alternative completion\n19\u2022When completing stories from the dataset, the completions usually turn out to be very different than the\noriginal story.\n\u2022Typical k-grams in generated completions rarely appear in the dataset, for values of kas small as 4 or 5.\n\u2022The closest point in the dataset to each generated completion is typically still quite far from it.\nAll the above, taken together with the ability of models trained on TinyStories-Instruct to successfully follow sets\ninstructions which we can easily be verified to be disjoint from the dataset (for example, combinations of words\ncan be checked), provides strong evidence that our models produce genuinely novel and diverse stories, rather than\nsimple variations of existing stories.\nWe remark that nevertheless, we are not able to completely rule out the possibility that the models perform\ncomplex template matching, as it is hard to define and measure what constitutes a novel plot or a novel story.\nWe acknowledge that this is a limitation of our evaluation. Another possibility is that the stories in the dataset\nessentially span the entirety of support of the distribution in the (weak) metric of complex template matching.\n5 Interpretability\nUnderstanding the inner workings of deep neural networks and language models in particular is a major challenge\nin this field of study. For example, it is often difficult to assign a specific function to a given component of a\nneural network. This may be because, contrary to our intuition based on human-designed programs, the network\ncomponents may not have distinct roles, but rather interact in a complex and messy way. In this section, we present\nsome preliminary evidence that training smaller models on TinyStories leads to higher interpretability, suggesting\nthat when networks are constrained in size, we may be able to gain some insights into their internal mechanisms.\nWe focus on two aspects of the model: the attention heads and the neurons in the MLP.\nAs this is not the main focus on our paper, this section is by no means exhaustive and much more work is\nrequired in order to reach more conclusive findings. Rather, we only give some preliminary evidence which may\nhopefully motivate future work.\nAttention heads. In the study of attention heads, we take advantage of the fact that we were able to train a\nvery shallow model (having only one transformer block) which still manages to generate meaningful text. Since\nthe model has only one layer, the attention heads are directly responsible for generating the output tokens, and\nthus they may have more interpretable functions than in deeper models. We use the method of Voita et al [30] to\nanalyze the attention patterns of the heads and classify them into different types, such as positional, syntactic, or\nsemantic. We also use the method of Clark et al [6] to visualize the attention maps of the heads and inspect their\nbehavior on specific examples.\nOur findings suggest that the attention heads exhibit diverse and meaningful functions, such as attending to the\nprevious word, the subject of the sentence, the end of the sentence, or the main topic of the story. We also observe\nthat some attention heads specialize in generating certain types of words, such as nouns, verbs, or punctuation.\nThese results suggest that the attention heads learn to perform different linguistic tasks and capture different\naspects of the stories.\nNeurons in the MLP. We also give some initial evidence that in smaller models, some neurons in the MLP\nhave roles that are interpretable by humans. We use the method similar to [18] to identify the most influential\ntokens in the MLP for each neuron. We find that some neurons are activated on words that have a specific role\nin the sentence (such as the subject or the action), or in the story (such as the introduction of the protagonist).\nThese findings suggest that the neurons in the MLP learn to encode different semantic and stylistic information\nand influence the generation process.\n5.1 Interpreting the role of different attention heads\nTo understand the model\u2019s attention pattern after training, we use a 1-layer model with hidden dimension 1024\nand 16 attention heads that was trained on TinyStories . We visualize the attention patterns that it produces when\nprocessing the following paragraph (the bold form is the prompt, the highlighted text is generated by the model):\nOne day, Lucy asks Tom: \u201dI am looking for a banana but I can\u2019t find it\u201d. Tom says: \u201dDon\u2019t worry, Iwillhelp you\u201d.\nLucy andTom gotothepark. They look forthebanana together. Afterawhile, they found thebanana. Lucy ishappy. Shesays:\n\u201dThank you, Tom. Youareagood friend.\u201d Tom: \u201dYou arewelcome, Lucy. Iamhappy tohelp you. Let\u2019s eatthebanana together!\u201d\n20There seems to be a clear separation between heads with attention pattern based mainly on the distance between\ntokens, and heads whose attention pattern has a stronger dependence on the semantic meaning:\nDistance based attention. Out of the 16 attention heads, we observe multiple positional-based attention heads,\nsuch that each token attends to tokens with a prescribed relative distance. Different heads are associated with\ndifferent distances.\nFigure 19: Multi-scale distance-based attention.\nSemantic based attention. We also observe that there is (1). one head that the word \u201cthe\u201d and \u201ca\u201d all attend\nto the word \u201cbanana\u201d, interestingly, the \u201cthe\u201d at \u201cthe park\u201d also attends to \u201cbanana\u201d, but the model still manage\nto generate \u201cpark\u201d, which is the consistent completion. (2). Another attention head gives a pattern where the\ntokens \u201cthe\u201d and \u201ca\u201d all attend to \u201cpark\u201d. (3). There is third head that most of the words attend to the name of\n\u201cTom\u201d and \u201cLucy\u201d.\nWe remark that it makes sense that the generation of words like \u201cthe\u201d, \u201ca\u201d, \u201cand\u201d or \u201c,\u201d would be induced by\ndistance-based, local attention heads, since those are tokens with a grammatical role which depends on the short-\nrange interactions within a single sentence. On the other hand, the main entities in the story such as \u201cbanana\u201d,\n\u201cpark\u201d, \u201cLucy\u201d and \u201cTom\u201d cannot usually be predicted (as a next token) only based on the neighboring tokens,\nwhich is why the model needs to use semantic attention heads for their generation.\nFigure 20: Semantic attentions according to (1), (2), (3).\n215.2 Interpreting the roles of different Neurons\nIn order to examine whether neurons have meaningful roles, we follow [18], and visualize the most significant tokens\nfor each neuron. More precisely, we take a collection of 20 stories (about 8,000 tokens) from our dataset. We take\na model that was trained on TinyStories , we pick a transformer layer, and from the MLP associated with it we pick\none coordinate in its intermediate layer. We refer to such a choice as a neuron . We process the collection of stories\nwith the model to obtain their internal representations, which gives us an activation value for each combination of\ntoken and neuron. Then, for each neuron we look at the tokens with highest activations from the entire collection.\nWe highlight those tokens in red (and present them along with the sentence they are contained in). We repeated\nthis for two models: a small model of hidden dimension 64 and 1M parameters, trained on TinyStories (Figure 21),\nand on GPT2-XL (Figure 22).\nIn the 1M-parameter model trained on TinyStories , Figure 21 first presents the activated tokens for the first two\nneurons in the before-last layer10. Note that, since the architecture is invariant to permutations between neurons,\ntaking the two first neurons is the same as taking an arbitrary choice of two neurons, the point being that these\nneurons are neither unique nor have been cherry-picked . We see (top row of the figure) that each of those\nneurons is activated on tokens with a common role (one is activated on pronouns which are also the subject in the\nsentence, and the other is activated on the action in the sentence). In addition, we present the activated tokens for\nthe first neuron in another layer (layer 6), where the neuron is activates only on adjectives. Finally, we picked the\nneuron which has the largest activation values over all combinations of token and neuron. This neuron (depicted in\nthe bottom right) seems to have the role of identifying the first time that the protagonist of the story is presented.\nFor comparison, Figure 22 presents the activated tokens for first two neurons of layer 12 for GPT-XL, a much\nlarger neural network. In this case, none of the two neurons seem to have an apparent role.\nLayer #7, Neuron #1\nMom and Dad smiled and said, \u201dWe thought you would\nlike this better, Lily!\u201d\nTim said, \u201dI know it\u2019s yummy, but I don\u2019t want my tummy\nto hurt.\nHer mom said, \u201dI don\u2019t know, Lucy.\nDad said, \u201dI turned off the water to fix a pipe.\nSam thought for a moment and said, \u201dI think I left it in\nthe kitchen.\u201d\nSam said, \u201dI\u2019m sorry I lost it.Layer #7, Neuron #2\nThe bird flew up to the tree and tried to push the ball out.\nShe kicked it and ran after it, laughing.\nShe pushed and pulled, but the box would not open.\nThey both pushed and pulled, but the tough box still did\nnot open.\nThen, she saw her friend Tom come to the park.\nShe found her toy box and pushed it to the shelf.\nLayer #6, Neuron #1\nThey went home and shared the delicious apple.\nShe did not like the mean king.\nThe duck did not like the sm elly pond.\nThe new pond was not smelly.\nLucy loved to play outside under the big sky.\nHe suggested, \u201dLet\u2019s play a game to forget the scary wind.\u201dLayer #7, Neuron #54\nOne day, a girl named Amy wanted to have a fun day with\nher friends.\nOnce upon a time, there was a modest girl named Sue.\nOn the mountain, there was a small boy named Tim.\nOne day, a girl named Sue found a big, tough box.\nOnce upon a time, in an ancient land, there lived a little\nfrog named Freddy.\nFigure 21: Tokens which induce high activations to different neurons, for a small model trained on TinyStories .\n6 Exploring architectures and hyperparameters for NLP with TinyS-\ntories\nOne of the main challenges in developing large language models (LLMs) comes from the high computational cost\ninvolved in training. Finding the best architectures, training algorithms and hyperparameters for LLMs requires a\nlot of resources and experimentation. Therefore, it would be useful to have a smaller and simpler dataset that can\nstill capture some of the basic capabilities of LLMs, and allow us to study how different design choices affect their\n10The rationale behind choosing the penultimate layer is that tokens have already been processed by most layers at this point. We\ntake the before-last rather than the last layer since the hidden representation in the last layer only has only the role of predicting the\nnext token, so information may be lost at that point.\n22Layer #13, Neuron #1\nIt\u2019s not safe to play in the fog.\nShe liked to keep her toys and books in the right place.\nOnce upon a time, there was a gr umpy nurse.\nShe was quiet and kind.\nThe gr umpy nurse liked Lily very much.\nShe called her friends on the phone and said, \u201dHi!\nSally was tired from playing, so she went inside.\nLucy was very upset.Layer #12, Neuron #2\nMia looked at Worry and said, \u201dMy dad is in the navy.\nSo, the wise fish told the little fish to mark a spot in the\nsea where they could meet every day.\nThe wise fish told the little fish, \u201dAlways remember the\nspot you mark and never forget to learn new things.\u201d\nSuddenly, the dinosaurs came to life! But the dinosaurs\nwere nice and just wanted to play.\nHe was stuck in the mirror world with Jim and could not\ngo back to his mom and dad.\nFigure 22: Tokens which induce high activations to different neurons in GPT-XL\nperformance. TinyStories is such a dataset, as it enables us to train and evaluate LMs that are orders of magnitude\nsmaller than the state-of-the-art models, yet still have the basic capability of producing coherent text.\nIn this work, we take the first steps towards using TinyStories as a testbed for exploring architectures and\nhyperparameters for NLP. We show that our small models exhibit some similar patterns to the ones observed in\nLLMs in certain aspects. In particular, we investigate two questions: how to balance model size and learning budget\nfor a fixed amount of training flops, and how to choose the number of attention heads for a given model width and\ndepth.\nModel size versus the training FLOPs. For a fixed amount of training flops, there is a trade-off between the\nsize of the model and the number of training steps (the total number of flops is the product of both). Previous\nworks [16, 11] have shown that there is a polynomial scaling law between model size and learning budget for LLMs,\ni.e., the optimal model size for a given amount of flops is proportional to the flops raised to some power \u03b1 > 1.\nHowever, these works used different ranges of model sizes (from a few million to tens of billions of parameters) and\nfound different values of \u03b1(around 0.7 and 0.5, respectively). A natural question is whether this scaling law is\nuniversal or depends on the dataset. Our dataset allows us to conduct a similar experiment but with much smaller\nmodels and flops. Surprisingly, we find evidence for a polynomial scaling law as well, which suggests that there\nmight be a universal phenomenon here.\nWe train models of various sizes and architectures on TinyStories . For each amount of flops, we select the model\nand the number of training steps that achieve the lowest validation loss among the possible combinations. We vary\nthe number of layers from 2 ,4,8,12 and the hidden dimension from 64 ,128,256,512,768,1024,2048. The result is\nshown in Figure 6. Although the number of points may be a bit small for the data to be very conclusive, the plot\npoints to a polynomial dependence.\nChoosing the number of heads. Another design choice for transformers is the number of attention heads for\neach layer. It is not obvious how the number of heads affects the performance of the model, given a fixed model\nwidth and depth. Our results, shown in Figure 24, suggest that in the regime where the number of heads is small,\nincreasing it improves the performance of the model across all metrics.\n7 Related Works\nGenerative language models (LMs) have achieved impressive results in various natural language processing tasks,\nsuch as text summarization, dialogue generation, and story completion. However, most of these models are very\nlarge, with hundreds of millions or even billions of parameters, which poses significant challenges for training, infer-\nence, and deployment. For example, GPT-3 [4], one of largest LM to date, has 175 billion parameters and requires\nhundreds of petaflops of compute to train. Smaller models, such as GPT-2 small with 125 million parameters, can\nhardly generate coherent and consistent sentences beyond a few words, even after extensive pre-training on large\ncorpora [23].\nSeveral methods have been proposed to compress or distill large LMs into smaller ones, such as knowledge\ndistillation [10, 2], pruning [8], and quantization [13]. However, these methods are much more effective for BERT-\nlike models [25, 28], which are designed for masked language modeling and downstream classification tasks, than\nfor GPT-like models, which are designed for autoregressive language generation [26].\nAnother challenge for generative LMs is the evaluation of their outputs. Unlike BERT-like models, which can\nbe fine-tuned and evaluated on downstream tasks with labeled data, GPT-like models are more difficult to measure\n23Figure 23: The scaling law of the best model versus the total number of training flops.\nHidden size Layer Head Eval loss Grammar Creativity Consistency\n768 2 2 1.38 7.77 6.5 7.78\n768 2 4 1.34 8.05 6.57 8.16\n768 2 8 1.33 8.25 6.53 8.16\n768 1 2 1.58 7.13 5.83 6.38\n768 1 4 1.56 7.43 5.90 6.75\n768 1 8 1.54 7.45 6.28 7.02\nFigure 24: Model performance with different number of attention heads\nin terms of how well they can \u201dspeak and understand natural language\u201d. Most existing benchmarks for generative\nLMs, such as LAMBADA [22], CLOZE [29], TriviaQA [15], and Winograd Schema Challenge [17], require the\nmodels to produce a single word or a short phrase as the answer, which does not capture the richness and diversity\nof generating natural language. Moreover, these benchmarks are often limited by the size and quality of the datasets,\nthe ambiguity and subjectivity of the answers, and the lack of human evaluation. Larger and more diversed datasets\nsuch as the BigBench [27] are simply way too complicated for SLMs. Some other benchmarks, such as WikiSQL [33],\nhave a more structured output format, which makes them easier to evaluate, but also less representative of natural\nlanguage generation.\nOur work is also beneficial to the theoretical analysis of transformer models and their learning process. Most\nof the existing theory works focus on models with one transformer block, which are easier to analyze than models\nwith multiple blocks. For example, Voita et al [30] showed that one transformer block can learn to perform different\nlinguistic tasks depending on the position of the self-attention layer. Li et al [19] shows a transformer block can\nencode topical models. Jelassi et al [14] shows one transformer block can encode patch associations. Our work\nprovides empirical evidence that one transformer block can also generate diverse and consistent stories, which\nsuggests that the transformer architecture has a strong expressive power even with a small number of parameters\n24and layers.\n8 Conclusion\nIn this work, we have presented TinyStories , a synthetic dataset of short stories that only contain words that a typical\n3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We have shown that TinyStories can be\nused to train and evaluate small language models (SLMs) that are much smaller than the state-of-the-art models,\nyet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect\ngrammar, and demonstrate reasoning capabilities.\nWhile large models trained on the huge and diverse language corpuses on the internet exhibit very impressive\ncapabilities, those datasets appear to be too large for SLMs to capture the complex aspects of language. In this work\nwe have argued that TinyStories enables us to observe and study the emergence of capabilities such as generation\nof coherent text, reasoning and instruction following in LMs on a much smaller scale, in terms of the size of both\nmodel and dataset. By training SLMs on our dataset, we have also observed many behaviors similar to LLMs such\nas scaling laws, trade-offs between width and depth, etc. Moreover, we have shown that the trained SLMs have\nmuch higher interpretability than larger ones, and that we can visualize and analyze their attention and activation\npatterns to understand how they generate and comprehend stories.\nWe provided evidence to the fact that the models trained on TinyStories are able to produce genuinely new\nstories, rather than just copying chunks of text the dataset. It remains a challenge, however, to assess the true\nextent of the \u201dcreativity\u201d of our models, and to which the models reflect a certain \u201dunderstanding\u201d (on a very\nlow level of course) of the stories that they produce as opposed to just template matching to create a plausible\ncontinuation. We hope that this dataset can be used in future works to obtain insights about the degree of creativity\nof language models.\nWe have also introduced a new paradigm for the evaluation of language models, which uses GPT-4 to grade the\ncontent generated by these models as if those were stories written by students and graded by a (human) teacher.\nThis new paradigm overcomes the flaws of standard benchmarks, which often require the model\u2019s output to be very\nstructured, and moreover provides a multidimensional score for the model, providing scores for different capabilities.\nWe believe that this paradigm can be useful much beyond TinyStories .\nFinally, we have presented initial findings which point to the roles of width vs. depth in the intellectual\ncapabilities of generative networks, which suggest that width is more important for capturing factual knowledge\nwhereas depth is more important for contextual tracking. Moreover our findings suggest that in terms of emergence,\ngrammatic and syntactic abilities appear earlier than the ability to produce consistent text, which in turn appears\nahead of ability to generate content that would be considered as creative. These preliminary findings are only\nsuggestive (and have not been the main focus of this work) but they show how our dataset and evaluation paradigm\ncan enable more fine-grained analysis of the emergence and evaluation of various language capabilities in generative\nmodels.\nWe hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource\nor specialized domains, and shed light on the emergence of language capabilities in LMs. A general question that\narises from this work is whether synthesizing a refined dataset can be beneficial in training networks for practical\nuses. For example, perhaps it is possible to train a customer service chatbot by synthesizing a large dataset of\nhypothetical calls.\nReferences\n[1] Common crawl. Accessed: 2019.\n[2] Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and self-distillation\nin deep learning. arXiv preprint arXiv:2012.09816 , 2020.\n[3] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive\nLanguage Modeling with Mesh-Tensorflow, March 2021.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems , 33:1877\u20131901, 2020.\n25[5] S\u00b4 ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter\nLee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments\nwith gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\n[6] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look at? an\nanalysis of bert\u2019s attention. arXiv preprint arXiv:1906.04341 , 2019.\n[7] John H Flavell, Eleanor R Flavell, Frances L Green, and Louis J Moses. Young children\u2019s understanding of\nfact beliefs versus value beliefs. Child development , 61(4):915\u2013928, 1990.\n[8] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks.\narXiv preprint arXiv:1803.03635 , 2018.\n[9] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace\nHe, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027 , 2020.\n[10] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531 , 2015.\n[11] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 , 2022.\n[12] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\narXiv preprint arXiv:1904.09751 , 2019.\n[13] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized neural\nnetworks: Training neural networks with low precision weights and activations. The Journal of Machine\nLearning Research , 18(1):6869\u20136898, 2017.\n[14] Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. Advances\nin Neural Information Processing Systems , 35:37822\u201337836, 2022.\n[15] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\nchallenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017.\n[16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361 , 2020.\n[17] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth\ninternational conference on the principles of knowledge representation and reasoning , 2012.\n[18] Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. Visualizing and understanding neural models in nlp.\narXiv preprint arXiv:1506.01066 , 2015.\n[19] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic\nunderstanding. arXiv preprint arXiv:2303.04245 , 2023.\n[20] Wick Miller and Susan Ervin. The development of grammar in child language. Monographs of the Society for\nResearch in Child Development , pages 9\u201334, 1964.\n[21] OpenAI. Gpt-4 technical report, 2023.\n[22] Denis Paperno, Germ\u00b4 an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00b4 andez. The lambada dataset: Word prediction\nrequiring a broad discourse context. arXiv preprint arXiv:1606.06031 , 2016.\n[23] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\n26[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The\nJournal of Machine Learning Research , 21(1):5485\u20135551, 2020.\n[25] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.\n[26] Michael Santacroce, Zixin Wen, Yelong Shen, and Yuanzhi Li. What matters in the structured pruning of\ngenerative language models? arXiv preprint arXiv:2302.03773 , 2023.\n[27] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adri` a Garriga-Alonso, et al. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 , 2022.\n[28] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact\ntask-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984 , 2020.\n[29] Wilson L Taylor. \u201ccloze procedure\u201d: A new tool for measuring readability. Journalism quarterly , 30(4):415\u2013433,\n1953.\n[30] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention:\nSpecialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418 , 2019.\n[31] Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00b4 an, Armand\nJoulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. arXiv\npreprint arXiv:1911.00359 , 2019.\n[32] Terry Winograd. Understanding natural language. Cognitive psychology , 3(1):1\u2013191, 1972.\n[33] Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural\nlanguage using reinforcement learning. arXiv preprint arXiv:1709.00103 , 2017.\n27"
}