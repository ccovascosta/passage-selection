{
    "filename": "LLM (Large Language Model) inference multi-cloud support.docx",
    "type": "docx",
    "title": "",
    "authors": "Jie Tong",
    "lastmodifiedtime": "2024-06-27T22:57:11+00:00",
    "text": "infe\nProject Nexus (LLM Inference Multi-cloud Support)\nAn AzureML-Arc based solution\nThis document will be revised based on the current Nexus architecture and components (6/16/23).\nMotivation\nLarge language models are demonstrating their powers in many different domains \u2013 e.g., code generation (co-pilot), QnA (ChatGPT) - and it is just the beginning. In the meantime, the serving of these LLMs require a tremendous amount of GPU capacity, which might not be even fulfilled by a single cloud provider due to the supply-chain constraints. In the near future WebXT needs a significant batch of A100 GPUs that beyond what Azure can provide, so the team is looking into whether it is feasible to leverage the capacity from other cloud provider(s) and what is the right software stack for that to work. This document is describing a proposal based on AzureML\u2019s multi-cloud/on-premises solution.\nRequirements\nCluster setup\nCloud provider cluster setup\nAzureML Arc compute setup \nMulti-node model hosting\nNode group level health probe\nNode group management (group allocation, re-grouping, etc)\nGroup level traffic management\nDeploymentFlow integration\nPreset deployment workflow integration\nQuota Management\nReliability and efficiency\nCustomized group scheduling (UD/FD)\nNode group Gracefully shutdown\nDeployment resilience (bad node)\nTransparent Infra components update/vulnerability fix\nBad node detection/eviction/replacement\nSecurity and Network\nSecure network between other cloud and Azure\nData Exfiltration Prevention\nPull IPP model/container image in other clouds (model/container will be hosted in Azure blob/ACR which are connected as private endpoints)\nSystem/Container events auditing\nMonitoring/observability\nEyes-off mode DRI support\nLogs/Metrics (especially group level metrics like capacity, etc.)\nTerminology\n\nHigh-level Architecture\n\n\n\n\n \nControl Plane\n\n\nNetwork Security\n\nFrom a network perspective, the design contains the following network entities.\nOCI vcn.  It is connected to Azure vnet via ExpressRoute and FastConnect.\nAll outbound and inbound traffics are disabled by default. \nIt can only access the peered Ainzure vnet via ExpressRoute/FastConnect connection.\nAzure vnet. It is connected to OCI vcn via ExpressRoute and FastConnect. NSG rules need to be applied according to the following network requirements.\nK8s control plane is installed in this vnet. \nAzure Arc agent is installed in this vnet.\nAzure Arc - Network requirements\nData exchange between cluster and Azure\nAzureml extension is installed in this vnet.\nNetwork requirements  For system log, we have different design and will not use kusto.\nAzureml Arc extension creates a private endpoint of Relay service for control plane.\nAzureml IDP calls to the relay server via public address with trusted service firewall rule.\nAzureml Arc extension creates public load balancer for Mesh on data plane. This public load balancer needs NSG rules to allow requests from FrontDoor.\nwaAzureml workspace creates a private endpoint in this vnet.\nIdentity Proxy calls Azureml identity bridge via this private endpoint.\nHow model and image is pushed to staging area in this vnet is TBD.\nNSG rules for Azure vnet needing below service tags:\nInfra images download from MCR: MicrosoftContainerRegistry, AzureFrontDoor.FirstParty\nCluster certificate download from keyvault: AzureKeyVault, AzureActiveDirectory\nOpen Questions for Security Review\nIn control plane and data plane, we would like to confirm if the following design is allowed from security perspective.\nRelay is used for communication between AzureML deployment plane (DP) and AKS cluster. We can create a private endpoint for the relay service in AKS vnet. From DP to relay service, we will disable public network access and enable trusted Microsoft services to bypass firewall.\nIn data plane, a public load balancer will be created in AKS vnet for FrontDoor to call mesh in AKS.  This is consistent with the current MIR design. \nPull model/image/key from a scoped registry. There will be a separate user assigned identity which has access to the Blob/ACR/KV in scoped registry.  \nWho\u2019s responsible for encrypting model files, and setting up the UAI that can access the models/images/kvs? \nIs it allowed to pull the storage account(via PE), container registry(via PE) and KV from Nexus AML boundary?\nIf the \u201cscoped registry\u201d proposal is approved, we will be in charge of publishing/deleting models from each scope?\n\nData Plane\n\nMIR Frontdoor\nUser will send traffic to frontdoor, the endpoint/deployment user aware is MIR endpoint/deployment.\nWe need to pass the endpoint and deployment info from MIR frontdoor to Mesh so that Mesh has enough information to route the traffic.\nAdd new deployment property \u201cComputeType\u201d, if this endpoint is created for ARC, then set this property to \u201camlarc\u201d\nFor amlarc endpoint, apply the following changes:\nRewrite request Url to include endpoint name like this /api/v1/endpoint/{endpoint_name}/\u2026\u201d, this is the request url format Mesh can handle.\nSet request header \u201cazureml-model-deployment\u201d with the real mir deployment name.\nValidate ARC cluster Cert, one ARC cluster will have only one cert, so this will be different from the endpoint level cert, we can use subscription as cert scope:.\nCertsubject name: \nMesh\nMesh detail design spec could be found here at Nexus_dataplane_in_cluster.docx\nMesh should have a public SLB so FD can forward request to it. Use the NSG and mTLS to verify the traffic between FD and Cluster.\nDisable Auth since auth is already done by FD.\nEnable mTLS\nFD cert SAN: {region}.inference.ml.azure.com\nARC cluster cert SAN: \nHeader based routing (already supported)\nAlways route to Rank-0\nRank-0 pod should has label \u201crank-0\u201d \nSticky routing(align with MIR)\nPod level sticky routing (refer to MIR)\nResponse code rewrite (align with MIR)\n429 (503 in scoringfe)\n424 (502 in scoringfe)\nMIR endpoint/deployment mapping with ARC endpoint/deployment\nRequirement for distributed inference:\nSet all deployments pods with label rank info:\nFor example:  rank: 0\nFor non distributed inference, rank: 0 needs to be added as well.\nFor distributed training, rank 0 becomes healthy when other ranks are healthy. \nRequirements for Preset:\nWhen creating online deployment, register the ScoringFE service IP info to the frontdoor. And endpoint type. \nScoringfe to model worker with http in cluster (align with MIR)\nEnvoy metrics publish to geneva, currently we only have QPM and latencies, need to figure out the gap and add the missing metrics\nScoringfe forward request to CMP port (not model port) if CMP is enabled\nMesh certificate management\n\nScenarios when installing an extension:\n(manual)Generate certificate with ARC cluster cert SAN manually by using Keyvault in Vienna infra regional subscription with Name using cluster name.  (JIT by AME account)\n(manual) Get the AML extension subscription ID and add the subscription ID to identity bridge config whitelist. (JIT by AME account)\n(manual)Install the amlarc extension by sslSecret parameter with using a test certificate, keyvault name and certificate name.\nsslSecret : The name of the Kubernetes secret in the\u00a0azureml\u00a0namespace. This config is used to store\u00a0cert.pem\u00a0(PEM-encoded TLS/SSL cert) and\u00a0key.pem\u00a0(PEM-encoded TLS/SSL key), which are required for inference HTTPS endpoint support when\u00a0allowInsecureConnections\u00a0is set to\u00a0False. Details could be found here. \n(manual)Get the amlarc extension client id and add it to the identity bridge config whitelist managed identity and assign the kevvault the keyvault reader access. (key vault and the extension needs in the same tenant(AME).)\n(Automation) CertGetter in amlarc extension in the cluster to download the certificate from the Keyvault every 24 hours by using the extension msi by calling identity bridge. Identity bridge verifies the extension msi and sends related certificate.  the keyvault. The service checks the certificate whether it is changed. If it is changed, it will update the sslSecret secret (See above chart).\n(Automation)The Keyvault will update the certificate when it is near expiration and will be downloaded and updated to sslsecret secret in the cluster.\nWork to do:\nUpdate identity bridge to support the auth basing on extension token and the config whitelist\nAdd CertGetter in gateway in extension to support downloading the certificate and update the k8s secret.\nScenarios when installing the helm chart:\nInfra team (one time work): Config the subscription(which holds the keyvault) in the onecert with certificate domain name: nexus.<location>.cloudapp.azure.com.\nInfra team: Create Keyvault which apply a certificate with\nSubject: <cluster-name>.nexus.<location>.cloudapp.azure.com\nSAN: \nDNS Name= <cluster-name>.nexus.<location>.cloudapp.azure.com\nDNS Name=*. <cluster-name>.nexus.<location>.cloudapp.azure.com\nDNS Name=*. SubscriptionGUIDshortRegion\nInfra team: Assign cluster identity to the keyvault reader access. \nInfra team: Install nexus by parameter keyvault name, cluster name, region\nApp team: CertGetter in the cluster to download the certificate from the Keyvault every 24 hours by using the cluster msi by calling the keyvault. The service checks the certificate whether it is changed. If it is changed, it will update the k8s secret.\nNotes: The Keyvault will update the certificate when it is near expiration and will be downloaded and updated to sslsecret secret in the cluster.\nCMP\nUpdate user model deployment spec to include cmp as side-car\nCMP can get UAI (for RAI access) token (from identity proxy -> identity bridge?)\nCMP can access RAI endpoint (proper NSG config) \nWhere to pull CMP image? From MCR? Is there security concerns for DV3?\nModel Data Collection (MDC)\nAlign with MIR\nDetails could be found at here.\nModel Data Collection (MDC) (preview)\u00a0supports three data sink options:\nWorkspace Blob Storage: Use workspace private endpoint\nCustom Blob Storage URI: To check. \nEventHub\uff1a Eventhub service tag needed if using Eventhub. \nBandwidth Usage\nMIR frontdoor dashboard\n\n\n\n\nPrivate Endpoint Option (Optional)\nIf public SLB on AKS cluster passes security review, then we will take public SLB option. Otherwise, this private endpoint option will be used to make the traffic through private link from MIR Frontdoor to Mesh.\n\nThe flow to manage the private endpoint is:\nIn ARC cluster creation flow\nCreate internal load balancer for Mesh\nCreate private link service for intenral load balancer, get the private link service resourceId\nCreate PE for all reginal Mir frontdoors (use which identity? How to manage the FD list?)\nGet the PE private IP, this IP will be used later for deployment (where to store this IP?)\nIn online deployment creation flow\nGet the PE IP that online deployment will be created on, and set the IP as target IP in frontdoor route table\n\nData exfiltration prevention\nDV3 has a very high-standard security requirement to make sure the model & image\u2019s safety, in MIR, this is done by using Managed Vnet with very limited outbound whitelist(NSG rules):\nTented ACR & Storage\nLogs -> Geneva Ingestion Gateway (GIG) \nMetrics -> Runhistory (via PE)\nCertificate -> AML service (via PE)\nToken\nAML service (for online endpoint identity, via PE)\nIMDS (tented identity, via azure proxy vm)\nFor AML Arc, we need to think about what outbound connections are required and need to have them reviewed with DV3 security team.\nAML Arc outbound connections:\nAzure Relay\nAzure Event hub\nAzure Arc (network requirements)\nNetwork requrements\nAzureML Extension\nNetwork requirements\nAzure-Arc \nNetwork requirements\nData exchange between cluster and Azure\nPreset Flow for AMLArc\nSee https://microsoftapc-my.sharepoint.com/:w:/g/personal/mingj_microsoft_com/Ed7PvVayyL5NrJAs75jV8ncBcbK2jBkp6EnG0rFRL05V-A?e=Ejvucy\nModel & Image Download\nWe are adopting a push model where models/weights are \u201cpushed\u201d from tented registry/blob into third-party worker pool\u2019s private network.\n   Questions:\nWhich service should be responsible for syncing the model/image from Azure to local cluster storage, when the sync is triggered?\nHow many models/images need to be maintained in local storage, what the estimation total storage size is, especially, how to retire old versions, manually or automatically delete oldest version?\nHow is CMP image downloaded? From MCR or local image registry?\nAre the local storage nodes stable? Does the membership need to be dynamically updated frequently?\nHow many models are to be cached, and how to maintain, especially, how to retire old versions?  - At lease 4-5 version(1.2T per version)\nIs it safe that when keep the model files / images on a node after the deployment is removed? - Not a must.\n\u2026\n There are 3(or could be more) potential paths to push down the needed model and weights:\nArtifact push service directly pushes data to in-cluster cache within the 3rd party network boundary. Deployment pulls data from in-cluster cache.\nArtifact push service pushes data to a Staging area in Azure vNet boundary, and a service pushes data down(or sync-ed from) to in-cluster cache within the 3rd party ntework boundary. Deployment pulls data from in-cluster cache.\nArtifact push service pushes data to a cache in Azure vNet boundary, and deployment pulls data from in-Azure cache.\n\nOption 1: potential security concerns\nFor Option 2, one potential implementation mechanism is to use a MIR cluster as a proxy\n\nWe will pre-configure a MIR cluster which has permission to download the model the cluster. This MIR cluster will be vnet-peering with AmlArc cluster. When user deploy new deployment to the AmlArc cluster, the workflow will check if the model exists on the MIR cluster and do file sync to get the model from the MIR cluster.  \n\n\nProposed storage solution\nModel & Image publish\nTBD\nImage download\nImage is downloaded from the asset registry service by kubelet directly. The asset registry follows the OCI distribution spec.\nModel download\nUser pod has a PVC pointing to the required model version.\nThe corresponding PV should be created when the model is published to the OCI cluster.\nThe PV is attached (downloaded actually) to the node where the user Pod is scheduled. The download action is triggered by CSI controller plugin when it receives ControllerPublishVolume RPC call and is done by CSI node plugin.\nCSI node plugin is a Daemonset running on every worker node and acts as the download client, it first tries to resolve the model manifest from the cluster asset registry and get the full list of storage items to download, then it starts the download in parallel from the local storage cluster.\nModel & Image reconcile / repair\nThe asset push service in Azure should periodically poll the cluster asset registry service to check whether there\u2019s any asset broken and try to push it again.\nThe asset registry service in the cluster should maintain the latest health state of each asset in case the underlying storage cluster.\n\nMetrics and Logging\nOCI-Network-Logging-Metrics-Review.docx\nThis table lists the metrics/logs currenlty used by DV3 (refer to The DV3 dashboard in MIR):\n\n \nIdentity\nIdentity On Nexus.docx\nHealth Monitoring and Repair\nWe leverage Compute Health Manager Draft Design.docx  for node problem detection and analysis.  In this session, we will focus on compute specific logic that includes workload precheck, workload repair, and node repair.\n\nTwo new services will be added:\nClusterHealthManager: this service is responsible for workload repair, including cordoning bad nodes and draining workloads. It also acts as a node repair history provider, storing nodes' repair history in configmaps. Additionally, it monitors the health manager\u2019s heartbeat of each node and node not-ready events to prevent situations where nodes are unable to report their unhealthy status themselves.\nNodeProblemResolver: running on each node to perform node repair. for hot repair and reboot, we prefer to do it locally; for action such as reimage and re-provisioning, the repair will be routed to a compute service like AKS or OCI.\n  \nPrecheck flow:\nPrecheck is to ensure the node is in good status before user container is started. Node precheck will be triggered inside the init-container of the user pod. If the precheck fails, workload manager will take immediate action by cordoning the node and rescheduling the pod on other nodes.\n\n\nRepair flow:\nAzure core team might provide a new API to wrapper external compute related operations in the future, details have not yet come out, so in current design, we would call into OCI layer for node repair directly. \n\n\n\nHealthManager currently requests four typical repair actions from NodeProblemResolver: hotrepair, reboot, reimage, and reprovision. For hotrepair, the action can be performed locally. However, for the other repair actions, we need to route the requests to OCI cloud service:\nAuthentication:\nOCI supports Instance Principal Authentication. After setting up required resources and policies, NodeProblemResolver running on each OCI instance can call Oracle Cloud Infrastructure public services, without the need to configure user credentials or a configuration file; This would require some additional steps during cluster setup to authorize OCI workers to make OCI API calls.\nPrivate access: \nTo ensure that this traffic does not go over the internet, we probably need to setup service gateway.\n\nAdditionally, if a node is in such bad health that it cannot trigger the repair itself, we will rely on ClusterHealthManager from Azure infra node to redirect the request to some other healthy node. We will let another worker make the call to OCI API service, ensuring a reliable repair process. as shown in the following diagram\n\nQuestions:\nWill node management be supported by AKS service in the near future?\nOCI does not support reimage right now, looking into alternative solutions that can achieve a similar outcome.\n\n\nAppendix\nWork item breakdown:\nK8s compute setup & Networking\nConnect customer vnet and OCI VCN via ExpressRoute&FastConnect\nCreate PE to arc, workspace, tented acr/blob(??).\nRelay or proxy for DP->k8s cluster communication.\nProxy requires code change in MLC and DP\nArc control plane should go through private link\nNode Health manager\nNode repair operator for OCI compute\nHealth manager to cordon node and drain job\nNode repair config & node repair history\nNode grouping\nExpose correct envs for DV3.\nUpdate identity flow to handle tented arc/blob\nHandle scale up and down.\nAdd failure analysis.\nAlways send SIGTERM signal to rank 0 for gracefully shutdown.\nRoute scoring request to rank 0 pod. \nUpdate scoringfe auto-scale logic.\nTelemetry\nService log: Geneva \ngpu/cpu/memory metrics: az monitor\nSupport for Sentinel for system/container events\nPerformance\nShared model\nRedis of OCI\nModel Download/Image pull\nModel/Image cache layer in the cluster\nModel/Image publish solutions\nPresets workflow\nRegistry and preset workflow support for AzureML Arc\nFrontdoor with ScoringFE integration\n\n\n Summary of POC\n\nOverview\n\nConnect Azure network to OCI networks with ExpressRoute/FastConnect\n--\n1. https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/connectivity-to-other-providers-oci\n2. https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/azure.htm\n\nSetup Network\nWe created two virtual networks in Azure and Oracle respectively and connect them with an interconnection with Azure ExpressRoute and Oracle FastConnect. The maximum bandwidth is 10Gbps. \nCreate workspace\nWe created a private AzureML workspace with private resource (storage account, container registry and key vault) and connect them to the Azure virtual networks (VNet) via private endpoint.\nWe also configured the DNS records for the resources in Oracle virtual networks (VCN).\nInstall AMLArc Extension and attach compute\nWe created an Azure Relay connect it to the VNet via private endpoint. We installed AMLArc extension on OKE and attached it to the AzureML workspace via this relay.\nDeploy model\nSubmit request: The user submits the deploy request from a VM in the Azure VNet.\nStart distributed deployment: The AMLArc extension reads number of the workers (nodes) from environment variable of the deployment specification and create workers for the deployment.\nDownload image and model: Every worker downloads the image from the private registry and downloads model from private storage account (blob) to the OKE.\nStart MPI task: AMLArc extension set up SSH environment for every worker and start distributed MPI task on all workers.\nLogs and metrics\nSystem logs are sent to Kusto by The AMLArc extension via public network.\nRunning \"az online-deployment log\" command could get deployment logs.\nPerformance\nWe run following test,\nNetwork bandwidth test\nTransmission from Azure VM to Oracle VM: 977.01Mbit/s\nBlob download test\nDownload from blob to Oracle VM: 532.44Mbit/s\n(Standard, GRS, AzCli 10 maximum-connections)\nThe model downloading relies on,\nStorage Account (Blob)\nAccount type.\nNumber of total connections\nThrottling.\nNetwork.\nAzure Virtual network gateway.\nExpressRoute/FastConnect bandwidth\nOKE node (VM) bandwidth\nFor now, the test is limited by the total network bandwidth of the Oracle VM (0.7Gbps). We are also not sure if multiple interconnections could increase bandwidth.\n"
}