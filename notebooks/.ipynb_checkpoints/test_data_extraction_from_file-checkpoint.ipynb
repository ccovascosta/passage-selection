{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6dced42-74b7-422a-afbe-78a62e2f1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.data_ingestion import load_document\n",
    "from src.preprocess import preprocess_text\n",
    "from src.query_processing import process_query\n",
    "from src.document_retrieval import retrieve_documents\n",
    "from src.passage_extraction import extract_relevant_passages\n",
    "from src.utils import read_config, save_debug_json\n",
    "\n",
    "document_folder_path = '../sample_data'\n",
    "\n",
    "config = read_config('../config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c71fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in folder: ['A Comprehensive Overview of Large Language Models.pdf', 'A Survey of Large Language Models.pdf', 'BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf', 'Getting started with Large Language Models.docx', 'GitHub Copilot - Presentation by vinitshahdeo.pdf', 'Language Models are Few-Shot Learners.pdf', 'Large Language Models for Dummies.pptx', 'Large language models SIMPLIFIED.pptx', 'LLM (Large Language Model) inference multi-cloud support.docx', 'Microsoft-Cloud-for-Nonprofit-Overview.pdf']\n",
      "\n",
      "\n",
      "Reading document: LLM (Large Language Model) inference multi-cloud support.docx\n",
      "Text loaded successfully.\n",
      "First 500 characters of the text:\n",
      "infe\n",
      "Project Nexus (LLM Inference Multi-cloud Support)\n",
      "An AzureML-Arc based solution\n",
      "This document will be revised based on the current Nexus architecture and components (6/16/23).\n",
      "Motivation\n",
      "Large language models are demonstrating their powers in many different domains – e.g., code generation (co-pilot), QnA (ChatGPT) - and it is just the beginning. In the meantime, the serving of these LLMs require a tremendous amount of GPU capacity, which might not be even fulfilled by a single cloud provider due to the supply-chain constraints. In the near future WebXT needs a significant batch of A100 GPUs that beyond what Azure can provide, so the team is looking into whether it is feasible to leverage the capacity from other cloud provider(s) and what is the right software stack for that to work. This document is describing a proposal based on AzureML’s multi-cloud/on-premises solution.\n",
      "Requirements\n",
      "Cluster setup\n",
      "Cloud provider cluster setup\n",
      "AzureML Arc compute setup \n",
      "Multi-node model hosting\n",
      "Node group level health probe\n",
      "Node group management (group allocation, re-grouping, etc)\n",
      "Group level traffic management\n",
      "DeploymentFlow integration\n",
      "Preset deployment workflow integration\n",
      "Quota Management\n",
      "Reliability and efficiency\n",
      "Customized group scheduling (UD/FD)\n",
      "Node group Gracefully shutdown\n",
      "Deployment resilience (bad node)\n",
      "Transparent Infra components update/vulnerability fix\n",
      "Bad node detection/eviction/replacement\n",
      "Security and Network\n",
      "Secure network between other cloud and Azure\n",
      "Data Exfiltration Prevention\n",
      "Pull IPP model/container image in other clouds (model/container will be hosted in Azure blob/ACR which are connected as private endpoints)\n",
      "System/Container events auditing\n",
      "Monitoring/observability\n",
      "Eyes-off mode DRI support\n",
      "Logs/Metrics (especially group level metrics like capacity, etc.)\n",
      "Terminology\n",
      "\n",
      "High-level Architecture\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Control Plane\n",
      "\n",
      "\n",
      "Network Security\n",
      "\n",
      "From a network perspective, the design contains the following network entities.\n",
      "OCI vcn.  It is connected to Azure vnet via ExpressRoute and FastConnect.\n",
      "All outbound and inbound traffics are disabled by default. \n",
      "It can only access the peered Ainzure vnet via ExpressRoute/FastConnect connection.\n",
      "Azure vnet. It is connected to OCI vcn via ExpressRoute and FastConnect. NSG rules need to be applied according to the following network requirements.\n",
      "K8s control plane is installed in this vnet. \n",
      "Azure Arc agent is installed in this vnet.\n",
      "Azure Arc - Network requirements\n",
      "Data exchange between cluster and Azure\n",
      "Azureml extension is installed in this vnet.\n",
      "Network requirements  For system log, we have different design and will not use kusto.\n",
      "Azureml Arc extension creates a private endpoint of Relay service for control plane.\n",
      "Azureml IDP calls to the relay server via public address with trusted service firewall rule.\n",
      "Azureml Arc extension creates public load balancer for Mesh on data plane. This public load balancer needs NSG rules to allow requests from FrontDoor.\n",
      "waAzureml workspace creates a private endpoint in this vnet.\n",
      "Identity Proxy calls Azureml identity bridge via this private endpoint.\n",
      "How model and image is pushed to staging area in this vnet is TBD.\n",
      "NSG rules for Azure vnet needing below service tags:\n",
      "Infra images download from MCR: MicrosoftContainerRegistry, AzureFrontDoor.FirstParty\n",
      "Cluster certificate download from keyvault: AzureKeyVault, AzureActiveDirectory\n",
      "Open Questions for Security Review\n",
      "In control plane and data plane, we would like to confirm if the following design is allowed from security perspective.\n",
      "Relay is used for communication between AzureML deployment plane (DP) and AKS cluster. We can create a private endpoint for the relay service in AKS vnet. From DP to relay service, we will disable public network access and enable trusted Microsoft services to bypass firewall.\n",
      "In data plane, a public load balancer will be created in AKS vnet for FrontDoor to call mesh in AKS.  This is consistent with the current MIR design. \n",
      "Pull model/image/key from a scoped registry. There will be a separate user assigned identity which has access to the Blob/ACR/KV in scoped registry.  \n",
      "Who’s responsible for encrypting model files, and setting up the UAI that can access the models/images/kvs? \n",
      "Is it allowed to pull the storage account(via PE), container registry(via PE) and KV from Nexus AML boundary?\n",
      "If the “scoped registry” proposal is approved, we will be in charge of publishing/deleting models from each scope?\n",
      "\n",
      "Data Plane\n",
      "\n",
      "MIR Frontdoor\n",
      "User will send traffic to frontdoor, the endpoint/deployment user aware is MIR endpoint/deployment.\n",
      "We need to pass the endpoint and deployment info from MIR frontdoor to Mesh so that Mesh has enough information to route the traffic.\n",
      "Add new deployment property “ComputeType”, if this endpoint is created for ARC, then set this property to “amlarc”\n",
      "For amlarc endpoint, apply the following changes:\n",
      "Rewrite request Url to include endpoint name like this /api/v1/endpoint/{endpoint_name}/…”, this is the request url format Mesh can handle.\n",
      "Set request header “azureml-model-deployment” with the real mir deployment name.\n",
      "Validate ARC cluster Cert, one ARC cluster will have only one cert, so this will be different from the endpoint level cert, we can use subscription as cert scope:.\n",
      "Certsubject name: \n",
      "Mesh\n",
      "Mesh detail design spec could be found here at Nexus_dataplane_in_cluster.docx\n",
      "Mesh should have a public SLB so FD can forward request to it. Use the NSG and mTLS to verify the traffic between FD and Cluster.\n",
      "Disable Auth since auth is already done by FD.\n",
      "Enable mTLS\n",
      "FD cert SAN: {region}.inference.ml.azure.com\n",
      "ARC cluster cert SAN: \n",
      "Header based routing (already supported)\n",
      "Always route to Rank-0\n",
      "Rank-0 pod should has label “rank-0” \n",
      "Sticky routing(align with MIR)\n",
      "Pod level sticky routing (refer to MIR)\n",
      "Response code rewrite (align with MIR)\n",
      "429 (503 in scoringfe)\n",
      "424 (502 in scoringfe)\n",
      "MIR endpoint/deployment mapping with ARC endpoint/deployment\n",
      "Requirement for distributed inference:\n",
      "Set all deployments pods with label rank info:\n",
      "For example:  rank: 0\n",
      "For non distributed inference, rank: 0 needs to be added as well.\n",
      "For distributed training, rank 0 becomes healthy when other ranks are healthy. \n",
      "Requirements for Preset:\n",
      "When creating online deployment, register the ScoringFE service IP info to the frontdoor. And endpoint type. \n",
      "Scoringfe to model worker with http in cluster (align with MIR)\n",
      "Envoy metrics publish to geneva, currently we only have QPM and latencies, need to figure out the gap and add the missing metrics\n",
      "Scoringfe forward request to CMP port (not model port) if CMP is enabled\n",
      "Mesh certificate management\n",
      "\n",
      "Scenarios when installing an extension:\n",
      "(manual)Generate certificate with ARC cluster cert SAN manually by using Keyvault in Vienna infra regional subscription with Name using cluster name.  (JIT by AME account)\n",
      "(manual) Get the AML extension subscription ID and add the subscription ID to identity bridge config whitelist. (JIT by AME account)\n",
      "(manual)Install the amlarc extension by sslSecret parameter with using a test certificate, keyvault name and certificate name.\n",
      "sslSecret : The name of the Kubernetes secret in the azureml namespace. This config is used to store cert.pem (PEM-encoded TLS/SSL cert) and key.pem (PEM-encoded TLS/SSL key), which are required for inference HTTPS endpoint support when allowInsecureConnections is set to False. Details could be found here. \n",
      "(manual)Get the amlarc extension client id and add it to the identity bridge config whitelist managed identity and assign the kevvault the keyvault reader access. (key vault and the extension needs in the same tenant(AME).)\n",
      "(Automation) CertGetter in amlarc extension in the cluster to download the certificate from the Keyvault every 24 hours by using the extension msi by calling identity bridge. Identity bridge verifies the extension msi and sends related certificate.  the keyvault. The service checks the certificate whether it is changed. If it is changed, it will update the sslSecret secret (See above chart).\n",
      "(Automation)The Keyvault will update the certificate when it is near expiration and will be downloaded and updated to sslsecret secret in the cluster.\n",
      "Work to do:\n",
      "Update identity bridge to support the auth basing on extension token and the config whitelist\n",
      "Add CertGetter in gateway in extension to support downloading the certificate and update the k8s secret.\n",
      "Scenarios when installing the helm chart:\n",
      "Infra team (one time work): Config the subscription(which holds the keyvault) in the onecert with certificate domain name: nexus.<location>.cloudapp.azure.com.\n",
      "Infra team: Create Keyvault which apply a certificate with\n",
      "Subject: <cluster-name>.nexus.<location>.cloudapp.azure.com\n",
      "SAN: \n",
      "DNS Name= <cluster-name>.nexus.<location>.cloudapp.azure.com\n",
      "DNS Name=*. <cluster-name>.nexus.<location>.cloudapp.azure.com\n",
      "DNS Name=*. SubscriptionGUIDshortRegion\n",
      "Infra team: Assign cluster identity to the keyvault reader access. \n",
      "Infra team: Install nexus by parameter keyvault name, cluster name, region\n",
      "App team: CertGetter in the cluster to download the certificate from the Keyvault every 24 hours by using the cluster msi by calling the keyvault. The service checks the certificate whether it is changed. If it is changed, it will update the k8s secret.\n",
      "Notes: The Keyvault will update the certificate when it is near expiration and will be downloaded and updated to sslsecret secret in the cluster.\n",
      "CMP\n",
      "Update user model deployment spec to include cmp as side-car\n",
      "CMP can get UAI (for RAI access) token (from identity proxy -> identity bridge?)\n",
      "CMP can access RAI endpoint (proper NSG config) \n",
      "Where to pull CMP image? From MCR? Is there security concerns for DV3?\n",
      "Model Data Collection (MDC)\n",
      "Align with MIR\n",
      "Details could be found at here.\n",
      "Model Data Collection (MDC) (preview) supports three data sink options:\n",
      "Workspace Blob Storage: Use workspace private endpoint\n",
      "Custom Blob Storage URI: To check. \n",
      "EventHub： Eventhub service tag needed if using Eventhub. \n",
      "Bandwidth Usage\n",
      "MIR frontdoor dashboard\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Private Endpoint Option (Optional)\n",
      "If public SLB on AKS cluster passes security review, then we will take public SLB option. Otherwise, this private endpoint option will be used to make the traffic through private link from MIR Frontdoor to Mesh.\n",
      "\n",
      "The flow to manage the private endpoint is:\n",
      "In ARC cluster creation flow\n",
      "Create internal load balancer for Mesh\n",
      "Create private link service for intenral load balancer, get the private link service resourceId\n",
      "Create PE for all reginal Mir frontdoors (use which identity? How to manage the FD list?)\n",
      "Get the PE private IP, this IP will be used later for deployment (where to store this IP?)\n",
      "In online deployment creation flow\n",
      "Get the PE IP that online deployment will be created on, and set the IP as target IP in frontdoor route table\n",
      "\n",
      "Data exfiltration prevention\n",
      "DV3 has a very high-standard security requirement to make sure the model & image’s safety, in MIR, this is done by using Managed Vnet with very limited outbound whitelist(NSG rules):\n",
      "Tented ACR & Storage\n",
      "Logs -> Geneva Ingestion Gateway (GIG) \n",
      "Metrics -> Runhistory (via PE)\n",
      "Certificate -> AML service (via PE)\n",
      "Token\n",
      "AML service (for online endpoint identity, via PE)\n",
      "IMDS (tented identity, via azure proxy vm)\n",
      "For AML Arc, we need to think about what outbound connections are required and need to have them reviewed with DV3 security team.\n",
      "AML Arc outbound connections:\n",
      "Azure Relay\n",
      "Azure Event hub\n",
      "Azure Arc (network requirements)\n",
      "Network requrements\n",
      "AzureML Extension\n",
      "Network requirements\n",
      "Azure-Arc \n",
      "Network requirements\n",
      "Data exchange between cluster and Azure\n",
      "Preset Flow for AMLArc\n",
      "See https://microsoftapc-my.sharepoint.com/:w:/g/personal/mingj_microsoft_com/Ed7PvVayyL5NrJAs75jV8ncBcbK2jBkp6EnG0rFRL05V-A?e=Ejvucy\n",
      "Model & Image Download\n",
      "We are adopting a push model where models/weights are “pushed” from tented registry/blob into third-party worker pool’s private network.\n",
      "   Questions:\n",
      "Which service should be responsible for syncing the model/image from Azure to local cluster storage, when the sync is triggered?\n",
      "How many models/images need to be maintained in local storage, what the estimation total storage size is, especially, how to retire old versions, manually or automatically delete oldest version?\n",
      "How is CMP image downloaded? From MCR or local image registry?\n",
      "Are the local storage nodes stable? Does the membership need to be dynamically updated frequently?\n",
      "How many models are to be cached, and how to maintain, especially, how to retire old versions?  - At lease 4-5 version(1.2T per version)\n",
      "Is it safe that when keep the model files / images on a node after the deployment is removed? - Not a must.\n",
      "…\n",
      " There are 3(or could be more) potential paths to push down the needed model and weights:\n",
      "Artifact push service directly pushes data to in-cluster cache within the 3rd party network boundary. Deployment pulls data from in-cluster cache.\n",
      "Artifact push service pushes data to a Staging area in Azure vNet boundary, and a service pushes data down(or sync-ed from) to in-cluster cache within the 3rd party ntework boundary. Deployment pulls data from in-cluster cache.\n",
      "Artifact push service pushes data to a cache in Azure vNet boundary, and deployment pulls data from in-Azure cache.\n",
      "\n",
      "Option 1: potential security concerns\n",
      "For Option 2, one potential implementation mechanism is to use a MIR cluster as a proxy\n",
      "\n",
      "We will pre-configure a MIR cluster which has permission to download the model the cluster. This MIR cluster will be vnet-peering with AmlArc cluster. When user deploy new deployment to the AmlArc cluster, the workflow will check if the model exists on the MIR cluster and do file sync to get the model from the MIR cluster.  \n",
      "\n",
      "\n",
      "Proposed storage solution\n",
      "Model & Image publish\n",
      "TBD\n",
      "Image download\n",
      "Image is downloaded from the asset registry service by kubelet directly. The asset registry follows the OCI distribution spec.\n",
      "Model download\n",
      "User pod has a PVC pointing to the required model version.\n",
      "The corresponding PV should be created when the model is published to the OCI cluster.\n",
      "The PV is attached (downloaded actually) to the node where the user Pod is scheduled. The download action is triggered by CSI controller plugin when it receives ControllerPublishVolume RPC call and is done by CSI node plugin.\n",
      "CSI node plugin is a Daemonset running on every worker node and acts as the download client, it first tries to resolve the model manifest from the cluster asset registry and get the full list of storage items to download, then it starts the download in parallel from the local storage cluster.\n",
      "Model & Image reconcile / repair\n",
      "The asset push service in Azure should periodically poll the cluster asset registry service to check whether there’s any asset broken and try to push it again.\n",
      "The asset registry service in the cluster should maintain the latest health state of each asset in case the underlying storage cluster.\n",
      "\n",
      "Metrics and Logging\n",
      "OCI-Network-Logging-Metrics-Review.docx\n",
      "This table lists the metrics/logs currenlty used by DV3 (refer to The DV3 dashboard in MIR):\n",
      "\n",
      " \n",
      "Identity\n",
      "Identity On Nexus.docx\n",
      "Health Monitoring and Repair\n",
      "We leverage Compute Health Manager Draft Design.docx  for node problem detection and analysis.  In this session, we will focus on compute specific logic that includes workload precheck, workload repair, and node repair.\n",
      "\n",
      "Two new services will be added:\n",
      "ClusterHealthManager: this service is responsible for workload repair, including cordoning bad nodes and draining workloads. It also acts as a node repair history provider, storing nodes' repair history in configmaps. Additionally, it monitors the health manager’s heartbeat of each node and node not-ready events to prevent situations where nodes are unable to report their unhealthy status themselves.\n",
      "NodeProblemResolver: running on each node to perform node repair. for hot repair and reboot, we prefer to do it locally; for action such as reimage and re-provisioning, the repair will be routed to a compute service like AKS or OCI.\n",
      "  \n",
      "Precheck flow:\n",
      "Precheck is to ensure the node is in good status before user container is started. Node precheck will be triggered inside the init-container of the user pod. If the precheck fails, workload manager will take immediate action by cordoning the node and rescheduling the pod on other nodes.\n",
      "\n",
      "\n",
      "Repair flow:\n",
      "Azure core team might provide a new API to wrapper external compute related operations in the future, details have not yet come out, so in current design, we would call into OCI layer for node repair directly. \n",
      "\n",
      "\n",
      "\n",
      "HealthManager currently requests four typical repair actions from NodeProblemResolver: hotrepair, reboot, reimage, and reprovision. For hotrepair, the action can be performed locally. However, for the other repair actions, we need to route the requests to OCI cloud service:\n",
      "Authentication:\n",
      "OCI supports Instance Principal Authentication. After setting up required resources and policies, NodeProblemResolver running on each OCI instance can call Oracle Cloud Infrastructure public services, without the need to configure user credentials or a configuration file; This would require some additional steps during cluster setup to authorize OCI workers to make OCI API calls.\n",
      "Private access: \n",
      "To ensure that this traffic does not go over the internet, we probably need to setup service gateway.\n",
      "\n",
      "Additionally, if a node is in such bad health that it cannot trigger the repair itself, we will rely on ClusterHealthManager from Azure infra node to redirect the request to some other healthy node. We will let another worker make the call to OCI API service, ensuring a reliable repair process. as shown in the following diagram\n",
      "\n",
      "Questions:\n",
      "Will node management be supported by AKS service in the near future?\n",
      "OCI does not support reimage right now, looking into alternative solutions that can achieve a similar outcome.\n",
      "\n",
      "\n",
      "Appendix\n",
      "Work item breakdown:\n",
      "K8s compute setup & Networking\n",
      "Connect customer vnet and OCI VCN via ExpressRoute&FastConnect\n",
      "Create PE to arc, workspace, tented acr/blob(??).\n",
      "Relay or proxy for DP->k8s cluster communication.\n",
      "Proxy requires code change in MLC and DP\n",
      "Arc control plane should go through private link\n",
      "Node Health manager\n",
      "Node repair operator for OCI compute\n",
      "Health manager to cordon node and drain job\n",
      "Node repair config & node repair history\n",
      "Node grouping\n",
      "Expose correct envs for DV3.\n",
      "Update identity flow to handle tented arc/blob\n",
      "Handle scale up and down.\n",
      "Add failure analysis.\n",
      "Always send SIGTERM signal to rank 0 for gracefully shutdown.\n",
      "Route scoring request to rank 0 pod. \n",
      "Update scoringfe auto-scale logic.\n",
      "Telemetry\n",
      "Service log: Geneva \n",
      "gpu/cpu/memory metrics: az monitor\n",
      "Support for Sentinel for system/container events\n",
      "Performance\n",
      "Shared model\n",
      "Redis of OCI\n",
      "Model Download/Image pull\n",
      "Model/Image cache layer in the cluster\n",
      "Model/Image publish solutions\n",
      "Presets workflow\n",
      "Registry and preset workflow support for AzureML Arc\n",
      "Frontdoor with ScoringFE integration\n",
      "\n",
      "\n",
      " Summary of POC\n",
      "\n",
      "Overview\n",
      "\n",
      "Connect Azure network to OCI networks with ExpressRoute/FastConnect\n",
      "--\n",
      "1. https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ready/azure-best-practices/connectivity-to-other-providers-oci\n",
      "2. https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/azure.htm\n",
      "\n",
      "Setup Network\n",
      "We created two virtual networks in Azure and Oracle respectively and connect them with an interconnection with Azure ExpressRoute and Oracle FastConnect. The maximum bandwidth is 10Gbps. \n",
      "Create workspace\n",
      "We created a private AzureML workspace with private resource (storage account, container registry and key vault) and connect them to the Azure virtual networks (VNet) via private endpoint.\n",
      "We also configured the DNS records for the resources in Oracle virtual networks (VCN).\n",
      "Install AMLArc Extension and attach compute\n",
      "We created an Azure Relay connect it to the VNet via private endpoint. We installed AMLArc extension on OKE and attached it to the AzureML workspace via this relay.\n",
      "Deploy model\n",
      "Submit request: The user submits the deploy request from a VM in the Azure VNet.\n",
      "Start distributed deployment: The AMLArc extension reads number of the workers (nodes) from environment variable of the deployment specification and create workers for the deployment.\n",
      "Download image and model: Every worker downloads the image from the private registry and downloads model from private storage account (blob) to the OKE.\n",
      "Start MPI task: AMLArc extension set up SSH environment for every worker and start distributed MPI task on all workers.\n",
      "Logs and metrics\n",
      "System logs are sent to Kusto by The AMLArc extension via public network.\n",
      "Running \"az online-deployment log\" command could get deployment logs.\n",
      "Performance\n",
      "We run following test,\n",
      "Network bandwidth test\n",
      "Transmission from Azure VM to Oracle VM: 977.01Mbit/s\n",
      "Blob download test\n",
      "Download from blob to Oracle VM: 532.44Mbit/s\n",
      "(Standard, GRS, AzCli 10 maximum-connections)\n",
      "The model downloading relies on,\n",
      "Storage Account (Blob)\n",
      "Account type.\n",
      "Number of total connections\n",
      "Throttling.\n",
      "Network.\n",
      "Azure Virtual network gateway.\n",
      "ExpressRoute/FastConnect bandwidth\n",
      "OKE node (VM) bandwidth\n",
      "For now, the test is limited by the total network bandwidth of the Oracle VM (0.7Gbps). We are also not sure if multiple interconnections could increase bandwidth.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(document_folder_path)\n",
    "print(\"Files in folder:\", files)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "file_name = files[8]\n",
    "file_path = os.path.join(document_folder_path, file_name)\n",
    "\n",
    "print(f\"Reading document: {file_name}\")\n",
    "text, metadata = load_document(file_path)\n",
    "\n",
    "if text:\n",
    "    print(\"Text loaded successfully.\")\n",
    "    print(f\"First 100000 characters of the text:\\n{text[:100000]}\")\n",
    "else:\n",
    "    print(\"Failed to load text.\")\n",
    "    print(\"Metadata:\", metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35968a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
